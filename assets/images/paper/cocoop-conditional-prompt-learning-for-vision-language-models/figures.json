[
  {
    "page": 2,
    "bbox": [
      309.931396484375,
      609.1184539794922,
      546.58349609375,
      720.1232299804688
    ],
    "caption": "Figure 2. Our approach, Conditional Context Optimization (Co-CoOp), consists of two learnable components: a set of context vectors and a lightweight neural network (Meta-Net) that generates for each image an input-conditional token.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/CoCoOp-Conditional-Prompt-Learning-for-Vision-Language-Models/fig_01.png"
  },
  {
    "page": 2,
    "bbox": [
      309.931396484375,
      609.1184539794922,
      546.58349609375,
      720.1232299804688
    ],
    "caption": "Figure 2. Our approach, Conditional Context Optimization (Co-CoOp), consists of two learnable components: a set of context vectors and a lightweight neural network (Meta-Net) that generates for each image an input-conditional token.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      93.16999816894531,
      370.9936981201172,
      245.79539489746094,
      417.3711395263672
    ],
    "caption": "Context Optimization (CoOp) aims to overcome the inefficiency problem in prompt engineering for better adapting pre-trained vision-language models to downstream applications [(<>)63]. The key idea in CoOp is to model each context token using a continuous vector that can be end-to-end learned from data. Concretely, instead of using “a photo of a” as the context, CoOp introduces M learnable context vectors, {v1, v2, . . . , vM }, each having the same dimension with the word embeddings. The prompt for the i-th class, denoted by ti, now becomes ti = {v1, v2, . . . , vM , ci} where ci is the word embedding(s) for the class name. The context vectors are shared among all classes.3 (<>)Let g(·) denote the text encoder, the prediction probability is then",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/CoCoOp-Conditional-Prompt-Learning-for-Vision-Language-Models/fig_03.png"
  },
  {
    "page": 3,
    "bbox": [
      93.16999816894531,
      370.9936981201172,
      245.79539489746094,
      417.3711395263672
    ],
    "caption": "Context Optimization (CoOp) aims to overcome the inefficiency problem in prompt engineering for better adapting pre-trained vision-language models to downstream applications [(<>)63]. The key idea in CoOp is to model each context token using a continuous vector that can be end-to-end learned from data. Concretely, instead of using “a photo of a” as the context, CoOp introduces M learnable context vectors, {v1, v2, . . . , vM }, each having the same dimension with the word embeddings. The prompt for the i-th class, denoted by ti, now becomes ti = {v1, v2, . . . , vM , ci} where ci is the word embedding(s) for the class name. The context vectors are shared among all classes.3 (<>)Let g(·) denote the text encoder, the prediction probability is then",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      90.75100708007812,
      141.03970336914062,
      248.2144012451172,
      187.4261016845703
    ],
    "caption": "3CoOp has an alternative version that learns class-specific context, which is not considered here because it is not straightforward to transfer class-specific context to unseen classes.",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/CoCoOp-Conditional-Prompt-Learning-for-Vision-Language-Models/fig_05.png"
  },
  {
    "page": 3,
    "bbox": [
      90.75100708007812,
      141.03970336914062,
      248.2144012451172,
      187.4261016845703
    ],
    "caption": "3CoOp has an alternative version that learns class-specific context, which is not considered here because it is not straightforward to transfer class-specific context to unseen classes.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      342.343994140625,
      315.02870178222656,
      547.6079559326172,
      361.41510009765625
    ],
    "caption": "Let hθ(·) denote the Meta-Net parameterized by θ, each context token is now obtained by vm(x) = vm + π where π = hθ(x) and m ∈ {1, 2, ..., M}. The prompt for the i-th class is thus conditioned on the input, i.e., ti(x) = {v1(x), v2(x), . . . , vM (x), ci}. The prediction probability is computed as",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/CoCoOp-Conditional-Prompt-Learning-for-Vision-Language-Models/fig_07.png"
  },
  {
    "page": 3,
    "bbox": [
      342.343994140625,
      315.02870178222656,
      547.6079559326172,
      361.41510009765625
    ],
    "caption": "Let hθ(·) denote the Meta-Net parameterized by θ, each context token is now obtained by vm(x) = vm + π where π = hθ(x) and m ∈ {1, 2, ..., M}. The prompt for the i-th class is thus conditioned on the input, i.e., ti(x) = {v1(x), v2(x), . . . , vM (x), ci}. The prediction probability is computed as",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      47.42625427246094,
      529.9109954833984,
      547.4983978271484,
      723.1608734130859
    ],
    "caption": "Figure 3. Comprehensive comparisons of CoCoOp and CoOp in the base-to-new generalization setting. (a) CoCoOp is able to gain consistent improvements over CoOp in unseen classes on all datasets. (b) CoCoOp’s declines in base accuracy are mostly under 3%, which are far outweighed by the gains in generalization.",
    "file_name": [
      "figures/fileoutpart31.png"
    ],
    "output_file": "assets/images/paper/CoCoOp-Conditional-Prompt-Learning-for-Vision-Language-Models/fig_09.png"
  },
  {
    "page": 5,
    "bbox": [
      47.42625427246094,
      529.9109954833984,
      547.4983978271484,
      723.1608734130859
    ],
    "caption": "Figure 3. Comprehensive comparisons of CoCoOp and CoOp in the base-to-new generalization setting. (a) CoCoOp is able to gain consistent improvements over CoOp in unseen classes on all datasets. (b) CoCoOp’s declines in base accuracy are mostly under 3%, which are far outweighed by the gains in generalization.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      47.452392578125,
      -14.12164306640625,
      288.3811340332031,
      611.3023376464844
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart38.png"
    ],
    "output_file": "assets/images/paper/CoCoOp-Conditional-Prompt-Learning-for-Vision-Language-Models/fig_11.png"
  },
  {
    "page": 7,
    "bbox": [
      47.452392578125,
      -14.12164306640625,
      288.3811340332031,
      611.3023376464844
    ],
    "caption": "",
    "file_name": ""
  }
]