[
  {
    "page": 1,
    "bbox": [
      19.070083618164062,
      402.19683837890625,
      576.3878021240234,
      715.7528228759766
    ],
    "caption": "Although prompt-based zero-shot transfer learning showed promising performances, designing good prompts remains an engineering problem that demands substantial time and domain knowledge. To address the issue, Context Optimization (CoOp) [(<>)75] further proposed to learn continuous soft prompts with few-shot examples for replacing the carefully-chosen hard prompts. CoOp brings about significant improvement on few-shot classification over both zero-shot CLIP and linear probe CLIP settings, exhibiting the potential of prompt tuning on large-scale pretrained vision-language models. In this paper, we propose a different approach for better adapting vision-language models with feature adapters instead of prompt tuning. Different from CoOp that performs soft prompt optimization, we simply conduct fine-tuning on the light-weight additional feature adapters. Because of the over-parameterization of CLIP and lack of enough training examples, naive finetuning would lead to overfitting on specific datasets and the training process would be very slow owing to the forward and backward propagations across all CLIP layers.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_01.png"
  },
  {
    "page": 1,
    "bbox": [
      19.070083618164062,
      402.19683837890625,
      576.3878021240234,
      715.7528228759766
    ],
    "caption": "Although prompt-based zero-shot transfer learning showed promising performances, designing good prompts remains an engineering problem that demands substantial time and domain knowledge. To address the issue, Context Optimization (CoOp) [(<>)75] further proposed to learn continuous soft prompts with few-shot examples for replacing the carefully-chosen hard prompts. CoOp brings about significant improvement on few-shot classification over both zero-shot CLIP and linear probe CLIP settings, exhibiting the potential of prompt tuning on large-scale pretrained vision-language models. In this paper, we propose a different approach for better adapting vision-language models with feature adapters instead of prompt tuning. Different from CoOp that performs soft prompt optimization, we simply conduct fine-tuning on the light-weight additional feature adapters. Because of the over-parameterization of CLIP and lack of enough training examples, naive finetuning would lead to overfitting on specific datasets and the training process would be very slow owing to the forward and backward propagations across all CLIP layers.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      82.108642578125,
      366.4044189453125,
      461.9344024658203,
      672.1782379150391
    ],
    "caption": "Fig. 2 Comparison of different visual classification architectures. The image in the top row with a green region shows the naive pipeline for image classification [(<>)32], where f and W represents the visual feature and classifier weight respectively. The following pink, yellow and blue regions represent the pipeline of CLIP [(<>)50], CoOp [(<>)75], and our proposed CLIP-Adapter respectively.",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_03.png"
  },
  {
    "page": 3,
    "bbox": [
      82.108642578125,
      366.4044189453125,
      461.9344024658203,
      672.1782379150391
    ],
    "caption": "Fig. 2 Comparison of different visual classification architectures. The image in the top row with a green region shows the naive pipeline for image classification [(<>)32], where f and W represents the visual feature and classifier weight respectively. The following pink, yellow and blue regions represent the pipeline of CLIP [(<>)50], CoOp [(<>)75], and our proposed CLIP-Adapter respectively.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      289.01300048828125,
      120.39900207519531,
      503.1222381591797,
      167.8286590576172
    ],
    "caption": "a feature manifold f ∈ RD , where D represents the feature dimensionality. To perform classification, the image feature vector f is then multiplied with a classifier weight matrix W ∈ RD×K , where K represents the number of classes to be classified. After matrix multiplication, we can obtain a K-dimensional logit. A Softmax function is used to convert the logit into a probability vector p ∈ RK over the K classes. The whole process can be written as the following equations:",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_05.png"
  },
  {
    "page": 3,
    "bbox": [
      289.01300048828125,
      120.39900207519531,
      503.1222381591797,
      167.8286590576172
    ],
    "caption": "a feature manifold f ∈ RD , where D represents the feature dimensionality. To perform classification, the image feature vector f is then multiplied with a classifier weight matrix W ∈ RD×K , where K represents the number of classes to be classified. After matrix multiplication, we can obtain a K-dimensional logit. A Softmax function is used to convert the logit into a probability vector p ∈ RK over the K classes. The whole process can be written as the following equations:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      60.194793701171875,
      357.1715545654297,
      242.58375549316406,
      369.3564147949219
    ],
    "caption": "Alternatively, CoOp adopts continuous prompts instead of hand-crafted hard prompts. CoOp creates a list of random-initialized learnable soft tokens S ∈ RL×D , where L stands for the length of the soft token sequence. The soft token sequence S is then concatenated to each class name Ci and thus form a prompt. We represent the whole process as",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_07.png"
  },
  {
    "page": 4,
    "bbox": [
      60.194793701171875,
      357.1715545654297,
      242.58375549316406,
      369.3564147949219
    ],
    "caption": "Alternatively, CoOp adopts continuous prompts instead of hand-crafted hard prompts. CoOp creates a list of random-initialized learnable soft tokens S ∈ RL×D , where L stands for the length of the soft token sequence. The soft token sequence S is then concatenated to each class name Ci and thus form a prompt. We represent the whole process as",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      56.418975830078125,
      225.35255432128906,
      236.3987579345703,
      237.53640747070312
    ],
    "caption": "For both CLIP and CoOp, with the generated classifier weight Wi, where i ∈ {1,· · · ,K}, we can thus calculate the prediction probability pi for class i by the previously mentioned Eq. ((<>)1).",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_09.png"
  },
  {
    "page": 4,
    "bbox": [
      56.418975830078125,
      225.35255432128906,
      236.3987579345703,
      237.53640747070312
    ],
    "caption": "For both CLIP and CoOp, with the generated classifier weight Wi, where i ∈ {1,· · · ,K}, we can thus calculate the prediction probability pi for class i by the previously mentioned Eq. ((<>)1).",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      327.9954833984375,
      267.7392578125,
      461.39125061035156,
      298.7854766845703
    ],
    "caption": "where W1v , W2v and W1t , W2t are the weights of bottleneck linear layers for visual branch and text branch, respectively. The new knowledge captured via finetuning is added with the original features via residual connections:",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_11.png"
  },
  {
    "page": 4,
    "bbox": [
      327.9954833984375,
      267.7392578125,
      461.39125061035156,
      298.7854766845703
    ],
    "caption": "where W1v , W2v and W1t , W2t are the weights of bottleneck linear layers for visual branch and text branch, respectively. The new knowledge captured via finetuning is added with the original features via residual connections:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      327.9193115234375,
      150.00689697265625,
      461.46417236328125,
      186.41146850585938
    ],
    "caption": "After obtaining new image feature f⋆ and classifier weight W⋆ , we also adopt Equation ((<>)1) to calculate the category probability vector P = {pi}iK=1 and predict the image category by selecting the class ˆi that has the highest probability: ˆi = arg maxi pi.",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_13.png"
  },
  {
    "page": 4,
    "bbox": [
      327.9193115234375,
      150.00689697265625,
      461.46417236328125,
      186.41146850585938
    ],
    "caption": "After obtaining new image feature f⋆ and classifier weight W⋆ , we also adopt Equation ((<>)1) to calculate the category probability vector P = {pi}iK=1 and predict the image category by selecting the class ˆi that has the highest probability: ˆi = arg maxi pi.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      67.99699401855469,
      583.8549957275391,
      241.77125549316406,
      639.1369934082031
    ],
    "caption": "3.3 Variants of CLIP-Adapter",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_15.png"
  },
  {
    "page": 5,
    "bbox": [
      67.99699401855469,
      583.8549957275391,
      241.77125549316406,
      639.1369934082031
    ],
    "caption": "3.3 Variants of CLIP-Adapter",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      42.9468994140625,
      584.3197174072266,
      324.0199890136719,
      689.79833984375
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 80 Score (%) Zero-shot CLIP CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_17.png"
  },
  {
    "page": 6,
    "bbox": [
      42.9468994140625,
      584.3197174072266,
      324.0199890136719,
      689.79833984375
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 80 Score (%) Zero-shot CLIP CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      193.6680145263672,
      584.4281311035156,
      474.67079162597656,
      689.88037109375
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 80 Score (%) Zero-shot CLIP CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_19.png"
  },
  {
    "page": 6,
    "bbox": [
      193.6680145263672,
      584.4281311035156,
      474.67079162597656,
      689.88037109375
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 80 Score (%) Zero-shot CLIP CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      344.38890075683594,
      584.3197174072266,
      625.4619903564453,
      689.79833984375
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 Score (%)Zero-shot CLIP CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_21.png"
  },
  {
    "page": 6,
    "bbox": [
      344.38890075683594,
      584.3197174072266,
      625.4619903564453,
      689.79833984375
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 Score (%)Zero-shot CLIP CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      42.9468994140625,
      474.9977111816406,
      324.0199890136719,
      580.4763336181641
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 Score (%) Zero-shot CLIP Food101 CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart11.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_23.png"
  },
  {
    "page": 6,
    "bbox": [
      42.9468994140625,
      474.9977111816406,
      324.0199890136719,
      580.4763336181641
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 Score (%) Zero-shot CLIP Food101 CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      193.66790771484375,
      474.9977111816406,
      474.7409973144531,
      580.4763336181641
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 Score (%) Zero-shot CLIP Food101 CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_25.png"
  },
  {
    "page": 6,
    "bbox": [
      193.66790771484375,
      474.9977111816406,
      474.7409973144531,
      580.4763336181641
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 Score (%) Zero-shot CLIP Food101 CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      344.38890075683594,
      474.9977111816406,
      625.4619903564453,
      580.4763336181641
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 10 15 20 25 30 35 40 Score (%) Zero-shot CLIP FGVCAircraft CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart13.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_27.png"
  },
  {
    "page": 6,
    "bbox": [
      344.38890075683594,
      474.9977111816406,
      625.4619903564453,
      580.4763336181641
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 10 15 20 25 30 35 40 Score (%) Zero-shot CLIP FGVCAircraft CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      42.9468994140625,
      365.6759490966797,
      324.0199890136719,
      471.1545715332031
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 25 30 35 40 45 50 55 60 65 Score (%) Zero-shot CLIP DTD CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart14.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_29.png"
  },
  {
    "page": 6,
    "bbox": [
      42.9468994140625,
      365.6759490966797,
      324.0199890136719,
      471.1545715332031
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 25 30 35 40 45 50 55 60 65 Score (%) Zero-shot CLIP DTD CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      193.66790771484375,
      365.6759490966797,
      474.7409973144531,
      471.1545715332031
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 25 30 35 40 45 50 55 60 65 Score (%) Zero-shot CLIP DTD CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_31.png"
  },
  {
    "page": 6,
    "bbox": [
      193.66790771484375,
      365.6759490966797,
      474.7409973144531,
      471.1545715332031
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 25 30 35 40 45 50 55 60 65 Score (%) Zero-shot CLIP DTD CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      344.38890075683594,
      365.6759490966797,
      625.4619903564453,
      471.1545715332031
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 20 30 40 50 60 70 80 Score (%) Zero-shot CLIP CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart16.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_33.png"
  },
  {
    "page": 6,
    "bbox": [
      344.38890075683594,
      365.6759490966797,
      625.4619903564453,
      471.1545715332031
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 20 30 40 50 60 70 80 Score (%) Zero-shot CLIP CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      42.9468994140625,
      256.49171447753906,
      324.0199890136719,
      361.9703369140625
    ],
    "caption": "Fig. 3 Main results of few-shot learning on 11 datasets. CLIP-Adapter consistently shows better performance over previous baselines across different training shots.",
    "file_name": [
      "figures/fileoutpart17.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_35.png"
  },
  {
    "page": 6,
    "bbox": [
      42.9468994140625,
      256.49171447753906,
      324.0199890136719,
      361.9703369140625
    ],
    "caption": "Fig. 3 Main results of few-shot learning on 11 datasets. CLIP-Adapter consistently shows better performance over previous baselines across different training shots.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      193.66790771484375,
      256.49171447753906,
      474.7409973144531,
      361.9703369140625
    ],
    "caption": "Fig. 3 Main results of few-shot learning on 11 datasets. CLIP-Adapter consistently shows better performance over previous baselines across different training shots.",
    "file_name": [
      "figures/fileoutpart18.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_37.png"
  },
  {
    "page": 6,
    "bbox": [
      193.66790771484375,
      256.49171447753906,
      474.7409973144531,
      361.9703369140625
    ],
    "caption": "Fig. 3 Main results of few-shot learning on 11 datasets. CLIP-Adapter consistently shows better performance over previous baselines across different training shots.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      344.38890075683594,
      256.49171447753906,
      625.4619903564453,
      361.9703369140625
    ],
    "caption": "Fig. 3 Main results of few-shot learning on 11 datasets. CLIP-Adapter consistently shows better performance over previous baselines across different training shots.",
    "file_name": [
      "figures/fileoutpart19.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_39.png"
  },
  {
    "page": 6,
    "bbox": [
      344.38890075683594,
      256.49171447753906,
      625.4619903564453,
      361.9703369140625
    ],
    "caption": "Fig. 3 Main results of few-shot learning on 11 datasets. CLIP-Adapter consistently shows better performance over previous baselines across different training shots.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      300.02247619628906,
      411.36412048339844,
      472.0478210449219,
      549.4385528564453
    ],
    "caption": "Fig. 4 Absolute performance gain of CLIP-Adapter against hand-crafted prompts on different datasets.",
    "file_name": [
      "figures/fileoutpart22.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_41.png"
  },
  {
    "page": 7,
    "bbox": [
      300.02247619628906,
      411.36412048339844,
      472.0478210449219,
      549.4385528564453
    ],
    "caption": "Fig. 4 Absolute performance gain of CLIP-Adapter against hand-crafted prompts on different datasets.",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      -71.08064270019531,
      540.4924774169922,
      249.93043518066406,
      682.4317169189453
    ],
    "caption": "Fig. 5 Comparison among different variants of CLIP-Adapter.",
    "file_name": [
      "figures/fileoutpart23.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_43.png"
  },
  {
    "page": 8,
    "bbox": [
      -71.08064270019531,
      540.4924774169922,
      249.93043518066406,
      682.4317169189453
    ],
    "caption": "Fig. 5 Comparison among different variants of CLIP-Adapter.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      65.76400756835938,
      226.99099731445312,
      483.0196533203125,
      504.49322509765625
    ],
    "caption": "Fig. 6 Visualization of different learned feature manifolds via t-SNE.",
    "file_name": [
      "figures/fileoutpart34.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_45.png"
  },
  {
    "page": 13,
    "bbox": [
      41.24690246582031,
      508.83770751953125,
      322.3199920654297,
      614.3163299560547
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 80 90 Score (%) Zero-shot CLIP OxfordPets CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart43.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_46.png"
  },
  {
    "page": 13,
    "bbox": [
      41.24690246582031,
      508.83770751953125,
      322.3199920654297,
      614.3163299560547
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 80 90 Score (%) Zero-shot CLIP OxfordPets CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      191.96791076660156,
      508.83770751953125,
      473.04100036621094,
      614.3163299560547
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 80 90 Score (%) Zero-shot CLIP OxfordPets CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart44.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_48.png"
  },
  {
    "page": 13,
    "bbox": [
      191.96791076660156,
      508.83770751953125,
      473.04100036621094,
      614.3163299560547
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 80 90 Score (%) Zero-shot CLIP OxfordPets CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      342.68890380859375,
      508.83770751953125,
      623.7619934082031,
      614.3163299560547
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 Score (%)Zero-shot CLIP CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart45.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_50.png"
  },
  {
    "page": 13,
    "bbox": [
      342.68890380859375,
      508.83770751953125,
      623.7619934082031,
      614.3163299560547
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 Score (%)Zero-shot CLIP CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      41.24690246582031,
      399.5159606933594,
      322.3199920654297,
      504.9945831298828
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 80 Score (%) Zero-shot CLIP Food101 CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart46.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_52.png"
  },
  {
    "page": 13,
    "bbox": [
      41.24690246582031,
      399.5159606933594,
      322.3199920654297,
      504.9945831298828
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 80 Score (%) Zero-shot CLIP Food101 CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      191.96791076660156,
      399.5159606933594,
      473.04100036621094,
      504.9945831298828
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 80 Score (%) Zero-shot CLIP Food101 CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart47.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_54.png"
  },
  {
    "page": 13,
    "bbox": [
      191.96791076660156,
      399.5159606933594,
      473.04100036621094,
      504.9945831298828
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 30 40 50 60 70 80 Score (%) Zero-shot CLIP Food101 CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      342.68890380859375,
      399.5159606933594,
      623.7619934082031,
      504.9945831298828
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 15 20 25 30 35 40 Score (%) Zero-shot CLIP FGVCAircraft CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart48.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_56.png"
  },
  {
    "page": 13,
    "bbox": [
      342.68890380859375,
      399.5159606933594,
      623.7619934082031,
      504.9945831298828
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 15 20 25 30 35 40 Score (%) Zero-shot CLIP FGVCAircraft CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      41.24690246582031,
      290.19471740722656,
      322.3199920654297,
      395.67333984375
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 25 30 35 40 45 50 55 60 65 Score (%) Zero-shot CLIP DTD CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart49.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_58.png"
  },
  {
    "page": 13,
    "bbox": [
      41.24690246582031,
      290.19471740722656,
      322.3199920654297,
      395.67333984375
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 25 30 35 40 45 50 55 60 65 Score (%) Zero-shot CLIP DTD CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      191.96791076660156,
      290.19471740722656,
      473.04100036621094,
      395.67333984375
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 25 30 35 40 45 50 55 60 65 Score (%) Zero-shot CLIP DTD CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart50.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_60.png"
  },
  {
    "page": 13,
    "bbox": [
      191.96791076660156,
      290.19471740722656,
      473.04100036621094,
      395.67333984375
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 25 30 35 40 45 50 55 60 65 Score (%) Zero-shot CLIP DTD CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      342.68890380859375,
      290.19471740722656,
      623.7619934082031,
      395.67333984375
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 40 50 60 70 80 Score (%) Zero-shot CLIP EuroSAT CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": [
      "figures/fileoutpart51.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_62.png"
  },
  {
    "page": 13,
    "bbox": [
      342.68890380859375,
      290.19471740722656,
      623.7619934082031,
      395.67333984375
    ],
    "caption": "0 2 4 6 8 10 12 14 16 Number of labeled training examples per class 40 50 60 70 80 Score (%) Zero-shot CLIP EuroSAT CLIP-Adapter CoOp Linear probe CLIP Zero-shot CLIP",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      39.585906982421875,
      181.00970458984375,
      320.65899658203125,
      286.4883270263672
    ],
    "caption": "Fig. 7 Results under CLIP-style preprocessing of few-shot learning on 11 datasets. Compared to CoOp-style preprocessing in Figure 3 of the main body, all results are boosted and CLIP-Adapter still shows leading performance over previous baselines across different training shots.",
    "file_name": [
      "figures/fileoutpart52.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_64.png"
  },
  {
    "page": 13,
    "bbox": [
      39.585906982421875,
      181.00970458984375,
      320.65899658203125,
      286.4883270263672
    ],
    "caption": "Fig. 7 Results under CLIP-style preprocessing of few-shot learning on 11 datasets. Compared to CoOp-style preprocessing in Figure 3 of the main body, all results are boosted and CLIP-Adapter still shows leading performance over previous baselines across different training shots.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      190.30690002441406,
      181.00970458984375,
      471.37998962402344,
      286.4883270263672
    ],
    "caption": "Fig. 7 Results under CLIP-style preprocessing of few-shot learning on 11 datasets. Compared to CoOp-style preprocessing in Figure 3 of the main body, all results are boosted and CLIP-Adapter still shows leading performance over previous baselines across different training shots.",
    "file_name": [
      "figures/fileoutpart53.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_66.png"
  },
  {
    "page": 13,
    "bbox": [
      190.30690002441406,
      181.00970458984375,
      471.37998962402344,
      286.4883270263672
    ],
    "caption": "Fig. 7 Results under CLIP-style preprocessing of few-shot learning on 11 datasets. Compared to CoOp-style preprocessing in Figure 3 of the main body, all results are boosted and CLIP-Adapter still shows leading performance over previous baselines across different training shots.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      341.0279083251953,
      181.00970458984375,
      622.1009979248047,
      286.4883270263672
    ],
    "caption": "Fig. 7 Results under CLIP-style preprocessing of few-shot learning on 11 datasets. Compared to CoOp-style preprocessing in Figure 3 of the main body, all results are boosted and CLIP-Adapter still shows leading performance over previous baselines across different training shots.",
    "file_name": [
      "figures/fileoutpart54.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_68.png"
  },
  {
    "page": 13,
    "bbox": [
      341.0279083251953,
      181.00970458984375,
      622.1009979248047,
      286.4883270263672
    ],
    "caption": "Fig. 7 Results under CLIP-style preprocessing of few-shot learning on 11 datasets. Compared to CoOp-style preprocessing in Figure 3 of the main body, all results are boosted and CLIP-Adapter still shows leading performance over previous baselines across different training shots.",
    "file_name": ""
  }
]