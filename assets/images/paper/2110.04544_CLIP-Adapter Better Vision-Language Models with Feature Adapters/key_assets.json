{
  "architecture": {
    "page": 1,
    "bbox": [
      19.070083618164062,
      402.19683837890625,
      576.3878021240234,
      715.7528228759766
    ],
    "caption": "Although prompt-based zero-shot transfer learning showed promising performances, designing good prompts remains an engineering problem that demands substantial time and domain knowledge. To address the issue, Context Optimization (CoOp) [(<>)75] further proposed to learn continuous soft prompts with few-shot examples for replacing the carefully-chosen hard prompts. CoOp brings about significant improvement on few-shot classification over both zero-shot CLIP and linear probe CLIP settings, exhibiting the potential of prompt tuning on large-scale pretrained vision-language models. In this paper, we propose a different approach for better adapting vision-language models with feature adapters instead of prompt tuning. Different from CoOp that performs soft prompt optimization, we simply conduct fine-tuning on the light-weight additional feature adapters. Because of the over-parameterization of CLIP and lack of enough training examples, naive finetuning would lead to overfitting on specific datasets and the training process would be very slow owing to the forward and backward propagations across all CLIP layers.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_01.png"
  },
  "results": {
    "page": 11,
    "bbox": [
      83.6033935546875,
      452.6175231933594,
      460.6784362792969,
      538.7385711669922
    ],
    "caption": "Table 10 Comparison of finetuning different components of CLIP-Adapter.",
    "file_name": [
      "tables/fileoutpart39.xlsx",
      "tables/fileoutpart40.png"
    ],
    "output_file": "assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/table_355.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/fig_01.png)\n캡션: Although prompt-based zero-shot transfer learning showed promising performances, designing good prompts remains an engineering problem that demands substantial time and domain knowledge. To address the issue, Context Optimization (CoOp) [(<>)75] further proposed to learn continuous soft prompts with few-shot examples for replacing the carefully-chosen hard prompts. CoOp brings about significant im…\n\n### Main Results Table\n![Results](/assets/images/paper/2110.04544_CLIP-Adapter Better Vision-Language Models with Feature Adapters/table_355.png)\n캡션: Table 10 Comparison of finetuning different components of CLIP-Adapter."
}