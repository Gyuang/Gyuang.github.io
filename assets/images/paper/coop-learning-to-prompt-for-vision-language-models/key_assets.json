{
  "architecture": {
    "page": 1,
    "bbox": [
      296.88478088378906,
      597.5517730712891,
      355.94482421875,
      667.062255859375
    ],
    "caption": "Fig. 1 Prompt engineering vs Context Optimization (CoOp). The former needs to use a held-out validation set for words tuning, which is ineﬃcient; the latter automates the process and requires only a few labeled images for learning.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/CoOp-Learning-to-Prompt-for-Vision-Language-Models/fig_03.png"
  },
  "results": {
    "page": 11,
    "bbox": [
      89.71499633789062,
      548.2433929443359,
      489.76123046875,
      732.3494415283203
    ],
    "caption": "Table 7 Domain generalization results on DOSCO-2k, a recently proposed benchmark focusing on broader contextual domain shift. Among the three approaches, CoOp and its follow-up, CoCoOp, contain learnable components while CLIP here denotes the zero-shot model. Both CoOp and CoCoOp use four learnable context tokens initialized with the word embeddings of “a photo of a”. Bold denotes the best performance on each dataset for a specific architecture.",
    "file_name": [
      "tables/fileoutpart25.xlsx",
      "tables/fileoutpart26.png"
    ],
    "output_file": "assets/images/paper/CoOp-Learning-to-Prompt-for-Vision-Language-Models/table_605.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/CoOp-Learning-to-Prompt-for-Vision-Language-Models/fig_03.png)\n캡션: Fig. 1 Prompt engineering vs Context Optimization (CoOp). The former needs to use a held-out validation set for words tuning, which is ineﬃcient; the latter automates the process and requires only a few labeled images for learning.\n\n### Main Results Table\n![Results](/assets/images/paper/CoOp-Learning-to-Prompt-for-Vision-Language-Models/table_605.png)\n캡션: Table 7 Domain generalization results on DOSCO-2k, a recently proposed benchmark focusing on broader contextual domain shift. Among the three approaches, CoOp and its follow-up, CoCoOp, contain learnable components while CLIP here denotes the zero-shot model. Both CoOp and CoCoOp use four learnable context tokens initialized with the word embeddings of “a photo of a”. Bold denotes the best perform…"
}