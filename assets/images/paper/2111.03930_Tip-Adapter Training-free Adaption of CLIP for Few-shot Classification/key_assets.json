{
  "architecture": {
    "page": 3,
    "bbox": [
      -68.19412231445312,
      359.8735656738281,
      650.9335021972656,
      791.4262084960938
    ],
    "caption": "We propose Tip-Adapter, which is a training-free and non-parametric extension of CLIP-Adapter [(<>)17] but performs comparably and even better than it. To achieve this goal, it adopts the same architecture as CLIP-Adapter, but we construct a key-value cache model from the few-shot training set and transform the cache into the weights of the adapter MLP in a non-parametric manner without fine-tuning. Surprisingly, with weights constructed by a properly designed approach, Tip-Adapter without fine-tuning can achieve comparable performance as CLIP-Adapter with fine-tuning. In addition, if fine-tuning is allowable, further fine-tuning with such weights as network initialization is able to achieve much higher accuracy with super-fast convergence speed.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_06.png"
  },
  "results": {
    "page": 6,
    "bbox": [
      54.7572021484375,
      417.0545654296875,
      283.66587829589844,
      477.1726837158203
    ],
    "caption": "Table 1. Performances (%) of different models on various vision backbones. RN50 denotes ResNet-50, and ViT/32 denotes ViT-Base with 32 × 32 patch size, and RN50×16 denotes ResNet-50 with 16 times more computations [(<>)51].",
    "file_name": [
      "tables/fileoutpart9.xlsx",
      "tables/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/table_01.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_06.png)\n캡션: We propose Tip-Adapter, which is a training-free and non-parametric extension of CLIP-Adapter [(<>)17] but performs comparably and even better than it. To achieve this goal, it adopts the same architecture as CLIP-Adapter, but we construct a key-value cache model from the few-shot training set and transform the cache into the weights of the adapter MLP in a non-parametric manner without fine-tunin…\n\n### Main Results Table\n![Results](/assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/table_01.png)\n캡션: Table 1. Performances (%) of different models on various vision backbones. RN50 denotes ResNet-50, and ViT/32 denotes ViT-Base with 32 × 32 patch size, and RN50×16 denotes ResNet-50 with 16 times more computations [(<>)51]."
}