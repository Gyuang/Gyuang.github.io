[
  {
    "page": -1,
    "bbox": [
      308.86199951171875,
      364.9830017089844,
      546.4611053466797,
      536.3896484375
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_01.png"
  },
  {
    "page": 2,
    "bbox": [
      364.52099609375,
      234.22625732421875,
      491.94525146484375,
      248.8084716796875
    ],
    "caption": "where ϕ denotes the activation function in the MLP. Then, the adapted feature fa is linearly combined with the pre-trained feature fc with a hyper-parameter α ∈ [0, 1] to output the final classification logits. In this way, the prior knowledge of CLIP on the input image is updated by the adapter in an additive manner,",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_02.png"
  },
  {
    "page": 2,
    "bbox": [
      364.52099609375,
      234.22625732421875,
      491.94525146484375,
      248.8084716796875
    ],
    "caption": "where ϕ denotes the activation function in the MLP. Then, the adapted feature fa is linearly combined with the pre-trained feature fc with a hyper-parameter α ∈ [0, 1] to output the final classification logits. In this way, the prior knowledge of CLIP on the input image is updated by the adapter in an additive manner,",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      371.63427734375,
      133.15225219726562,
      484.8352508544922,
      147.73446655273438
    ],
    "caption": "where Wc is the weights of the text classifier. To construct Wc, following zero-shot CLIP, CLIP-Adapter places each category name into the pre-defined prompt template and encodes them by CLIP’s pre-trained textual encoder.",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_04.png"
  },
  {
    "page": 2,
    "bbox": [
      371.63427734375,
      133.15225219726562,
      484.8352508544922,
      147.73446655273438
    ],
    "caption": "where Wc is the weights of the text classifier. To construct Wc, following zero-shot CLIP, CLIP-Adapter places each category name into the pre-defined prompt template and encodes them by CLIP’s pre-trained textual encoder.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      -68.19412231445312,
      359.8735656738281,
      650.9335021972656,
      791.4262084960938
    ],
    "caption": "We propose Tip-Adapter, which is a training-free and non-parametric extension of CLIP-Adapter [(<>)17] but performs comparably and even better than it. To achieve this goal, it adopts the same architecture as CLIP-Adapter, but we construct a key-value cache model from the few-shot training set and transform the cache into the weights of the adapter MLP in a non-parametric manner without fine-tuning. Surprisingly, with weights constructed by a properly designed approach, Tip-Adapter without fine-tuning can achieve comparable performance as CLIP-Adapter with fine-tuning. In addition, if fine-tuning is allowable, further fine-tuning with such weights as network initialization is able to achieve much higher accuracy with super-fast convergence speed.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_06.png"
  },
  {
    "page": 3,
    "bbox": [
      -68.19412231445312,
      359.8735656738281,
      650.9335021972656,
      791.4262084960938
    ],
    "caption": "We propose Tip-Adapter, which is a training-free and non-parametric extension of CLIP-Adapter [(<>)17] but performs comparably and even better than it. To achieve this goal, it adopts the same architecture as CLIP-Adapter, but we construct a key-value cache model from the few-shot training set and transform the cache into the weights of the adapter MLP in a non-parametric manner without fine-tuning. Surprisingly, with weights constructed by a properly designed approach, Tip-Adapter without fine-tuning can achieve comparable performance as CLIP-Adapter with fine-tuning. In addition, if fine-tuning is allowable, further fine-tuning with such weights as network initialization is able to achieve much higher accuracy with super-fast convergence speed.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      365.50535583496094,
      257.1685028076172,
      490.96006774902344,
      284.2977752685547
    ],
    "caption": "To create the key-value cache, the CLIP-encoded representations Ftrain are treated as keys, while the one-hot ground-truth vectors Ltrain are used as their values. In this way, the key-value cache contains all the new knowledge extracted from the few-shot training set, which can be converted to the weights of the adapter MLP to update the prior knowledge encoded in the pre-trained CLIP.",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_08.png"
  },
  {
    "page": 3,
    "bbox": [
      365.50535583496094,
      257.1685028076172,
      490.96006774902344,
      284.2977752685547
    ],
    "caption": "To create the key-value cache, the CLIP-encoded representations Ftrain are treated as keys, while the one-hot ground-truth vectors Ltrain are used as their values. In this way, the key-value cache contains all the new knowledge extracted from the few-shot training set, which can be converted to the weights of the adapter MLP to update the prior knowledge encoded in the pre-trained CLIP.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      103.31765747070312,
      666.6201019287109,
      235.64715576171875,
      686.5594787597656
    ],
    "caption": "where β stands for a modulating hyper-parameter. Since both query and key features are L2 normalized, the term (1 − ftestFtrain T ) is equivalent to calculating the Euclidean distances between the test feature ftest and all few-shot training images’ features FT The exponential functiontrain. is adopted to convert query-key Euclidean distances to nonnegative affinities A with β modulating its sharpness. Afterwards, the retrieved value from the cache model can be obtained via the multiplication between the query-key affinities and the cached values as ALtrain.",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_10.png"
  },
  {
    "page": 4,
    "bbox": [
      103.31765747070312,
      666.6201019287109,
      235.64715576171875,
      686.5594787597656
    ],
    "caption": "where β stands for a modulating hyper-parameter. Since both query and key features are L2 normalized, the term (1 − ftestFtrain T ) is equivalent to calculating the Euclidean distances between the test feature ftest and all few-shot training images’ features FT The exponential functiontrain. is adopted to convert query-key Euclidean distances to nonnegative affinities A with β modulating its sharpness. Afterwards, the retrieved value from the cache model can be obtained via the multiplication between the query-key affinities and the cached values as ALtrain.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      80.89883422851562,
      480.2222595214844,
      258.0692596435547,
      511.25547790527344
    ],
    "caption": "where Wc represents CLIP’s text classifier, α denotes the residual ratio, and we define ϕ(x) = exp(−β(1 − x)). Intuitively, Tip-Adapter’s class prediction contains two terms: predictions according to values retrieved from the cache model and predictions from the pre-trained CLIP. The former term adaptively summarizes information from the few-shot training set. The values Ltrain (class predictions of the cached samples) in the cache are linearly combined according to the query-key affinities A. The latter term preserves the prior knowledge from the original CLIP by directly using the pre-trained classifier WcT to process the test image feature ftest. The two terms are balanced by the weight α. Empirically, α is set to be large if the domain gap between pre-trained and downstream tasks is large, since more knowledge from the few-shot set is required, and small otherwise.",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_12.png"
  },
  {
    "page": 4,
    "bbox": [
      80.89883422851562,
      480.2222595214844,
      258.0692596435547,
      511.25547790527344
    ],
    "caption": "where Wc represents CLIP’s text classifier, α denotes the residual ratio, and we define ϕ(x) = exp(−β(1 − x)). Intuitively, Tip-Adapter’s class prediction contains two terms: predictions according to values retrieved from the cache model and predictions from the pre-trained CLIP. The former term adaptively summarizes information from the few-shot training set. The values Ltrain (class predictions of the cached samples) in the cache are linearly combined according to the query-key affinities A. The latter term preserves the prior knowledge from the original CLIP by directly using the pre-trained classifier WcT to process the test image feature ftest. The two terms are balanced by the weight α. Empirically, α is set to be large if the domain gap between pre-trained and downstream tasks is large, since more knowledge from the few-shot set is required, and small otherwise.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      78.21965026855469,
      200.93099975585938,
      288.8526306152344,
      235.81446838378906
    ],
    "caption": "The differences between CLIP-Adapter and Tip-Adapter can be summarized as follows. Firstly, CLIP-Adapter randomly initializes W1 and W2 and learns them via SGD, while Tip-Adapter directly sets W1 as cached training features Ftrain and W2 as the transposed one-hot encoding of the ground-truth labels Ltrain, which are non-parametric and training-free. Secondly, the bottleneck dimension of Tip-Adapter is equal to NK, while CLIP-Adapter selects a low-dimensional bottleneck to prevent the risk of over-fitting. This indicates that with such proper initialization,",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_14.png"
  },
  {
    "page": 4,
    "bbox": [
      78.21965026855469,
      200.93099975585938,
      288.8526306152344,
      235.81446838378906
    ],
    "caption": "The differences between CLIP-Adapter and Tip-Adapter can be summarized as follows. Firstly, CLIP-Adapter randomly initializes W1 and W2 and learns them via SGD, while Tip-Adapter directly sets W1 as cached training features Ftrain and W2 as the transposed one-hot encoding of the ground-truth labels Ltrain, which are non-parametric and training-free. Secondly, the bottleneck dimension of Tip-Adapter is equal to NK, while CLIP-Adapter selects a low-dimensional bottleneck to prevent the risk of over-fitting. This indicates that with such proper initialization,",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      57.00578308105469,
      528.0328063964844,
      283.64788818359375,
      718.7401275634766
    ],
    "caption": "Figure 3. Classification accuracy of Tip-Adapter, Tip-Adapter-F and other models under different few-shot settings with CLIP-style pre-processing.",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_16.png"
  },
  {
    "page": 6,
    "bbox": [
      57.00578308105469,
      528.0328063964844,
      283.64788818359375,
      718.7401275634766
    ],
    "caption": "Figure 3. Classification accuracy of Tip-Adapter, Tip-Adapter-F and other models under different few-shot settings with CLIP-style pre-processing.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      37.686431884765625,
      546.1012725830078,
      657.814697265625,
      726.1889343261719
    ],
    "caption": "Figure 4. Results of few-shot classification on 10 datasets. Tip-Adapter exceeds zero-shot ClIP by a large margin, and Tip-Adapter-F further surpasses all compared methods by few-epoch fine-tuning.",
    "file_name": [
      "figures/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_18.png"
  },
  {
    "page": 7,
    "bbox": [
      37.686431884765625,
      546.1012725830078,
      657.814697265625,
      726.1889343261719
    ],
    "caption": "Figure 4. Results of few-shot classification on 10 datasets. Tip-Adapter exceeds zero-shot ClIP by a large margin, and Tip-Adapter-F further surpasses all compared methods by few-epoch fine-tuning.",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      76.38034057617188,
      569.0263671875,
      518.4886932373047,
      714.6784820556641
    ],
    "caption": "Figure 5. t-SNE Visualization of W1 in Tip-Adapter. Dots in different colors stand for embeddings of different categories. From left to right, three distributions indicate the variation of W1 during fine-tuning.",
    "file_name": [
      "figures/fileoutpart18.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_20.png"
  },
  {
    "page": 8,
    "bbox": [
      76.38034057617188,
      569.0263671875,
      518.4886932373047,
      714.6784820556641
    ],
    "caption": "Figure 5. t-SNE Visualization of W1 in Tip-Adapter. Dots in different colors stand for embeddings of different categories. From left to right, three distributions indicate the variation of W1 during fine-tuning.",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      60.52839660644531,
      -504.59381103515625,
      273.0768585205078,
      514.8031921386719
    ],
    "caption": "Figure 5. t-SNE Visualization of W1 in Tip-Adapter. Dots in different colors stand for embeddings of different categories. From left to right, three distributions indicate the variation of W1 during fine-tuning.",
    "file_name": [
      "figures/fileoutpart19.png"
    ],
    "output_file": "assets/images/paper/2111.03930_Tip-Adapter Training-free Adaption of CLIP for Few-shot Classification/fig_22.png"
  },
  {
    "page": 8,
    "bbox": [
      60.52839660644531,
      -504.59381103515625,
      273.0768585205078,
      514.8031921386719
    ],
    "caption": "Figure 5. t-SNE Visualization of W1 in Tip-Adapter. Dots in different colors stand for embeddings of different categories. From left to right, three distributions indicate the variation of W1 during fine-tuning.",
    "file_name": ""
  }
]