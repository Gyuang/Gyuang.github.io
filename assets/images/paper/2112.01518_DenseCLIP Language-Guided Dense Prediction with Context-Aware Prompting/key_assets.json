{
  "architecture": {
    "page": 4,
    "bbox": [
      82.00357055664062,
      113.96966552734375,
      256.9575653076172,
      133.9086456298828
    ],
    "caption": "Semantic segmentation. As discussed in Section (<>)3.2, our framework is model-agnostic and can be applied to any dense prediction pipelines. Moreover, we propose to use an auxiliary objective to make better use of our pixel-text score maps in segmentation. Since the score maps s ∈ RH4 W4×K can be viewed as smaller segmentation results, we therefore compute a segmentation loss on it:",
    "file_name": [
      "figures/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_32.png"
  },
  "results": {
    "page": 5,
    "bbox": [
      60.07499694824219,
      426.970458984375,
      537.4355316162109,
      669.7679595947266
    ],
    "caption": "Table 2. Ablation study. We demonstrate that performing post-model vision-to-language prompting can yield the better performance with fewer extra FLOPs and parameters.",
    "file_name": [
      "tables/fileoutpart14.xlsx",
      "tables/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/table_01.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_32.png)\n캡션: Semantic segmentation. As discussed in Section (<>)3.2, our framework is model-agnostic and can be applied to any dense prediction pipelines. Moreover, we propose to use an auxiliary objective to make better use of our pixel-text score maps in segmentation. Since the score maps s ∈ RH4 W4×K can be viewed as smaller segmentation results, we therefore compute a segmentation loss on it:\n\n### Main Results Table\n![Results](/assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/table_01.png)\n캡션: Table 2. Ablation study. We demonstrate that performing post-model vision-to-language prompting can yield the better performance with fewer extra FLOPs and parameters."
}