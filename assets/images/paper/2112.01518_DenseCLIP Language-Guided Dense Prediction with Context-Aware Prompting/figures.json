[
  {
    "page": -1,
    "bbox": [
      296.3516387939453,
      236.12648010253906,
      615.8831787109375,
      613.5013885498047
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_01.png"
  },
  {
    "page": -1,
    "bbox": [
      296.3516387939453,
      236.12648010253906,
      615.8831787109375,
      613.5013885498047
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      17.275421142578125,
      353.3416290283203,
      345.3954772949219,
      726.2002716064453
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_03.png"
  },
  {
    "page": 1,
    "bbox": [
      17.275421142578125,
      353.3416290283203,
      345.3954772949219,
      726.2002716064453
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      374.71478271484375,
      146.4195556640625,
      481.7593078613281,
      158.6044158935547
    ],
    "caption": "In the standard training process of CLIP, the global feature ¯z is used as the output of the image encoder while the other outputs z are usually neglected. However, we find z has two interesting properties: (1) z still retains sufficient spatial information thus can serve as a feature map. (2) since the",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_05.png"
  },
  {
    "page": 2,
    "bbox": [
      374.71478271484375,
      146.4195556640625,
      481.7593078613281,
      158.6044158935547
    ],
    "caption": "In the standard training process of CLIP, the global feature ¯z is used as the output of the image encoder while the other outputs z are usually neglected. However, we find z has two interesting properties: (1) z still retains sufficient spatial information thus can serve as a feature map. (2) since the",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      30.461929321289062,
      100.21363830566406,
      575.6382598876953,
      744.080810546875
    ],
    "caption": "(5)",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_07.png"
  },
  {
    "page": 3,
    "bbox": [
      30.461929321289062,
      100.21363830566406,
      575.6382598876953,
      744.080810546875
    ],
    "caption": "(5)",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      112.81912231445312,
      352.29551696777344,
      226.14247131347656,
      372.23565673828125
    ],
    "caption": "where ˆz and ˆt are the 2 normalized version of z and t along the channel dimension. The score maps characterize the results of pixel-text matching, which is one of the most crucial ingredients in our framework. Firstly, the score maps can be viewed as segmentation results with a lower resolution, and thus we can use them to compute an auxiliary segmentation loss. Secondly, we can concatenate the score maps to the last feature map to explicitly incorporate language priors, i.e., Our framework is model-agnostic because the modified feature maps can be directly used as usual in segmentation or detection with some minor modifications (e.g., the input dimension of FPN [(<>)25]).",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_09.png"
  },
  {
    "page": 3,
    "bbox": [
      112.81912231445312,
      352.29551696777344,
      226.14247131347656,
      372.23565673828125
    ],
    "caption": "where ˆz and ˆt are the 2 normalized version of z and t along the channel dimension. The score maps characterize the results of pixel-text matching, which is one of the most crucial ingredients in our framework. Firstly, the score maps can be viewed as segmentation results with a lower resolution, and thus we can use them to compute an auxiliary segmentation loss. Secondly, we can concatenate the score maps to the last feature map to explicitly incorporate language priors, i.e., Our framework is model-agnostic because the modified feature maps can be directly used as usual in segmentation or detection with some minor modifications (e.g., the input dimension of FPN [(<>)25]).",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      49.813995361328125,
      234.4765167236328,
      181.2261505126953,
      253.24681091308594
    ],
    "caption": "3.3. Context-Aware Prompting",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_11.png"
  },
  {
    "page": 3,
    "bbox": [
      55.860992431640625,
      241.0237274169922,
      58.15541076660156,
      253.1096649169922
    ],
    "caption": "3.3. Context-Aware Prompting",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      55.860992431640625,
      239.82420349121094,
      61.579681396484375,
      247.83035278320312
    ],
    "caption": "3.3. Context-Aware Prompting",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      84.38499450683594,
      240.80250549316406,
      88.3602294921875,
      247.77650451660156
    ],
    "caption": "3.3. Context-Aware Prompting",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      121.89799499511719,
      241.0237274169922,
      175.64686584472656,
      253.1096649169922
    ],
    "caption": "3.3. Context-Aware Prompting",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      49.813995361328125,
      234.4765167236328,
      181.2261505126953,
      253.24681091308594
    ],
    "caption": "3.3. Context-Aware Prompting",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      382.3290100097656,
      415.3885192871094,
      474.1431121826172,
      433.9002990722656
    ],
    "caption": "Vision-to-language prompting. Including descriptions of visual contexts can make the text more accurate. For example, “a photo of a cat in the grass.” is more accurate than “a photo of a cat.”. Therefore, we investigate how to use visual contexts to refine the text features. Generally, we can use the cross-attention mechanism in Transformer decoder [(<>)40] to model the interactions between vision and language.",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_17.png"
  },
  {
    "page": 3,
    "bbox": [
      382.3290100097656,
      415.3885192871094,
      474.1431121826172,
      433.9002990722656
    ],
    "caption": "Vision-to-language prompting. Including descriptions of visual contexts can make the text more accurate. For example, “a photo of a cat in the grass.” is more accurate than “a photo of a cat.”. Therefore, we investigate how to use visual contexts to refine the text features. Generally, we can use the cross-attention mechanism in Transformer decoder [(<>)40] to model the interactions between vision and language.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      360.828857421875,
      211.38250732421875,
      495.6379852294922,
      223.56777954101562
    ],
    "caption": "N×C",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_19.png"
  },
  {
    "page": 3,
    "bbox": [
      360.828857421875,
      211.38250732421875,
      495.6379852294922,
      223.56777954101562
    ],
    "caption": "N×C",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      308.5030059814453,
      179.49093627929688,
      547.6060638427734,
      205.50865173339844
    ],
    "caption": "Another choice is to refine the text features after the text encoder, namely post-model prompting. In this variant, we use CoOp [(<>)60] to generate text features and directly use them as the queries of the Transformer decoder:",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_21.png"
  },
  {
    "page": 3,
    "bbox": [
      361.06700134277344,
      192.6137237548828,
      381.80767822265625,
      205.50865173339844
    ],
    "caption": "Another choice is to refine the text features after the text encoder, namely post-model prompting. In this variant, we use CoOp [(<>)60] to generate text features and directly use them as the queries of the Transformer decoder:",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      524.1490020751953,
      192.39349365234375,
      536.9462890625,
      200.399658203125
    ],
    "caption": "Another choice is to refine the text features after the text encoder, namely post-model prompting. In this variant, we use CoOp [(<>)60] to generate text features and directly use them as the queries of the Transformer decoder:",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      316.0570068359375,
      180.65872192382812,
      336.79766845703125,
      193.55364990234375
    ],
    "caption": "Another choice is to refine the text features after the text encoder, namely post-model prompting. In this variant, we use CoOp [(<>)60] to generate text features and directly use them as the queries of the Transformer decoder:",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      308.5030059814453,
      179.49093627929688,
      547.6060638427734,
      205.50865173339844
    ],
    "caption": "Another choice is to refine the text features after the text encoder, namely post-model prompting. In this variant, we use CoOp [(<>)60] to generate text features and directly use them as the queries of the Transformer decoder:",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      359.7231140136719,
      77.76249694824219,
      496.74986267089844,
      89.94677734375
    ],
    "caption": "Another choice is to refine the text features after the text encoder, namely post-model prompting. In this variant, we use CoOp [(<>)60] to generate text features and directly use them as the queries of the Transformer decoder:",
    "file_name": [
      "figures/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_26.png"
  },
  {
    "page": 3,
    "bbox": [
      359.7231140136719,
      77.76249694824219,
      496.74986267089844,
      89.94677734375
    ],
    "caption": "Another choice is to refine the text features after the text encoder, namely post-model prompting. In this variant, we use CoOp [(<>)60] to generate text features and directly use them as the queries of the Transformer decoder:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      12.213409423828125,
      366.06085205078125,
      387.7311706542969,
      809.5569000244141
    ],
    "caption": "4.1. Semantic Segmentation",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_28.png"
  },
  {
    "page": 4,
    "bbox": [
      12.213409423828125,
      366.06085205078125,
      387.7311706542969,
      809.5569000244141
    ],
    "caption": "4.1. Semantic Segmentation",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      134.59823608398438,
      414.9026794433594,
      204.36346435546875,
      433.41392517089844
    ],
    "caption": "Although the two variants target the same goal, we prefer the post-model prompting for mainly two reasons: (1) The post-model prompting is efficient. The pre-model prompting requires extra forward passes of the text encoder during inference since its input is dependent on the image. In the case of post-model prompting, we can store the extracted text features after training and thus can reduce the overhead brought by the text encoder during inference. (2) Our empirical results show the post-model prompting can achieve better performance than pre-model prompting.",
    "file_name": [
      "figures/fileoutpart11.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_30.png"
  },
  {
    "page": 4,
    "bbox": [
      134.59823608398438,
      414.9026794433594,
      204.36346435546875,
      433.41392517089844
    ],
    "caption": "Although the two variants target the same goal, we prefer the post-model prompting for mainly two reasons: (1) The post-model prompting is efficient. The pre-model prompting requires extra forward passes of the text encoder during inference since its input is dependent on the image. In the case of post-model prompting, we can store the extracted text features after training and thus can reduce the overhead brought by the text encoder during inference. (2) Our empirical results show the post-model prompting can achieve better performance than pre-model prompting.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      82.00357055664062,
      113.96966552734375,
      256.9575653076172,
      133.9086456298828
    ],
    "caption": "Semantic segmentation. As discussed in Section (<>)3.2, our framework is model-agnostic and can be applied to any dense prediction pipelines. Moreover, we propose to use an auxiliary objective to make better use of our pixel-text score maps in segmentation. Since the score maps s ∈ RH4 W4×K can be viewed as smaller segmentation results, we therefore compute a segmentation loss on it:",
    "file_name": [
      "figures/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_32.png"
  },
  {
    "page": 4,
    "bbox": [
      82.00357055664062,
      113.96966552734375,
      256.9575653076172,
      133.9086456298828
    ],
    "caption": "Semantic segmentation. As discussed in Section (<>)3.2, our framework is model-agnostic and can be applied to any dense prediction pipelines. Moreover, we propose to use an auxiliary objective to make better use of our pixel-text score maps in segmentation. Since the score maps s ∈ RH4 W4×K can be viewed as smaller segmentation results, we therefore compute a segmentation loss on it:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      320.75782775878906,
      591.8795166015625,
      547.5998382568359,
      611.8216552734375
    ],
    "caption": "Applications to any backbone models. Another interesting usage of our framework is that we can replace the image encoder of CLIP with any backbones (e.g., ImageNet pre-trained models and self-supervised models). Although there might be no strong relation between the outputs of the visual backbone and the text encoder, the backbone can learn better and faster with language guidance. In other words, we can leverage the language priors from the pre-trained text encoder to improve the performance of any pre-trained image backbone, which makes DenseCLIP a more generic framework to improve dense prediction with the natural language priors learned from large-scale pre-training.",
    "file_name": [
      "figures/fileoutpart13.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_34.png"
  },
  {
    "page": 4,
    "bbox": [
      320.75782775878906,
      591.8795166015625,
      547.5998382568359,
      611.8216552734375
    ],
    "caption": "Applications to any backbone models. Another interesting usage of our framework is that we can replace the image encoder of CLIP with any backbones (e.g., ImageNet pre-trained models and self-supervised models). Although there might be no strong relation between the outputs of the visual backbone and the text encoder, the backbone can learn better and faster with language guidance. In other words, we can leverage the language priors from the pre-trained text encoder to improve the performance of any pre-trained image backbone, which makes DenseCLIP a more generic framework to improve dense prediction with the natural language priors learned from large-scale pre-training.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      43.65940856933594,
      387.44602966308594,
      387.5258026123047,
      580.9108428955078
    ],
    "caption": "notably. Specifically, DenseCLIP can bring ∼ 2.5% single-scale mIoU improvement for ResNet-50/101 with semantic FPN [(<>)21], and ∼ 0.8% improvement for Swin-T/S with UperNet [(<>)45]. These results clearly show that our Dense-CLIP can successfully guide any pre-trained 2D backbone by language priors to boost performance. Since the text encoder can be removed after training, our method provides a low-cost solution to improve arbitrary dense prediction models. Although these performances still lag behind our models with CLIP image encoders, the findings in this section provide a solution to generalize human knowledge learned from large-scale vision-language pre-training to a wider range of models. We expect this could be an interesting direction to connect vision and language researches in the future.",
    "file_name": [
      "figures/fileoutpart22.png"
    ],
    "output_file": "assets/images/paper/2112.01518_DenseCLIP Language-Guided Dense Prediction with Context-Aware Prompting/fig_36.png"
  },
  {
    "page": 7,
    "bbox": [
      43.65940856933594,
      387.44602966308594,
      387.5258026123047,
      580.9108428955078
    ],
    "caption": "notably. Specifically, DenseCLIP can bring ∼ 2.5% single-scale mIoU improvement for ResNet-50/101 with semantic FPN [(<>)21], and ∼ 0.8% improvement for Swin-T/S with UperNet [(<>)45]. These results clearly show that our Dense-CLIP can successfully guide any pre-trained 2D backbone by language priors to boost performance. Since the text encoder can be removed after training, our method provides a low-cost solution to improve arbitrary dense prediction models. Although these performances still lag behind our models with CLIP image encoders, the findings in this section provide a solution to generalize human knowledge learned from large-scale vision-language pre-training to a wider range of models. We expect this could be an interesting direction to connect vision and language researches in the future.",
    "file_name": ""
  }
]