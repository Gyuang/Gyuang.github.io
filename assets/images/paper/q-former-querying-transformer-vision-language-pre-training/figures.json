[
  {
    "page": -1,
    "bbox": [
      306.6732177734375,
      470.4518127441406,
      542.0471649169922,
      577.3828887939453
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/Q-Former-Querying-Transformer-Vision-Language-Pre-training/fig_01.png"
  },
  {
    "page": -1,
    "bbox": [
      306.6732177734375,
      470.4518127441406,
      542.0471649169922,
      577.3828887939453
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      55.76307678222656,
      623.9545440673828,
      541.7538299560547,
      723.7208099365234
    ],
    "caption": "Figure 2. (Left) Model architecture of Q-Former and BLIP-2’s first-stage vision-language representation learning objectives. We jointly optimize three objectives which enforce the queries (a set of learnable embeddings) to extract visual representation most relevant to the text. (Right) The self-attention masking strategy for each objective to control query-text interaction.",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/Q-Former-Querying-Transformer-Vision-Language-Pre-training/fig_03.png"
  },
  {
    "page": 2,
    "bbox": [
      55.76307678222656,
      623.9545440673828,
      541.7538299560547,
      723.7208099365234
    ],
    "caption": "Figure 2. (Left) Model architecture of Q-Former and BLIP-2’s first-stage vision-language representation learning objectives. We jointly optimize three objectives which enforce the queries (a set of learnable embeddings) to extract visual representation most relevant to the text. (Right) The self-attention masking strategy for each objective to control query-text interaction.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      78.80812072753906,
      593.3654632568359,
      516.3906555175781,
      725.2675170898438
    ],
    "caption": "Figure 3. BLIP-2’s second-stage vision-to-language generative pre-training, which bootstraps from frozen large language models (LLMs). (Top) Bootstrapping a decoder-based LLM (e.g. OPT). (Bottom) Bootstrapping an encoder-decoder-based LLM (e.g. FlanT5). The fully-connected layer adapts from the output dimension of the Q-Former to the input dimension of the chosen LLM.",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/Q-Former-Querying-Transformer-Vision-Language-Pre-training/fig_05.png"
  },
  {
    "page": 3,
    "bbox": [
      78.80812072753906,
      593.3654632568359,
      516.3906555175781,
      725.2675170898438
    ],
    "caption": "Figure 3. BLIP-2’s second-stage vision-to-language generative pre-training, which bootstraps from frozen large language models (LLMs). (Top) Bootstrapping a decoder-based LLM (e.g. OPT). (Bottom) Bootstrapping an encoder-decoder-based LLM (e.g. FlanT5). The fully-connected layer adapts from the output dimension of the Q-Former to the input dimension of the chosen LLM.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      41.438079833984375,
      69.73725891113281,
      560.8245086669922,
      725.4990844726562
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/Q-Former-Querying-Transformer-Vision-Language-Pre-training/fig_07.png"
  },
  {
    "page": 4,
    "bbox": [
      41.438079833984375,
      69.73725891113281,
      560.8245086669922,
      725.4990844726562
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      55.33396911621094,
      417.094970703125,
      289.426025390625,
      539.0549163818359
    ],
    "caption": "Figure 5. Effect of vision-language representation learning on vision-to-language generative learning. Without representation learning, the Q-Former fails the bridge the modality gap, leading to significantly lower performance on zero-shot VQA.",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/Q-Former-Querying-Transformer-Vision-Language-Pre-training/fig_09.png"
  },
  {
    "page": 6,
    "bbox": [
      55.33396911621094,
      417.094970703125,
      289.426025390625,
      539.0549163818359
    ],
    "caption": "Figure 5. Effect of vision-language representation learning on vision-to-language generative learning. Without representation learning, the Q-Former fails the bridge the modality gap, leading to significantly lower performance on zero-shot VQA.",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      41.43803405761719,
      97.15145874023438,
      560.8245849609375,
      266.48211669921875
    ],
    "caption": "Table 9. Hyperparameters for fine-tuning BLIP-2 on COCO image-text retrieval.",
    "file_name": [
      "figures/fileoutpart23.png"
    ],
    "output_file": "assets/images/paper/Q-Former-Querying-Transformer-Vision-Language-Pre-training/fig_11.png"
  },
  {
    "page": 11,
    "bbox": [
      41.43803405761719,
      97.15145874023438,
      560.8245849609375,
      266.48211669921875
    ],
    "caption": "Table 9. Hyperparameters for fine-tuning BLIP-2 on COCO image-text retrieval.",
    "file_name": ""
  },
  {
    "page": 12,
    "bbox": [
      55.88433837890625,
      368.6314392089844,
      289.35255432128906,
      499.1151580810547
    ],
    "caption": "Figure 7. Model architecture for VQA finetuning, where the LLM receives Q-Former’s output and the question as input, then predicts answers. We also provide the question as a condition to Q-Former, such that the extracted image features are more relevant to the question.",
    "file_name": [
      "figures/fileoutpart24.png"
    ],
    "output_file": "assets/images/paper/Q-Former-Querying-Transformer-Vision-Language-Pre-training/fig_13.png"
  },
  {
    "page": 12,
    "bbox": [
      55.88433837890625,
      368.6314392089844,
      289.35255432128906,
      499.1151580810547
    ],
    "caption": "Figure 7. Model architecture for VQA finetuning, where the LLM receives Q-Former’s output and the question as input, then predicts answers. We also provide the question as a condition to Q-Former, such that the extracted image features are more relevant to the question.",
    "file_name": ""
  }
]