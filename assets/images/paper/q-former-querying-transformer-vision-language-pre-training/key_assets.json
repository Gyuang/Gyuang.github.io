{
  "architecture": {
    "page": 2,
    "bbox": [
      55.76307678222656,
      623.9545440673828,
      541.7538299560547,
      723.7208099365234
    ],
    "caption": "Figure 2. (Left) Model architecture of Q-Former and BLIP-2’s first-stage vision-language representation learning objectives. We jointly optimize three objectives which enforce the queries (a set of learnable embeddings) to extract visual representation most relevant to the text. (Right) The self-attention masking strategy for each objective to control query-text interaction.",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/Q-Former-Querying-Transformer-Vision-Language-Pre-training/fig_03.png"
  },
  "results": {
    "page": 5,
    "bbox": [
      59.84922790527344,
      627.3142395019531,
      537.0381774902344,
      722.8238525390625
    ],
    "caption": "Table 1. Overview of BLIP-2 results on various zero-shot vision-language tasks. Compared with previous state-of-the-art models. BLIP-2 achieves the highest zero-shot performance while requiring the least number of trainable parameters during vision-language pre-training.",
    "file_name": [
      "tables/fileoutpart4.xlsx",
      "tables/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/Q-Former-Querying-Transformer-Vision-Language-Pre-training/table_01.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/Q-Former-Querying-Transformer-Vision-Language-Pre-training/fig_03.png)\n캡션: Figure 2. (Left) Model architecture of Q-Former and BLIP-2’s first-stage vision-language representation learning objectives. We jointly optimize three objectives which enforce the queries (a set of learnable embeddings) to extract visual representation most relevant to the text. (Right) The self-attention masking strategy for each objective to control query-text interaction.\n\n### Main Results Table\n![Results](/assets/images/paper/Q-Former-Querying-Transformer-Vision-Language-Pre-training/table_01.png)\n캡션: Table 1. Overview of BLIP-2 results on various zero-shot vision-language tasks. Compared with previous state-of-the-art models. BLIP-2 achieves the highest zero-shot performance while requiring the least number of trainable parameters during vision-language pre-training."
}