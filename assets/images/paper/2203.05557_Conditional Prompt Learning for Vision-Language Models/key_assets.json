{
  "architecture": {
    "page": 2,
    "bbox": [
      309.931396484375,
      609.1184539794922,
      546.58349609375,
      720.1232299804688
    ],
    "caption": "Figure 2. Our approach, Conditional Context Optimization (Co-CoOp), consists of two learnable components: a set of context vectors and a lightweight neural network (Meta-Net) that generates for each image an input-conditional token.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/2203.05557_Conditional Prompt Learning for Vision-Language Models/fig_01.png"
  },
  "results": {
    "page": 6,
    "bbox": [
      91.98899841308594,
      559.5844573974609,
      505.4745330810547,
      674.1889495849609
    ],
    "caption": "Table 3. Comparison of manual and learning-based prompts in domain generalization. CoOp and CoCoOp use as training data 16 images from each of the 1,000 classes on ImageNet. In general, CoCoOp is more domain-generalizable than CoOp.",
    "file_name": [
      "tables/fileoutpart32.xlsx",
      "tables/fileoutpart33.png"
    ],
    "output_file": "assets/images/paper/2203.05557_Conditional Prompt Learning for Vision-Language Models/table_421.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/2203.05557_Conditional Prompt Learning for Vision-Language Models/fig_01.png)\n캡션: Figure 2. Our approach, Conditional Context Optimization (Co-CoOp), consists of two learnable components: a set of context vectors and a lightweight neural network (Meta-Net) that generates for each image an input-conditional token.\n\n### Main Results Table\n![Results](/assets/images/paper/2203.05557_Conditional Prompt Learning for Vision-Language Models/table_421.png)\n캡션: Table 3. Comparison of manual and learning-based prompts in domain generalization. CoOp and CoCoOp use as training data 16 images from each of the 1,000 classes on ImageNet. In general, CoCoOp is more domain-generalizable than CoOp."
}