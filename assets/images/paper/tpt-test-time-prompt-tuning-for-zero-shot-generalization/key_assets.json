{
  "architecture": {
    "page": 1,
    "bbox": [
      131.6522216796875,
      548.6652679443359,
      477.9058380126953,
      721.0239868164062
    ],
    "caption": "Figure 1: Test-time Prompt Tuning (TPT) for image classification. We tune adaptive prompts on the fly with a single test sample, without the need for additional training data or annotations. TPT optimizes the prompt to encourage consistent predictions across augmented views by minimizing the marginal entropy. We introduce confidence selection to filter out noisy augmentations.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/TPT-Test-Time-Prompt-Tuning-for-Zero-Shot-Generalization/fig_01.png"
  },
  "results": {
    "page": 8,
    "bbox": [
      112.15640258789062,
      569.4586181640625,
      499.8563232421875,
      674.0710601806641
    ],
    "caption": "Results. In Table (<>)2, we compare TPT with few-shot prompt tuning methods on generalization from ImageNet to fine-grained datasets. Note that TPT works in a zero-shot manner; thus it is not trained on ImageNet. Nonetheless, we find TPT to achieve on-par generalization as ImageNet trained CoCoOp. In Figure (<>)3, we present the results of the more challenging setting of cross-dataset generalization, where there is no overlap between the source and target dataset. For better visualization, we plot the relative accuracy improvement acc = (acc − accbase)/accbase, normalized by the zero-shot baseline accuracy accbase of a CLIP-RN50. For example, baseline CLIP with a hand-crafted prompt achieves 61.75% accuracy on Flower102, while CoOp trained on DTD only has 33.41% on Flower102. In this case, we calculate acc as (33.41 − 61.75)/61.75 = −0.46. From Figure (<>)3, we can see that the averaged accuracy improvement (in the last column of each matrix) of few-shot prompt tuning methods is always negative, meaning that they do worse than the zero-shot baseline. TPT, on the other hand, shows consistent improvement in each of the 10 datasets.",
    "file_name": [
      "tables/fileoutpart14.xlsx",
      "tables/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/TPT-Test-Time-Prompt-Tuning-for-Zero-Shot-Generalization/table_242.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/TPT-Test-Time-Prompt-Tuning-for-Zero-Shot-Generalization/fig_01.png)\n캡션: Figure 1: Test-time Prompt Tuning (TPT) for image classification. We tune adaptive prompts on the fly with a single test sample, without the need for additional training data or annotations. TPT optimizes the prompt to encourage consistent predictions across augmented views by minimizing the marginal entropy. We introduce confidence selection to filter out noisy augmentations.\n\n### Main Results Table\n![Results](/assets/images/paper/TPT-Test-Time-Prompt-Tuning-for-Zero-Shot-Generalization/table_242.png)\n캡션: Results. In Table (<>)2, we compare TPT with few-shot prompt tuning methods on generalization from ImageNet to fine-grained datasets. Note that TPT works in a zero-shot manner; thus it is not trained on ImageNet. Nonetheless, we find TPT to achieve on-par generalization as ImageNet trained CoCoOp. In Figure (<>)3, we present the results of the more challenging setting of cross-dataset generalizati…"
}