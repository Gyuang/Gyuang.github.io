[
  {
    "page": 1,
    "bbox": [
      54.0,
      440.49989318847656,
      549.8999938964844,
      721.4499053955078
    ],
    "caption": "Fig.1. Overview of the proposed adapter-tuning MLLM framework: the image and text encoder are frozen while the trainable Adapter and Linear Projection Layer are updated. It integrates the fine-tuned newly acquired brain disease-related knowledge with the original medical knowledge inside the MLLM for the downstream tasks: brain disease identification.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_01.png"
  },
  {
    "page": 2,
    "bbox": [
      126.0,
      253.86000061035156,
      203.19540405273438,
      269.2799987792969
    ],
    "caption": "The M3Dâ€™s vision encoder adopts a 3D ViT to segment the T1 images into non-overlapping patches of size ğ‘ƒ = 4 Ã— 16 Ã— 16, resulting in patches {ğ‘ƒî€¯}î€²î€¯î€°î€±. Each patch is then mapped into a 256 Ã— 768 dimensional space D, representing 256 tokens with 768 feature dimensions. To minimize the computational cost, we kept the vision encoderâ€™s parameters frozen, updating only the linear projection layer during fine-tuning. The M3D text encoder utilizes LLaMA2-7B, a model that has demonstrated effectiveness in capturing linguistic patterns across various medical domains. The core text encoder module was kept frozen, while only the linear projection layer was updated.",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_02.png"
  },
  {
    "page": 2,
    "bbox": [
      126.0,
      253.86000061035156,
      203.19540405273438,
      269.2799987792969
    ],
    "caption": "The M3Dâ€™s vision encoder adopts a 3D ViT to segment the T1 images into non-overlapping patches of size ğ‘ƒ = 4 Ã— 16 Ã— 16, resulting in patches {ğ‘ƒî€¯}î€²î€¯î€°î€±. Each patch is then mapped into a 256 Ã— 768 dimensional space D, representing 256 tokens with 768 feature dimensions. To minimize the computational cost, we kept the vision encoderâ€™s parameters frozen, updating only the linear projection layer during fine-tuning. The M3D text encoder utilizes LLaMA2-7B, a model that has demonstrated effectiveness in capturing linguistic patterns across various medical domains. The core text encoder module was kept frozen, while only the linear projection layer was updated.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      314.64990234375,
      551.39990234375,
      548.1237030029297,
      721.4499053955078
    ],
    "caption": "Fig.2. Illustration of proposed Brain-Adapter, a lightweight residual-style convolutional structure adapter before the MLLMâ€™s vision encoder.",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_04.png"
  },
  {
    "page": 2,
    "bbox": [
      386.63999938964844,
      375.3000030517578,
      516.010009765625,
      397.44000244140625
    ],
    "caption": "â„’î€³â†’î€¡î€¯ = âˆ’ğ‘™ğ‘œğ‘” î€­î€¦î€« (âŒ©î€³î€¡,î€¡î€¡âŒª/î€») âˆ‘ î€­î€¦î€« (âŒ©î€³î€¡,î€¡î€¢âŒª/î€»)î€£î€¢î€¤î€¥",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_05.png"
  },
  {
    "page": 2,
    "bbox": [
      386.63999938964844,
      375.3000030517578,
      516.010009765625,
      397.44000244140625
    ],
    "caption": "â„’î€³â†’î€¡î€¯ = âˆ’ğ‘™ğ‘œğ‘” î€­î€¦î€« (âŒ©î€³î€¡,î€¡î€¡âŒª/î€») âˆ‘ î€­î€¦î€« (âŒ©î€³î€¡,î€¡î€¢âŒª/î€»)î€£î€¢î€¤î€¥",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      386.63999938964844,
      328.25999450683594,
      516.010009765625,
      350.3999938964844
    ],
    "caption": "âŒ©ğ‘¢, ğ‘£âŒª = î€¡î€¦î€³ â€–î€¡â€–â€–î€³â€–",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_07.png"
  },
  {
    "page": 2,
    "bbox": [
      386.63999938964844,
      328.25999450683594,
      516.010009765625,
      350.3999938964844
    ],
    "caption": "âŒ©ğ‘¢, ğ‘£âŒª = î€¡î€¦î€³ â€–î€¡â€–â€–î€³â€–",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      422.63999938964844,
      281.6999969482422,
      488.0618438720703,
      302.8800048828125
    ],
    "caption": "One of our goals is to align the brain image and the corresponding clinical reports within a common representational space. To achieve this, our method computes a cross-modal contrastive loss. This final loss is computed as a combination of (2) and (3):",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_09.png"
  },
  {
    "page": 2,
    "bbox": [
      422.63999938964844,
      281.6999969482422,
      488.0618438720703,
      302.8800048828125
    ],
    "caption": "One of our goals is to align the brain image and the corresponding clinical reports within a common representational space. To achieve this, our method computes a cross-modal contrastive loss. This final loss is computed as a combination of (2) and (3):",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      350.63999938964844,
      202.25999450683594,
      511.7831268310547,
      221.75999450683594
    ],
    "caption": "One of our goals is to align the brain image and the corresponding clinical reports within a common representational space. To achieve this, our method computes a cross-modal contrastive loss. This final loss is computed as a combination of (2) and (3):",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_11.png"
  },
  {
    "page": 2,
    "bbox": [
      350.63999938964844,
      202.25999450683594,
      511.7831268310547,
      221.75999450683594
    ],
    "caption": "One of our goals is to align the brain image and the corresponding clinical reports within a common representational space. To achieve this, our method computes a cross-modal contrastive loss. This final loss is computed as a combination of (2) and (3):",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      386.63999938964844,
      82.02000427246094,
      512.3596649169922,
      96.0
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_13.png"
  },
  {
    "page": 2,
    "bbox": [
      386.63999938964844,
      82.02000427246094,
      512.3596649169922,
      96.0
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      314.5,
      523.0498962402344,
      549.7857971191406,
      721.4499053955078
    ],
    "caption": "Fig. 3. t-SNE visualization of the embeddings",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_15.png"
  }
]