[
  {
    "page": 1,
    "bbox": [
      54.0,
      440.49989318847656,
      549.8999938964844,
      721.4499053955078
    ],
    "caption": "Fig.1. Overview of the proposed adapter-tuning MLLM framework: the image and text encoder are frozen while the trainable Adapter and Linear Projection Layer are updated. It integrates the fine-tuned newly acquired brain disease-related knowledge with the original medical knowledge inside the MLLM for the downstream tasks: brain disease identification.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_01.png"
  },
  {
    "page": 2,
    "bbox": [
      126.0,
      253.86000061035156,
      203.19540405273438,
      269.2799987792969
    ],
    "caption": "The M3D’s vision encoder adopts a 3D ViT to segment the T1 images into non-overlapping patches of size 𝑃 = 4 × 16 × 16, resulting in patches {𝑃}. Each patch is then mapped into a 256 × 768 dimensional space D, representing 256 tokens with 768 feature dimensions. To minimize the computational cost, we kept the vision encoder’s parameters frozen, updating only the linear projection layer during fine-tuning. The M3D text encoder utilizes LLaMA2-7B, a model that has demonstrated effectiveness in capturing linguistic patterns across various medical domains. The core text encoder module was kept frozen, while only the linear projection layer was updated.",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_02.png"
  },
  {
    "page": 2,
    "bbox": [
      126.0,
      253.86000061035156,
      203.19540405273438,
      269.2799987792969
    ],
    "caption": "The M3D’s vision encoder adopts a 3D ViT to segment the T1 images into non-overlapping patches of size 𝑃 = 4 × 16 × 16, resulting in patches {𝑃}. Each patch is then mapped into a 256 × 768 dimensional space D, representing 256 tokens with 768 feature dimensions. To minimize the computational cost, we kept the vision encoder’s parameters frozen, updating only the linear projection layer during fine-tuning. The M3D text encoder utilizes LLaMA2-7B, a model that has demonstrated effectiveness in capturing linguistic patterns across various medical domains. The core text encoder module was kept frozen, while only the linear projection layer was updated.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      314.64990234375,
      551.39990234375,
      548.1237030029297,
      721.4499053955078
    ],
    "caption": "Fig.2. Illustration of proposed Brain-Adapter, a lightweight residual-style convolutional structure adapter before the MLLM’s vision encoder.",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_04.png"
  },
  {
    "page": 2,
    "bbox": [
      386.63999938964844,
      375.3000030517578,
      516.010009765625,
      397.44000244140625
    ],
    "caption": "ℒ→ = −𝑙𝑜𝑔  (〈,〉/) ∑  (〈,〉/)",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_05.png"
  },
  {
    "page": 2,
    "bbox": [
      386.63999938964844,
      375.3000030517578,
      516.010009765625,
      397.44000244140625
    ],
    "caption": "ℒ→ = −𝑙𝑜𝑔  (〈,〉/) ∑  (〈,〉/)",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      386.63999938964844,
      328.25999450683594,
      516.010009765625,
      350.3999938964844
    ],
    "caption": "〈𝑢, 𝑣〉 =  ‖‖‖‖",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_07.png"
  },
  {
    "page": 2,
    "bbox": [
      386.63999938964844,
      328.25999450683594,
      516.010009765625,
      350.3999938964844
    ],
    "caption": "〈𝑢, 𝑣〉 =  ‖‖‖‖",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      422.63999938964844,
      281.6999969482422,
      488.0618438720703,
      302.8800048828125
    ],
    "caption": "One of our goals is to align the brain image and the corresponding clinical reports within a common representational space. To achieve this, our method computes a cross-modal contrastive loss. This final loss is computed as a combination of (2) and (3):",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_09.png"
  },
  {
    "page": 2,
    "bbox": [
      422.63999938964844,
      281.6999969482422,
      488.0618438720703,
      302.8800048828125
    ],
    "caption": "One of our goals is to align the brain image and the corresponding clinical reports within a common representational space. To achieve this, our method computes a cross-modal contrastive loss. This final loss is computed as a combination of (2) and (3):",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      350.63999938964844,
      202.25999450683594,
      511.7831268310547,
      221.75999450683594
    ],
    "caption": "One of our goals is to align the brain image and the corresponding clinical reports within a common representational space. To achieve this, our method computes a cross-modal contrastive loss. This final loss is computed as a combination of (2) and (3):",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_11.png"
  },
  {
    "page": 2,
    "bbox": [
      350.63999938964844,
      202.25999450683594,
      511.7831268310547,
      221.75999450683594
    ],
    "caption": "One of our goals is to align the brain image and the corresponding clinical reports within a common representational space. To achieve this, our method computes a cross-modal contrastive loss. This final loss is computed as a combination of (2) and (3):",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      386.63999938964844,
      82.02000427246094,
      512.3596649169922,
      96.0
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_13.png"
  },
  {
    "page": 2,
    "bbox": [
      386.63999938964844,
      82.02000427246094,
      512.3596649169922,
      96.0
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      314.5,
      523.0498962402344,
      549.7857971191406,
      721.4499053955078
    ],
    "caption": "Fig. 3. t-SNE visualization of the embeddings",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/Brain-Adapter-Enhancing-Neurological-Disorder-Analysis-with-Adapter-Tuning-Multimodal-Large-Language-Models/fig_15.png"
  }
]