[
  {
    "page": 1,
    "bbox": [
      108.00418090820312,
      292.5819854736328,
      184.49029541015625,
      311.1248779296875
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_01.png"
  },
  {
    "page": 1,
    "bbox": [
      117.83099365234375,
      303.4886016845703,
      127.74783325195312,
      310.35089111328125
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      135.7050018310547,
      304.2626037597656,
      145.62184143066406,
      311.1248779296875
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      135.3822021484375,
      298.6795196533203,
      140.72024536132812,
      304.65711975097656
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      153.57899475097656,
      304.2626037597656,
      163.4898681640625,
      311.1248779296875
    ],
    "caption": "l",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      153.25022888183594,
      297.1014404296875,
      155.65318298339844,
      303.07904052734375
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      167.10400390625,
      303.4886016845703,
      172.88430786132812,
      309.46620178222656
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      167.10400390625,
      298.01312255859375,
      179.01141357421875,
      303.99072265625
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      108.00418090820312,
      292.5819854736328,
      184.49029541015625,
      311.1248779296875
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      213.03799438476562,
      298.6795196533203,
      227.81483459472656,
      311.1248779296875
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_10.png"
  },
  {
    "page": 1,
    "bbox": [
      217.8979949951172,
      304.2626037597656,
      227.81483459472656,
      311.1248779296875
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      217.57521057128906,
      298.6795196533203,
      222.91323852539062,
      304.65711975097656
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      213.03799438476562,
      298.6795196533203,
      227.81483459472656,
      311.1248779296875
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      247.71099853515625,
      297.1014404296875,
      262.4878387451172,
      311.1248779296875
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_14.png"
  },
  {
    "page": 1,
    "bbox": [
      252.5709991455078,
      304.2626037597656,
      262.4878387451172,
      311.1248779296875
    ],
    "caption": "l",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      252.2482147216797,
      297.1014404296875,
      254.6511688232422,
      303.07904052734375
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      247.71099853515625,
      297.1014404296875,
      262.4878387451172,
      311.1248779296875
    ],
    "caption": "LDPO(πθ ; πref) = −E(x,yw ,yl)∼D  log σ  α log πθ (yw |x) πref(yw |x) − α log πθ (yl|x) πref(yl |x)  . (1) where πθ represents the reference policy, which is the LLM fine-tuned through supervised learning.",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      110.06399536132812,
      232.42454528808594,
      506.23828125,
      270.0418243408203
    ],
    "caption": "3 MMED-RAG: A VERSATILE MEDICAL RAG SYSTEM",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_18.png"
  },
  {
    "page": 1,
    "bbox": [
      110.06399536132812,
      232.42454528808594,
      506.23828125,
      270.0418243408203
    ],
    "caption": "3 MMED-RAG: A VERSATILE MEDICAL RAG SYSTEM",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      118.57151794433594,
      521.7513427734375,
      492.5879364013672,
      709.0858154296875
    ],
    "caption": "Figure 1: Overview of MMed-RAG, a versatile factual multimodal RAG system designed to enhance the reliability of Med-LVLMs. It introduces a domain-aware retrieval mechanism that effectively handles different domains of medical images by selecting suitable retrieval models. Additionally, it uses an adaptive context selection approach to determine the optimal number of retrieved contexts and employs preference fine-tuning to improve both cross-modality and overall alignment.",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_20.png"
  },
  {
    "page": 2,
    "bbox": [
      118.57151794433594,
      521.7513427734375,
      492.5879364013672,
      709.0858154296875
    ],
    "caption": "Figure 1: Overview of MMed-RAG, a versatile factual multimodal RAG system designed to enhance the reliability of Med-LVLMs. It introduces a domain-aware retrieval mechanism that effectively handles different domains of medical images by selecting suitable retrieval models. Additionally, it uses an adaptive context selection approach to determine the optimal number of retrieved contexts and employs preference fine-tuning to improve both cross-modality and overall alignment.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      108.0,
      259.7875671386719,
      506.2361145019531,
      303.5834655761719
    ],
    "caption": "V",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_22.png"
  },
  {
    "page": 2,
    "bbox": [
      108.0,
      259.7875671386719,
      506.2361145019531,
      303.5834655761719
    ],
    "caption": "V",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      135.46798706054688,
      242.01951599121094,
      185.2074432373047,
      261.46165466308594
    ],
    "caption": "img",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_24.png"
  },
  {
    "page": 2,
    "bbox": [
      163.86700439453125,
      248.5667266845703,
      185.2074432373047,
      261.46165466308594
    ],
    "caption": "img",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      135.46798706054688,
      242.01951599121094,
      185.2074432373047,
      261.46165466308594
    ],
    "caption": "img",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      107.99948120117188,
      228.5447235107422,
      221.781494140625,
      248.89549255371094
    ],
    "caption": "Finally, for the input image xt, after feeding into the corresponding multimodal retriever Rd(·), the multimodal retriever will retrieves the top-k most similar reports for the image. These retrieved reports xr = Rd(xv) are then provided to the Med-LVLM M(·) as references to guide the generation.",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_27.png"
  },
  {
    "page": 2,
    "bbox": [
      151.0850067138672,
      241.92149353027344,
      155.79246520996094,
      248.89549255371094
    ],
    "caption": "img",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      155.78900146484375,
      241.4237518310547,
      169.29747009277344,
      247.14193725585938
    ],
    "caption": "img",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      155.78900146484375,
      232.93475341796875,
      169.29747009277344,
      238.65293884277344
    ],
    "caption": "Finally, for the input image xt, after feeding into the corresponding multimodal retriever Rd(·), the multimodal retriever will retrieves the top-k most similar reports for the image. These retrieved reports xr = Rd(xv) are then provided to the Med-LVLM M(·) as references to guide the generation.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      193.031005859375,
      240.45574951171875,
      203.8347930908203,
      246.17393493652344
    ],
    "caption": "Finally, for the input image xt, after feeding into the corresponding multimodal retriever Rd(·), the multimodal retriever will retrieves the top-k most similar reports for the image. These retrieved reports xr = Rd(xv) are then provided to the Med-LVLM M(·) as references to guide the generation.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      185.9600067138672,
      228.5447235107422,
      207.1956787109375,
      241.4396514892578
    ],
    "caption": "Finally, for the input image xt, after feeding into the corresponding multimodal retriever Rd(·), the multimodal retriever will retrieves the top-k most similar reports for the image. These retrieved reports xr = Rd(xv) are then provided to the Med-LVLM M(·) as references to guide the generation.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      210.5240020751953,
      240.48350524902344,
      216.97494506835938,
      248.48965454101562
    ],
    "caption": "Finally, for the input image xt, after feeding into the corresponding multimodal retriever Rd(·), the multimodal retriever will retrieves the top-k most similar reports for the image. These retrieved reports xr = Rd(xv) are then provided to the Med-LVLM M(·) as references to guide the generation.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      107.99948120117188,
      228.5447235107422,
      221.781494140625,
      248.89549255371094
    ],
    "caption": "Finally, for the input image xt, after feeding into the corresponding multimodal retriever Rd(·), the multimodal retriever will retrieves the top-k most similar reports for the image. These retrieved reports xr = Rd(xv) are then provided to the Med-LVLM M(·) as references to guide the generation.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      363.41900634765625,
      85.60000610351562,
      502.0261993408203,
      178.0048065185547
    ],
    "caption": "Figure 2: Relations between selected contexts and similarity score.",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_35.png"
  },
  {
    "page": 3,
    "bbox": [
      409.9770050048828,
      86.42518615722656,
      506.49029541015625,
      104.96588134765625
    ],
    "caption": "To address cross-modality misalignment and the overall misalignment introduced by incorporating retrieved knowledge, as shown in Algorithm (<>)1, we propose a RAG-based preference fine-tuning (RAG-PT) approach to fine-tune the target Med-LVLM M(·). Specifically, RAG-PT constructs two types of preference pairs designed to mitigate both categories of misalignment.",
    "file_name": [
      "figures/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_36.png"
  },
  {
    "page": 3,
    "bbox": [
      439.46800231933594,
      98.10360717773438,
      449.3848419189453,
      104.96588134765625
    ],
    "caption": "To address cross-modality misalignment and the overall misalignment introduced by incorporating retrieved knowledge, as shown in Algorithm (<>)1, we propose a RAG-based preference fine-tuning (RAG-PT) approach to fine-tune the target Med-LVLM M(·). Specifically, RAG-PT constructs two types of preference pairs designed to mitigate both categories of misalignment.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      439.46800231933594,
      92.52052307128906,
      443.20399475097656,
      98.49812316894531
    ],
    "caption": "To address cross-modality misalignment and the overall misalignment introduced by incorporating retrieved knowledge, as shown in Algorithm (<>)1, we propose a RAG-based preference fine-tuning (RAG-PT) approach to fine-tune the target Med-LVLM M(·). Specifically, RAG-PT constructs two types of preference pairs designed to mitigate both categories of misalignment.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      457.7050018310547,
      98.10360717773438,
      467.62184143066406,
      104.96588134765625
    ],
    "caption": "To address cross-modality misalignment and the overall misalignment introduced by incorporating retrieved knowledge, as shown in Algorithm (<>)1, we propose a RAG-based preference fine-tuning (RAG-PT) approach to fine-tune the target Med-LVLM M(·). Specifically, RAG-PT constructs two types of preference pairs designed to mitigate both categories of misalignment.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      457.7050018310547,
      91.4146728515625,
      462.05072021484375,
      98.27694702148438
    ],
    "caption": "To address cross-modality misalignment and the overall misalignment introduced by incorporating retrieved knowledge, as shown in Algorithm (<>)1, we propose a RAG-based preference fine-tuning (RAG-PT) approach to fine-tune the target Med-LVLM M(·). Specifically, RAG-PT constructs two types of preference pairs designed to mitigate both categories of misalignment.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      475.57899475097656,
      97.32960510253906,
      484.00140380859375,
      103.30720520019531
    ],
    "caption": "To address cross-modality misalignment and the overall misalignment introduced by incorporating retrieved knowledge, as shown in Algorithm (<>)1, we propose a RAG-based preference fine-tuning (RAG-PT) approach to fine-tune the target Med-LVLM M(·). Specifically, RAG-PT constructs two types of preference pairs designed to mitigate both categories of misalignment.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      489.10499572753906,
      97.32960510253906,
      494.88531494140625,
      103.30720520019531
    ],
    "caption": "To address cross-modality misalignment and the overall misalignment introduced by incorporating retrieved knowledge, as shown in Algorithm (<>)1, we propose a RAG-based preference fine-tuning (RAG-PT) approach to fine-tune the target Med-LVLM M(·). Specifically, RAG-PT constructs two types of preference pairs designed to mitigate both categories of misalignment.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      489.10499572753906,
      91.8541259765625,
      501.0064392089844,
      97.83171081542969
    ],
    "caption": "To address cross-modality misalignment and the overall misalignment introduced by incorporating retrieved knowledge, as shown in Algorithm (<>)1, we propose a RAG-based preference fine-tuning (RAG-PT) approach to fine-tune the target Med-LVLM M(·). Specifically, RAG-PT constructs two types of preference pairs designed to mitigate both categories of misalignment.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      409.9770050048828,
      86.42518615722656,
      506.49029541015625,
      104.96588134765625
    ],
    "caption": "To address cross-modality misalignment and the overall misalignment introduced by incorporating retrieved knowledge, as shown in Algorithm (<>)1, we propose a RAG-based preference fine-tuning (RAG-PT) approach to fine-tune the target Med-LVLM M(·). Specifically, RAG-PT constructs two types of preference pairs designed to mitigate both categories of misalignment.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      97.03897094726562,
      405.1451873779297,
      504.39849853515625,
      707.0600891113281
    ],
    "caption": "response pi, where xr is the retrieved information. A dispreferred response ni is selected from cases where the model makes a correct inference based on an unrelated image, i.e., M(x ∗v, xt) ̸= y, but M(x ∗v, xt + xr) = y, reflecting the model’s reliance on the retrieved knowledge. The unrelated images xv∗ are generated through a two-step process: first, we use the retriever to select an image xv′ with the lowest similarity to the target image; then, we introduce diffusion noise into the selected unrelated image. We define the noise step as s, and the noised image at step s is expressed as:",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_45.png"
  },
  {
    "page": 4,
    "bbox": [
      281.1116027832031,
      451.0664367675781,
      287.8363952636719,
      461.87095642089844
    ],
    "caption": "response pi, where xr is the retrieved information. A dispreferred response ni is selected from cases where the model makes a correct inference based on an unrelated image, i.e., M(x ∗v, xt) ̸= y, but M(x ∗v, xt + xr) = y, reflecting the model’s reliance on the retrieved knowledge. The unrelated images xv∗ are generated through a two-step process: first, we use the retriever to select an image xv′ with the lowest similarity to the target image; then, we introduce diffusion noise into the selected unrelated image. We define the noise step as s, and the noised image at step s is expressed as:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      97.03897094726562,
      405.1451873779297,
      504.39849853515625,
      707.0600891113281
    ],
    "caption": "response pi, where xr is the retrieved information. A dispreferred response ni is selected from cases where the model makes a correct inference based on an unrelated image, i.e., M(x ∗v, xt) ̸= y, but M(x ∗v, xt + xr) = y, reflecting the model’s reliance on the retrieved knowledge. The unrelated images xv∗ are generated through a two-step process: first, we use the retriever to select an image xv′ with the lowest similarity to the target image; then, we introduce diffusion noise into the selected unrelated image. We define the noise step as s, and the noised image at step s is expressed as:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      249.56700134277344,
      270.8880157470703,
      364.67674255371094,
      305.4624481201172
    ],
    "caption": "Preference Pairs for Overall Alignment. Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model’s ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, D1 oa, is designed to strengthen the model’s comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., M(xv, xt + xr) = y, while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., M(xv, xt) ̸= y. Comparing these preferred and dispreferred responses enhances the model’s understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, D2 oa, the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., M(xv, xt) = y, while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., M(xv, xt + xr) ̸= y. This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, Doa = Doa 1 ∪ Doa 2 .",
    "file_name": [
      "figures/fileoutpart11.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_48.png"
  },
  {
    "page": 4,
    "bbox": [
      249.56700134277344,
      270.8880157470703,
      364.67674255371094,
      305.4624481201172
    ],
    "caption": "Preference Pairs for Overall Alignment. Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model’s ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, D1 oa, is designed to strengthen the model’s comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., M(xv, xt + xr) = y, while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., M(xv, xt) ̸= y. Comparing these preferred and dispreferred responses enhances the model’s understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, D2 oa, the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., M(xv, xt) = y, while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., M(xv, xt + xr) ̸= y. This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, Doa = Doa 1 ∪ Doa 2 .",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      136.12442016601562,
      243.35865783691406,
      196.17385864257812,
      280.53907775878906
    ],
    "caption": "Preference Pairs for Overall Alignment. Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model’s ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, D1 oa, is designed to strengthen the model’s comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., M(xv, xt + xr) = y, while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., M(xv, xt) ̸= y. Comparing these preferred and dispreferred responses enhances the model’s understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, D2 oa, the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., M(xv, xt) = y, while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., M(xv, xt + xr) ̸= y. This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, Doa = Doa 1 ∪ Doa 2 .",
    "file_name": [
      "figures/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_50.png"
  },
  {
    "page": 4,
    "bbox": [
      140.4739990234375,
      262.1385498046875,
      145.97633361816406,
      270.14447021484375
    ],
    "caption": "Preference Pairs for Overall Alignment. Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model’s ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, D1 oa, is designed to strengthen the model’s comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., M(xv, xt + xr) = y, while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., M(xv, xt) ̸= y. Comparing these preferred and dispreferred responses enhances the model’s understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, D2 oa, the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., M(xv, xt) = y, while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., M(xv, xt + xr) ̸= y. This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, Doa = Doa 1 ∪ Doa 2 .",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      172.19000244140625,
      268.6415557861328,
      175.94888305664062,
      275.6153564453125
    ],
    "caption": "Preference Pairs for Overall Alignment. Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model’s ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, D1 oa, is designed to strengthen the model’s comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., M(xv, xt + xr) = y, while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., M(xv, xt) ̸= y. Comparing these preferred and dispreferred responses enhances the model’s understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, D2 oa, the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., M(xv, xt) = y, while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., M(xv, xt + xr) ̸= y. This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, Doa = Doa 1 ∪ Doa 2 .",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      172.19000244140625,
      260.6426086425781,
      186.84194946289062,
      268.6485290527344
    ],
    "caption": "Preference Pairs for Overall Alignment. Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model’s ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, D1 oa, is designed to strengthen the model’s comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., M(xv, xt + xr) = y, while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., M(xv, xt) ̸= y. Comparing these preferred and dispreferred responses enhances the model’s understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, D2 oa, the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., M(xv, xt) = y, while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., M(xv, xt + xr) ̸= y. This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, Doa = Doa 1 ∪ Doa 2 .",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      191.61300659179688,
      262.1385498046875,
      196.17385864257812,
      270.14447021484375
    ],
    "caption": "Preference Pairs for Overall Alignment. Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model’s ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, D1 oa, is designed to strengthen the model’s comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., M(xv, xt + xr) = y, while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., M(xv, xt) ̸= y. Comparing these preferred and dispreferred responses enhances the model’s understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, D2 oa, the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., M(xv, xt) = y, while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., M(xv, xt + xr) ̸= y. This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, Doa = Doa 1 ∪ Doa 2 .",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      136.12442016601562,
      243.35865783691406,
      196.17385864257812,
      280.53907775878906
    ],
    "caption": "Preference Pairs for Overall Alignment. Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model’s ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, D1 oa, is designed to strengthen the model’s comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., M(xv, xt + xr) = y, while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., M(xv, xt) ̸= y. Comparing these preferred and dispreferred responses enhances the model’s understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, D2 oa, the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., M(xv, xt) = y, while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., M(xv, xt + xr) ̸= y. This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, Doa = Doa 1 ∪ Doa 2 .",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      216.8688201904297,
      255.81190490722656,
      267.0602722167969,
      274.3224182128906
    ],
    "caption": "Preference Pairs for Overall Alignment. Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model’s ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, D1 oa, is designed to strengthen the model’s comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., M(xv, xt + xr) = y, while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., M(xv, xt) ̸= y. Comparing these preferred and dispreferred responses enhances the model’s understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, D2 oa, the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., M(xv, xt) = y, while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., M(xv, xt + xr) ̸= y. This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, Doa = Doa 1 ∪ Doa 2 .",
    "file_name": [
      "figures/fileoutpart13.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_56.png"
  },
  {
    "page": 4,
    "bbox": [
      221.22900390625,
      262.1385498046875,
      226.7313232421875,
      270.14447021484375
    ],
    "caption": "Preference Pairs for Overall Alignment. Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model’s ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, D1 oa, is designed to strengthen the model’s comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., M(xv, xt + xr) = y, while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., M(xv, xt) ̸= y. Comparing these preferred and dispreferred responses enhances the model’s understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, D2 oa, the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., M(xv, xt) = y, while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., M(xv, xt + xr) ̸= y. This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, Doa = Doa 1 ∪ Doa 2 .",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      216.8688201904297,
      255.81190490722656,
      267.0602722167969,
      274.3224182128906
    ],
    "caption": "Preference Pairs for Overall Alignment. Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model’s ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, D1 oa, is designed to strengthen the model’s comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., M(xv, xt + xr) = y, while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., M(xv, xt) ̸= y. Comparing these preferred and dispreferred responses enhances the model’s understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, D2 oa, the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., M(xv, xt) = y, while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., M(xv, xt + xr) ̸= y. This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, Doa = Doa 1 ∪ Doa 2 .",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      108.00405883789062,
      679.364990234375,
      325.808837890625,
      697.9058837890625
    ],
    "caption": "Lpt = −E(x,yw,o,yl,o)∼D  log σ  αlog πθ (yw,o|x) πo(yw,o|x) − αlog πθ (yl,o|x ∗ ) πo(yl,o|x ∗)  .",
    "file_name": [
      "figures/fileoutpart14.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_59.png"
  },
  {
    "page": 5,
    "bbox": [
      115.11700439453125,
      685.464599609375,
      126.49237060546875,
      692.3268890380859
    ],
    "caption": "4 THEORETICAL ANALYSIS",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      141.53700256347656,
      685.464599609375,
      150.7245635986328,
      692.3268890380859
    ],
    "caption": "4 THEORETICAL ANALYSIS",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      211.45521545410156,
      685.4605255126953,
      222.88441467285156,
      691.4381256103516
    ],
    "caption": "Lpt = −E(x,yw,o,yl,o)∼D  log σ  αlog πθ (yw,o|x) πo(yw,o|x) − αlog πθ (yl,o|x ∗ ) πo(yl,o|x ∗)  .",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      232.01820373535156,
      683.8824310302734,
      241.8453826904297,
      690.7447204589844
    ],
    "caption": "Lpt = −E(x,yw,o,yl,o)∼D  log σ  αlog πθ (yw,o|x) πo(yw,o|x) − αlog πθ (yl,o|x ∗ ) πo(yl,o|x ∗)  .",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      245.86700439453125,
      684.7941131591797,
      257.7744140625,
      690.7717132568359
    ],
    "caption": "Lpt = −E(x,yw,o,yl,o)∼D  log σ  αlog πθ (yw,o|x) πo(yw,o|x) − αlog πθ (yl,o|x ∗ ) πo(yl,o|x ∗)  .",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      294.2742156982422,
      685.4605255126953,
      305.7033996582031,
      691.4381256103516
    ],
    "caption": "Lpt = −E(x,yw,o,yl,o)∼D  log σ  αlog πθ (yw,o|x) πo(yw,o|x) − αlog πθ (yl,o|x ∗ ) πo(yl,o|x ∗)  .",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      315.8919982910156,
      691.0435943603516,
      325.808837890625,
      697.9058837890625
    ],
    "caption": "l,o",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      315.5692138671875,
      683.8824310302734,
      325.39637756347656,
      690.7447204589844
    ],
    "caption": "Lpt = −E(x,yw,o,yl,o)∼D  log σ  αlog πθ (yw,o|x) πo(yw,o|x) − αlog πθ (yl,o|x ∗ ) πo(yl,o|x ∗)  .",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      108.00405883789062,
      679.364990234375,
      325.808837890625,
      697.9058837890625
    ],
    "caption": "Lpt = −E(x,yw,o,yl,o)∼D  log σ  αlog πθ (yw,o|x) πo(yw,o|x) − αlog πθ (yl,o|x ∗ ) πo(yl,o|x ∗)  .",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      170.96800231933594,
      631.2923889160156,
      443.2832794189453,
      665.8668212890625
    ],
    "caption": "In this section, we provide a theoretical analysis of the model obtained from equation (<>)4 and examine how the image input and retrieved context influences the model. Recall that xv, y, xt, xr denotes input medical image, groundtruth answer, question, and retrieved information, respectively.",
    "file_name": [
      "figures/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_69.png"
  },
  {
    "page": 5,
    "bbox": [
      170.96800231933594,
      631.2923889160156,
      443.2832794189453,
      665.8668212890625
    ],
    "caption": "In this section, we provide a theoretical analysis of the model obtained from equation (<>)4 and examine how the image input and retrieved context influences the model. Recall that xv, y, xt, xr denotes input medical image, groundtruth answer, question, and retrieved information, respectively.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      224.52256774902344,
      509.68939208984375,
      331.06260681152344,
      553.9984436035156
    ],
    "caption": "Definition 4.1 Define the weight of xv with respect to log πθ(y|x) as",
    "file_name": [
      "figures/fileoutpart16.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_71.png"
  },
  {
    "page": 5,
    "bbox": [
      224.52256774902344,
      509.68939208984375,
      331.06260681152344,
      553.9984436035156
    ],
    "caption": "Definition 4.1 Define the weight of xv with respect to log πθ(y|x) as",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      166.6269989013672,
      509.68939208984375,
      447.6153259277344,
      544.2638244628906
    ],
    "caption": "Definition 4.1 Define the weight of xv with respect to log πθ(y|x) as",
    "file_name": [
      "figures/fileoutpart17.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_73.png"
  },
  {
    "page": 5,
    "bbox": [
      166.6269989013672,
      509.68939208984375,
      447.6153259277344,
      544.2638244628906
    ],
    "caption": "Definition 4.1 Define the weight of xv with respect to log πθ(y|x) as",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      221.00999450683594,
      439.9923858642578,
      391.9857177734375,
      474.5685119628906
    ],
    "caption": "Lemma 4.1 For linear model y = θ1xv + θ2xt + ϵ such that ϵ ∼ N(0, 1), wt(xv, πθ) = θ12",
    "file_name": [
      "figures/fileoutpart18.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_75.png"
  },
  {
    "page": 5,
    "bbox": [
      221.00999450683594,
      439.9923858642578,
      391.9857177734375,
      474.5685119628906
    ],
    "caption": "Lemma 4.1 For linear model y = θ1xv + θ2xt + ϵ such that ϵ ∼ N(0, 1), wt(xv, πθ) = θ12",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      208.68699645996094,
      337.5814666748047,
      402.8632507324219,
      378.9524383544922
    ],
    "caption": "Assume that wt(xv, πo) < c2 , where",
    "file_name": [
      "figures/fileoutpart19.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_77.png"
  },
  {
    "page": 5,
    "bbox": [
      208.68699645996094,
      337.5814666748047,
      402.8632507324219,
      378.9524383544922
    ],
    "caption": "Assume that wt(xv, πo) < c2 , where",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      152.70700073242188,
      267.36236572265625,
      460.28871154785156,
      321.7005310058594
    ],
    "caption": "Theorem 4.1 Suppose that Assumption (<>)4.1 holds, cross-modality loss increase the weight of xv.",
    "file_name": [
      "figures/fileoutpart20.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_79.png"
  },
  {
    "page": 5,
    "bbox": [
      152.70700073242188,
      267.36236572265625,
      460.28871154785156,
      321.7005310058594
    ],
    "caption": "Theorem 4.1 Suppose that Assumption (<>)4.1 holds, cross-modality loss increase the weight of xv.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      260.98399353027344,
      217.95260620117188,
      353.2601623535156,
      228.79396057128906
    ],
    "caption": "Theorem (<>)4.1 indicates that when the weight of xv is too small in the initial model πo(y|x), the cross-modality loss function adjusts the model to place greater emphasis on images, informed by the retrieved data. Intuitively, for any sample (x, y), generating unrelated images causes the policy to rely less on images. By using samples from this distribution as negative samples, the new model diverges from the initial model, increasing its reliance on images.",
    "file_name": [
      "figures/fileoutpart21.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_81.png"
  },
  {
    "page": 5,
    "bbox": [
      260.98399353027344,
      217.95260620117188,
      353.2601623535156,
      228.79396057128906
    ],
    "caption": "Theorem (<>)4.1 indicates that when the weight of xv is too small in the initial model πo(y|x), the cross-modality loss function adjusts the model to place greater emphasis on images, informed by the retrieved data. Intuitively, for any sample (x, y), generating unrelated images causes the policy to rely less on images. By using samples from this distribution as negative samples, the new model diverges from the initial model, increasing its reliance on images.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      160.31300354003906,
      45.20170593261719,
      454.1790313720703,
      83.63742065429688
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart22.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_83.png"
  },
  {
    "page": 5,
    "bbox": [
      160.31300354003906,
      45.20170593261719,
      454.1790313720703,
      83.63742065429688
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      117.71200561523438,
      526.1044616699219,
      493.8382568359375,
      567.4754333496094
    ],
    "caption": "Assume that wt(xr, πo) < c12 and wt(˜xr, πo) > c22, where",
    "file_name": [
      "figures/fileoutpart23.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_85.png"
  },
  {
    "page": 6,
    "bbox": [
      117.71200561523438,
      526.1044616699219,
      493.8382568359375,
      567.4754333496094
    ],
    "caption": "Assume that wt(xr, πo) < c12 and wt(˜xr, πo) > c22, where",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      160.36099243164062,
      426.1341552734375,
      452.6347198486328,
      511.72557067871094
    ],
    "caption": "wt(xr, πθ) > wt(xr, πo), wt(˜xr, πθ) < wt(˜xr, πo)",
    "file_name": [
      "figures/fileoutpart24.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_87.png"
  },
  {
    "page": 6,
    "bbox": [
      160.36099243164062,
      426.1341552734375,
      452.6347198486328,
      511.72557067871094
    ],
    "caption": "wt(xr, πθ) > wt(xr, πo), wt(˜xr, πθ) < wt(˜xr, πo)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      203.07550048828125,
      395.50755310058594,
      411.4141082763672,
      407.954345703125
    ],
    "caption": "Theorem (<>)4.2 suggests that the model tend to improve the overall alignment. When x˜r generates a false answer, the training procedure tends to reduce the reliance on x˜r, resulting in a decrease in the weight assigned to x˜r. Conversely, if xr is helpful for generating the true answer, πθ(y|x) tend to enhance its use of xr.",
    "file_name": [
      "figures/fileoutpart25.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_89.png"
  },
  {
    "page": 6,
    "bbox": [
      203.07550048828125,
      395.50755310058594,
      411.4141082763672,
      407.954345703125
    ],
    "caption": "Theorem (<>)4.2 suggests that the model tend to improve the overall alignment. When x˜r generates a false answer, the training procedure tends to reduce the reliance on x˜r, resulting in a decrease in the weight assigned to x˜r. Conversely, if xr is helpful for generating the true answer, πθ(y|x) tend to enhance its use of xr.",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      328.64149475097656,
      457.65550231933594,
      501.1595001220703,
      458.45249938964844
    ],
    "caption": "LLaVA-Med-1.5 68.99 10.04 66.63 13.41 79.56 17.92 75.74 17.22 +RAG-PT (Ours) 85.80 29.80 87.18 20.42 nificantly improves the factuality of Med-LVLM, 77.12 13.23 72.69 15.89 with an average performance increase of 17.9% and 16.1% on the IU-Xray and FairVLMed datasets, re-",
    "file_name": [
      "figures/fileoutpart32.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_91.png"
  },
  {
    "page": 8,
    "bbox": [
      108.0,
      399.531494140625,
      501.1595001220703,
      437.21099853515625
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart33.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_92.png"
  },
  {
    "page": 8,
    "bbox": [
      335.01800537109375,
      418.4153594970703,
      350.38128662109375,
      426.81878662109375
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      335.0206756591797,
      410.4443054199219,
      353.87782287597656,
      418.8477325439453
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      108.0,
      399.531494140625,
      501.1595001220703,
      437.21099853515625
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      329.5865020751953,
      286.7955017089844,
      500.21449279785156,
      287.5924987792969
    ],
    "caption": "Ablation Studies. We conduct a series of ablation experiments to evaluate the impact of each compo- Table 4: Ablation results on two datasets nent in MMed-RAG. The results for both medical covering different domains. RG: report gen-VQA and report generation tasks on the IU-Xray and eration, FairVLMed: Harvard-FairVLMed. Harvard-FairVLMed datasets are summarized in Table (<>)4. According to the results, we can see that: (1) Model IU-Xray FairVLMed The domain-aware retrieval mechanism (DR) sig-VQA RG VQA RG spectively. Here, the retrieved knowledge aids the model in generating more factual responses. (2) Building on this, the introduction of adaptive retrieval context selection (RCS) further filters out unreliable retrieved contexts, yielding an additional performance boost of 19.3% and 6.3% on the IU-Xray and FairVLMed datasets. (3) The inclusion of RAG-based preference fine-tuning (RAG-PT) enhances the model’s understanding of the retrieved knowledge, leading to substantial performance gains of 37.1% and 16.9% on the respective datasets. This demonstrates that RAG-PT effectively addresses misalignment issues.",
    "file_name": [
      "figures/fileoutpart34.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_96.png"
  },
  {
    "page": 9,
    "bbox": [
      21.969818115234375,
      465.1340789794922,
      503.35980224609375,
      737.7963714599609
    ],
    "caption": "Retrieval Augmented Generation in Med-LVLMs. Retrieval-Augmented Generation (RAG) has proven to be a powerful technique for enhancing factual accuracy in language modeling ((<>)Gao et al., (<>)2023; (<>)Wu et al., (<>)2023c; (<>)Chen et al., (<>)2024c; (<>)Qu et al., (<>)2024; (<>)Sun et al., (<>)2024a). In the biomedical domain, RAG leverages external knowledge to guide the generation of Med-LVLMs, offering clear advantages in tasks such as medical VQA and report generation ((<>)Yuan et al., (<>)2023; (<>)Kumar (<>)& Marttinen, (<>)2024; (<>)Tao et al., (<>)2024; (<>)He et al., (<>)2024; (<>)Sun et al., (<>)2024b). However, these works mainly focus on enhancing the relevance of the retrieved contexts without considering the model’s understanding of retrieved knowledge. There are several recent work on RAG fine-tuning in LLMs. DPA-RAG ((<>)Dong et al., (<>)2024) addresses the alignment issues between the external reranker and the internal LLM through supervised fine-tuning. Then RAG-DDR ((<>)Li et al., (<>)2024b) leverages a rolling method to generate perturbed responses, further mitigating conflicts between parameter memory and external knowledge. In the biomedical domain, RULE ((<>)Xia et al., (<>)2024c) is proposed to use preference fine-tuning to reduce the model’s over-reliance on retrieved contexts. However, it still overlooks misalignment issues caused by RAG, as well as the generalizability of the retriever given the diverse domains of input images. In response, we propose MMed-RAG to mitigate these risks, enhancing the factuality of Med-LVLMs by addressing these overlooked factors. This can lead to a better cross-modality and overall alignment to enhance the understanding of retrieved knowledge and visual information, ensuring more consistent and reliable performance across tasks.",
    "file_name": [
      "figures/fileoutpart35.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_97.png"
  },
  {
    "page": 9,
    "bbox": [
      21.969818115234375,
      465.1340789794922,
      503.35980224609375,
      737.7963714599609
    ],
    "caption": "Retrieval Augmented Generation in Med-LVLMs. Retrieval-Augmented Generation (RAG) has proven to be a powerful technique for enhancing factual accuracy in language modeling ((<>)Gao et al., (<>)2023; (<>)Wu et al., (<>)2023c; (<>)Chen et al., (<>)2024c; (<>)Qu et al., (<>)2024; (<>)Sun et al., (<>)2024a). In the biomedical domain, RAG leverages external knowledge to guide the generation of Med-LVLMs, offering clear advantages in tasks such as medical VQA and report generation ((<>)Yuan et al., (<>)2023; (<>)Kumar (<>)& Marttinen, (<>)2024; (<>)Tao et al., (<>)2024; (<>)He et al., (<>)2024; (<>)Sun et al., (<>)2024b). However, these works mainly focus on enhancing the relevance of the retrieved contexts without considering the model’s understanding of retrieved knowledge. There are several recent work on RAG fine-tuning in LLMs. DPA-RAG ((<>)Dong et al., (<>)2024) addresses the alignment issues between the external reranker and the internal LLM through supervised fine-tuning. Then RAG-DDR ((<>)Li et al., (<>)2024b) leverages a rolling method to generate perturbed responses, further mitigating conflicts between parameter memory and external knowledge. In the biomedical domain, RULE ((<>)Xia et al., (<>)2024c) is proposed to use preference fine-tuning to reduce the model’s over-reliance on retrieved contexts. However, it still overlooks misalignment issues caused by RAG, as well as the generalizability of the retriever given the diverse domains of input images. In response, we propose MMed-RAG to mitigate these risks, enhancing the factuality of Med-LVLMs by addressing these overlooked factors. This can lead to a better cross-modality and overall alignment to enhance the understanding of retrieved knowledge and visual information, ensuring more consistent and reliable performance across tasks.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      275.6990051269531,
      361.7899932861328,
      279.0679931640625,
      362.18800354003906
    ],
    "caption": "Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew McDermott. Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323, 2019.",
    "file_name": [
      "figures/fileoutpart36.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_99.png"
  },
  {
    "page": 10,
    "bbox": [
      309.7310028076172,
      361.7899932861328,
      313.1000061035156,
      362.18800354003906
    ],
    "caption": "Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew McDermott. Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323, 2019.",
    "file_name": [
      "figures/fileoutpart37.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_100.png"
  },
  {
    "page": 10,
    "bbox": [
      466.1909942626953,
      329.5760040283203,
      472.8979949951172,
      329.9739990234375
    ],
    "caption": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. 2023.",
    "file_name": [
      "figures/fileoutpart38.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_101.png"
  },
  {
    "page": 10,
    "bbox": [
      320.5950012207031,
      243.23199462890625,
      324.0379943847656,
      243.6300048828125
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart39.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_102.png"
  },
  {
    "page": 10,
    "bbox": [
      331.45799255371094,
      243.23199462890625,
      334.9010009765625,
      243.6300048828125
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart40.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_103.png"
  },
  {
    "page": 10,
    "bbox": [
      346.19700622558594,
      243.23199462890625,
      349.63999938964844,
      243.6300048828125
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart41.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_104.png"
  },
  {
    "page": 10,
    "bbox": [
      360.3780059814453,
      243.23199462890625,
      363.8209991455078,
      243.6300048828125
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart42.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_105.png"
  },
  {
    "page": 10,
    "bbox": [
      402.13499450683594,
      243.23199462890625,
      405.5780029296875,
      243.6300048828125
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart43.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_106.png"
  },
  {
    "page": 10,
    "bbox": [
      414.6629943847656,
      243.23199462890625,
      418.1060028076172,
      243.6300048828125
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart44.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_107.png"
  },
  {
    "page": 10,
    "bbox": [
      449.8840026855469,
      243.23199462890625,
      453.3269958496094,
      243.6300048828125
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart45.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_108.png"
  },
  {
    "page": 10,
    "bbox": [
      466.8350067138672,
      243.23199462890625,
      470.2779998779297,
      243.6300048828125
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart46.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_109.png"
  },
  {
    "page": 10,
    "bbox": [
      158.5290069580078,
      232.2729949951172,
      161.8979949951172,
      232.67100524902344
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart47.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_110.png"
  },
  {
    "page": 10,
    "bbox": [
      198.08999633789062,
      232.2729949951172,
      201.45899963378906,
      232.67100524902344
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart48.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_111.png"
  },
  {
    "page": 10,
    "bbox": [
      212.19700622558594,
      232.2729949951172,
      215.5659942626953,
      232.67100524902344
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart49.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_112.png"
  },
  {
    "page": 10,
    "bbox": [
      248.4409942626953,
      232.2729949951172,
      251.80999755859375,
      232.67100524902344
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart50.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_113.png"
  },
  {
    "page": 10,
    "bbox": [
      292.9929962158203,
      232.2729949951172,
      296.36199951171875,
      232.67100524902344
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart51.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_114.png"
  },
  {
    "page": 10,
    "bbox": [
      320.93800354003906,
      232.2729949951172,
      324.3070068359375,
      232.67100524902344
    ],
    "caption": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024a.",
    "file_name": [
      "figures/fileoutpart52.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_115.png"
  },
  {
    "page": 10,
    "bbox": [
      262.3489990234375,
      189.10000610351562,
      265.71800231933594,
      189.4980010986328
    ],
    "caption": "Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024b.",
    "file_name": [
      "figures/fileoutpart53.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_116.png"
  },
  {
    "page": 10,
    "bbox": [
      296.38099670410156,
      189.10000610351562,
      299.75,
      189.4980010986328
    ],
    "caption": "Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024b.",
    "file_name": [
      "figures/fileoutpart54.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_117.png"
  },
  {
    "page": 10,
    "bbox": [
      363.5050048828125,
      145.92799377441406,
      366.38299560546875,
      146.3260040283203
    ],
    "caption": "Zhanpeng Chen, Chengjin Xu, Yiyan Qi, and Jian Guo. Mllm is a strong reranker: Advancing multimodal retrieval-augmented generation via knowledge-enhanced reranking and noise-injected training. arXiv preprint arXiv:2407.21439, 2024c.",
    "file_name": [
      "figures/fileoutpart55.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_118.png"
  },
  {
    "page": 10,
    "bbox": [
      397.0469970703125,
      145.92799377441406,
      399.9250030517578,
      146.3260040283203
    ],
    "caption": "Zhanpeng Chen, Chengjin Xu, Yiyan Qi, and Jian Guo. Mllm is a strong reranker: Advancing multimodal retrieval-augmented generation via knowledge-enhanced reranking and noise-injected training. arXiv preprint arXiv:2407.21439, 2024c.",
    "file_name": [
      "figures/fileoutpart56.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_119.png"
  },
  {
    "page": 10,
    "bbox": [
      177.02999877929688,
      102.75599670410156,
      180.3990020751953,
      103.15400695800781
    ],
    "caption": "Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to com-mercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024d.",
    "file_name": [
      "figures/fileoutpart57.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_120.png"
  },
  {
    "page": 10,
    "bbox": [
      211.06199645996094,
      102.75599670410156,
      214.43099975585938,
      103.15400695800781
    ],
    "caption": "Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to com-mercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024d.",
    "file_name": [
      "figures/fileoutpart58.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_121.png"
  },
  {
    "page": 10,
    "bbox": [
      352.1909942626953,
      59.58299255371094,
      355.55999755859375,
      59.98100280761719
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart59.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_122.png"
  },
  {
    "page": 10,
    "bbox": [
      386.22300720214844,
      59.58299255371094,
      389.5919952392578,
      59.98100280761719
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart60.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_123.png"
  },
  {
    "page": 11,
    "bbox": [
      284.16700744628906,
      644.8560028076172,
      287.53599548339844,
      645.2539978027344
    ],
    "caption": "Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. arXiv preprint arXiv:2309.03883, 2023.",
    "file_name": [
      "figures/fileoutpart61.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_124.png"
  },
  {
    "page": 11,
    "bbox": [
      318.1990051269531,
      644.8560028076172,
      321.5679931640625,
      645.2539978027344
    ],
    "caption": "Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. arXiv preprint arXiv:2309.03883, 2023.",
    "file_name": [
      "figures/fileoutpart62.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_125.png"
  },
  {
    "page": 11,
    "bbox": [
      468.18299865722656,
      614.0099945068359,
      472.8979949951172,
      614.4080047607422
    ],
    "caption": "Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman, Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and Clement J McDonald. Preparing a collection of radiol-ogy examinations for distribution and retrieval. Journal of the American Medical Informatics Association, 23(2):304–310, 2016.",
    "file_name": [
      "figures/fileoutpart63.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_126.png"
  },
  {
    "page": 11,
    "bbox": [
      347.08799743652344,
      561.2460021972656,
      351.5950012207031,
      561.6439971923828
    ],
    "caption": "Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen. Understand what llm needs: Dual preference alignment for retrieval-augmented generation. arXiv preprint arXiv:2406.18676, 2024.",
    "file_name": [
      "figures/fileoutpart64.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_127.png"
  },
  {
    "page": 11,
    "bbox": [
      359.01499938964844,
      561.2460021972656,
      363.5220031738281,
      561.6439971923828
    ],
    "caption": "Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen. Understand what llm needs: Dual preference alignment for retrieval-augmented generation. arXiv preprint arXiv:2406.18676, 2024.",
    "file_name": [
      "figures/fileoutpart65.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_128.png"
  },
  {
    "page": 11,
    "bbox": [
      374.8179931640625,
      561.2460021972656,
      379.3249969482422,
      561.6439971923828
    ],
    "caption": "Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen. Understand what llm needs: Dual preference alignment for retrieval-augmented generation. arXiv preprint arXiv:2406.18676, 2024.",
    "file_name": [
      "figures/fileoutpart66.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_129.png"
  },
  {
    "page": 11,
    "bbox": [
      417.72900390625,
      561.2460021972656,
      422.2360076904297,
      561.6439971923828
    ],
    "caption": "Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen. Understand what llm needs: Dual preference alignment for retrieval-augmented generation. arXiv preprint arXiv:2406.18676, 2024.",
    "file_name": [
      "figures/fileoutpart67.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_130.png"
  },
  {
    "page": 11,
    "bbox": [
      454.0050048828125,
      561.2460021972656,
      458.5119934082031,
      561.6439971923828
    ],
    "caption": "Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen. Understand what llm needs: Dual preference alignment for retrieval-augmented generation. arXiv preprint arXiv:2406.18676, 2024.",
    "file_name": [
      "figures/fileoutpart68.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_131.png"
  },
  {
    "page": 11,
    "bbox": [
      149.06500244140625,
      508.48199462890625,
      152.4340057373047,
      508.8800048828125
    ],
    "caption": "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.",
    "file_name": [
      "figures/fileoutpart69.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_132.png"
  },
  {
    "page": 11,
    "bbox": [
      149.06500244140625,
      466.677001953125,
      152.4340057373047,
      467.0749969482422
    ],
    "caption": "Pierre Gravel, Gilles Beaudoin, and Jacques A De Guise. A method for modeling noise in medical images. IEEE Transactions on medical imaging, 23(10):1221–1232, 2004.",
    "file_name": [
      "figures/fileoutpart70.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_133.png"
  },
  {
    "page": 11,
    "bbox": [
      173.4040069580078,
      435.8300018310547,
      176.7729949951172,
      436.2279968261719
    ],
    "caption": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.",
    "file_name": [
      "figures/fileoutpart71.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_134.png"
  },
  {
    "page": 11,
    "bbox": [
      226.45399475097656,
      435.8300018310547,
      229.822998046875,
      436.2279968261719
    ],
    "caption": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.",
    "file_name": [
      "figures/fileoutpart72.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_135.png"
  },
  {
    "page": 11,
    "bbox": [
      238.90699768066406,
      435.8300018310547,
      242.2760009765625,
      436.2279968261719
    ],
    "caption": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.",
    "file_name": [
      "figures/fileoutpart73.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_136.png"
  },
  {
    "page": 11,
    "bbox": [
      272.9389953613281,
      435.8300018310547,
      276.30799865722656,
      436.2279968261719
    ],
    "caption": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.",
    "file_name": [
      "figures/fileoutpart74.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_137.png"
  },
  {
    "page": 11,
    "bbox": [
      206.3300018310547,
      404.98399353027344,
      209.6959991455078,
      405.3820037841797
    ],
    "caption": "Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024.",
    "file_name": [
      "figures/fileoutpart75.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_138.png"
  },
  {
    "page": 11,
    "bbox": [
      217.11700439453125,
      404.98399353027344,
      220.48300170898438,
      405.3820037841797
    ],
    "caption": "Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024.",
    "file_name": [
      "figures/fileoutpart76.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_139.png"
  },
  {
    "page": 11,
    "bbox": [
      231.7790069580078,
      404.98399353027344,
      235.14500427246094,
      405.3820037841797
    ],
    "caption": "Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024.",
    "file_name": [
      "figures/fileoutpart77.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_140.png"
  },
  {
    "page": 11,
    "bbox": [
      255.84500122070312,
      404.98399353027344,
      259.21099853515625,
      405.3820037841797
    ],
    "caption": "Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024.",
    "file_name": [
      "figures/fileoutpart78.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_141.png"
  },
  {
    "page": 11,
    "bbox": [
      304.25,
      404.98399353027344,
      307.6159973144531,
      405.3820037841797
    ],
    "caption": "Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024.",
    "file_name": [
      "figures/fileoutpart79.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_142.png"
  },
  {
    "page": 11,
    "bbox": [
      316.7010040283203,
      404.98399353027344,
      320.06700134277344,
      405.3820037841797
    ],
    "caption": "Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024.",
    "file_name": [
      "figures/fileoutpart80.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_143.png"
  },
  {
    "page": 11,
    "bbox": [
      359.03900146484375,
      404.98399353027344,
      362.4049987792969,
      405.3820037841797
    ],
    "caption": "Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024.",
    "file_name": [
      "figures/fileoutpart81.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_144.png"
  },
  {
    "page": 11,
    "bbox": [
      387.4989929199219,
      404.98399353027344,
      390.86500549316406,
      405.3820037841797
    ],
    "caption": "Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024.",
    "file_name": [
      "figures/fileoutpart82.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_145.png"
  },
  {
    "page": 11,
    "bbox": [
      404.3719940185547,
      404.98399353027344,
      407.7380065917969,
      405.3820037841797
    ],
    "caption": "Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024.",
    "file_name": [
      "figures/fileoutpart83.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_146.png"
  },
  {
    "page": 11,
    "bbox": [
      434.9340057373047,
      404.98399353027344,
      438.3000030517578,
      405.3820037841797
    ],
    "caption": "Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024.",
    "file_name": [
      "figures/fileoutpart84.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_147.png"
  },
  {
    "page": 11,
    "bbox": [
      149.06500244140625,
      352.2200012207031,
      152.4340057373047,
      352.6179962158203
    ],
    "caption": "Robbie Holland, Thomas RP Taylor, Christopher Holmes, Sophie Riedl, Julia Mai, Maria Patsia-manidi, Dimitra Mitsopoulou, Paul Hager, Philip Muller,¨ Hendrik PN Scholl, et al. Specialist vision-language models for clinical ophthalmology. arXiv preprint arXiv:2407.08410, 2024.",
    "file_name": [
      "figures/fileoutpart85.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_148.png"
  },
  {
    "page": 11,
    "bbox": [
      349.05299377441406,
      310.4149932861328,
      352.4219970703125,
      310.81300354003906
    ],
    "caption": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.",
    "file_name": [
      "figures/fileoutpart86.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_149.png"
  },
  {
    "page": 11,
    "bbox": [
      383.0850067138672,
      310.4149932861328,
      386.45399475097656,
      310.81300354003906
    ],
    "caption": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.",
    "file_name": [
      "figures/fileoutpart87.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_150.png"
  },
  {
    "page": 11,
    "bbox": [
      467.30999755859375,
      279.5679931640625,
      472.8979949951172,
      279.96600341796875
    ],
    "caption": "Ming Hu, Peng Xia, Lin Wang, Siyuan Yan, Feilong Tang, Zhongxing Xu, Yimin Luo, Kaimin Song, Jurgen Leitner, Xuelian Cheng, et al. Ophnet: A large-scale video benchmark for ophthalmic surgical workflow understanding. In European Conference on Computer Vision, pp. 481–500. Springer, 2024.",
    "file_name": [
      "figures/fileoutpart88.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_151.png"
  },
  {
    "page": 11,
    "bbox": [
      308.8800048828125,
      226.8040008544922,
      313.1649932861328,
      227.20199584960938
    ],
    "caption": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. arXiv preprint arXiv:2311.17911, 2023.",
    "file_name": [
      "figures/fileoutpart89.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_152.png"
  },
  {
    "page": 11,
    "bbox": [
      358.2050018310547,
      226.8040008544922,
      362.49000549316406,
      227.20199584960938
    ],
    "caption": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. arXiv preprint arXiv:2311.17911, 2023.",
    "file_name": [
      "figures/fileoutpart90.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_153.png"
  },
  {
    "page": 11,
    "bbox": [
      371.5760040283203,
      226.8040008544922,
      375.8610076904297,
      227.20199584960938
    ],
    "caption": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. arXiv preprint arXiv:2311.17911, 2023.",
    "file_name": [
      "figures/fileoutpart91.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_154.png"
  },
  {
    "page": 11,
    "bbox": [
      414.83299255371094,
      226.8040008544922,
      419.1179962158203,
      227.20199584960938
    ],
    "caption": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. arXiv preprint arXiv:2311.17911, 2023.",
    "file_name": [
      "figures/fileoutpart92.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_155.png"
  },
  {
    "page": 11,
    "bbox": [
      344.7489929199219,
      174.0399932861328,
      348.1179962158203,
      174.43800354003906
    ],
    "caption": "Wisdom Ikezogwo, Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. Advances in neural information processing systems, 36, 2024.",
    "file_name": [
      "figures/fileoutpart93.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_156.png"
  },
  {
    "page": 11,
    "bbox": [
      378.781005859375,
      174.0399932861328,
      382.1499938964844,
      174.43800354003906
    ],
    "caption": "Wisdom Ikezogwo, Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. Advances in neural information processing systems, 36, 2024.",
    "file_name": [
      "figures/fileoutpart94.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_157.png"
  },
  {
    "page": 11,
    "bbox": [
      257.1790008544922,
      132.23500061035156,
      260.5480041503906,
      132.63299560546875
    ],
    "caption": "Yue Jiang, Jiawei Chen, Dingkang Yang, Mingcheng Li, Shunli Wang, Tong Wu, Ke Li, and Lihua Zhang. Medthink: Inducing medical large-scale visual language models to hallucinate less by thinking more. arXiv preprint arXiv:2406.11451, 2024.",
    "file_name": [
      "figures/fileoutpart95.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_158.png"
  },
  {
    "page": 11,
    "bbox": [
      267.4199981689453,
      132.23500061035156,
      270.78900146484375,
      132.63299560546875
    ],
    "caption": "Yue Jiang, Jiawei Chen, Dingkang Yang, Mingcheng Li, Shunli Wang, Tong Wu, Ke Li, and Lihua Zhang. Medthink: Inducing medical large-scale visual language models to hallucinate less by thinking more. arXiv preprint arXiv:2406.11451, 2024.",
    "file_name": [
      "figures/fileoutpart96.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_159.png"
  },
  {
    "page": 11,
    "bbox": [
      294.8070068359375,
      132.23500061035156,
      298.1759948730469,
      132.63299560546875
    ],
    "caption": "Yue Jiang, Jiawei Chen, Dingkang Yang, Mingcheng Li, Shunli Wang, Tong Wu, Ke Li, and Lihua Zhang. Medthink: Inducing medical large-scale visual language models to hallucinate less by thinking more. arXiv preprint arXiv:2406.11451, 2024.",
    "file_name": [
      "figures/fileoutpart97.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_160.png"
  },
  {
    "page": 11,
    "bbox": [
      344.34100341796875,
      132.23500061035156,
      347.7100067138672,
      132.63299560546875
    ],
    "caption": "Yue Jiang, Jiawei Chen, Dingkang Yang, Mingcheng Li, Shunli Wang, Tong Wu, Ke Li, and Lihua Zhang. Medthink: Inducing medical large-scale visual language models to hallucinate less by thinking more. arXiv preprint arXiv:2406.11451, 2024.",
    "file_name": [
      "figures/fileoutpart98.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_161.png"
  },
  {
    "page": 11,
    "bbox": [
      389.4409942626953,
      132.23500061035156,
      392.80999755859375,
      132.63299560546875
    ],
    "caption": "Yue Jiang, Jiawei Chen, Dingkang Yang, Mingcheng Li, Shunli Wang, Tong Wu, Ke Li, and Lihua Zhang. Medthink: Inducing medical large-scale visual language models to hallucinate less by thinking more. arXiv preprint arXiv:2406.11451, 2024.",
    "file_name": [
      "figures/fileoutpart99.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_162.png"
  },
  {
    "page": 11,
    "bbox": [
      202.21499633789062,
      90.42999267578125,
      205.58399963378906,
      90.8280029296875
    ],
    "caption": "Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports. arXiv preprint arXiv:1711.08195, 2017.",
    "file_name": [
      "figures/fileoutpart100.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_163.png"
  },
  {
    "page": 11,
    "bbox": [
      236.2469940185547,
      90.42999267578125,
      239.61599731445312,
      90.8280029296875
    ],
    "caption": "Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports. arXiv preprint arXiv:1711.08195, 2017.",
    "file_name": [
      "figures/fileoutpart101.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_164.png"
  },
  {
    "page": 11,
    "bbox": [
      139.95899963378906,
      59.58299255371094,
      143.3280029296875,
      59.98100280761719
    ],
    "caption": "Yue Jiang, Jiawei Chen, Dingkang Yang, Mingcheng Li, Shunli Wang, Tong Wu, Ke Li, and Lihua Zhang. Medthink: Inducing medical large-scale visual language models to hallucinate less by thinking more. arXiv preprint arXiv:2406.11451, 2024.",
    "file_name": [
      "figures/fileoutpart102.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_165.png"
  },
  {
    "page": 11,
    "bbox": [
      173.99099731445312,
      59.58299255371094,
      177.36000061035156,
      59.98100280761719
    ],
    "caption": "Yue Jiang, Jiawei Chen, Dingkang Yang, Mingcheng Li, Shunli Wang, Tong Wu, Ke Li, and Lihua Zhang. Medthink: Inducing medical large-scale visual language models to hallucinate less by thinking more. arXiv preprint arXiv:2406.11451, 2024.",
    "file_name": [
      "figures/fileoutpart103.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_166.png"
  },
  {
    "page": 12,
    "bbox": [
      391.7310028076172,
      675.7030029296875,
      395.24400329589844,
      676.1009979248047
    ],
    "caption": "Lie Ju, Yukun Zhou, Peng Xia, Daniel Alexander, Pearse Andrew Keane, and Zongyuan Ge. Explore vision-language model with hierarchical information for multiple retinal disease recognition. Investigative Ophthalmology & Visual Science, 65(7):1593–1593, 2024.",
    "file_name": [
      "figures/fileoutpart104.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_167.png"
  },
  {
    "page": 12,
    "bbox": [
      425.9080047607422,
      675.7030029296875,
      429.42100524902344,
      676.1009979248047
    ],
    "caption": "Lie Ju, Yukun Zhou, Peng Xia, Daniel Alexander, Pearse Andrew Keane, and Zongyuan Ge. Explore vision-language model with hierarchical information for multiple retinal disease recognition. Investigative Ophthalmology & Visual Science, 65(7):1593–1593, 2024.",
    "file_name": [
      "figures/fileoutpart105.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_168.png"
  },
  {
    "page": 12,
    "bbox": [
      189.572998046875,
      624.3999938964844,
      192.94200134277344,
      624.7980041503906
    ],
    "caption": "Yogesh Kumar and Pekka Marttinen. Improving medical multi-modal contrastive learning with expert annotations. arXiv preprint arXiv:2403.10153, 2024.",
    "file_name": [
      "figures/fileoutpart106.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_169.png"
  },
  {
    "page": 12,
    "bbox": [
      254.60800170898438,
      624.3999938964844,
      257.9770050048828,
      624.7980041503906
    ],
    "caption": "Yogesh Kumar and Pekka Marttinen. Improving medical multi-modal contrastive learning with expert annotations. arXiv preprint arXiv:2403.10153, 2024.",
    "file_name": [
      "figures/fileoutpart107.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_170.png"
  },
  {
    "page": 12,
    "bbox": [
      264.8500061035156,
      624.3999938964844,
      268.218994140625,
      624.7980041503906
    ],
    "caption": "Yogesh Kumar and Pekka Marttinen. Improving medical multi-modal contrastive learning with expert annotations. arXiv preprint arXiv:2403.10153, 2024.",
    "file_name": [
      "figures/fileoutpart108.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_171.png"
  },
  {
    "page": 12,
    "bbox": [
      292.7550048828125,
      624.3999938964844,
      296.1239929199219,
      624.7980041503906
    ],
    "caption": "Yogesh Kumar and Pekka Marttinen. Improving medical multi-modal contrastive learning with expert annotations. arXiv preprint arXiv:2403.10153, 2024.",
    "file_name": [
      "figures/fileoutpart109.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_172.png"
  },
  {
    "page": 12,
    "bbox": [
      219.21099853515625,
      595.0149993896484,
      222.5800018310547,
      595.4129943847656
    ],
    "caption": "Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13872–13882, 2024.",
    "file_name": [
      "figures/fileoutpart110.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_173.png"
  },
  {
    "page": 12,
    "bbox": [
      253.2429962158203,
      595.0149993896484,
      256.61199951171875,
      595.4129943847656
    ],
    "caption": "Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13872–13882, 2024.",
    "file_name": [
      "figures/fileoutpart111.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_174.png"
  },
  {
    "page": 12,
    "bbox": [
      251.67300415039062,
      554.6710052490234,
      255.1649932861328,
      555.0690002441406
    ],
    "caption": "Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau-mann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023a.",
    "file_name": [
      "figures/fileoutpart112.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_175.png"
  },
  {
    "page": 12,
    "bbox": [
      262.58599853515625,
      554.6710052490234,
      266.0780029296875,
      555.0690002441406
    ],
    "caption": "Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau-mann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023a.",
    "file_name": [
      "figures/fileoutpart113.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_176.png"
  },
  {
    "page": 12,
    "bbox": [
      277.3739929199219,
      554.6710052490234,
      280.8659973144531,
      555.0690002441406
    ],
    "caption": "Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau-mann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023a.",
    "file_name": [
      "figures/fileoutpart114.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_177.png"
  },
  {
    "page": 12,
    "bbox": [
      323.71400451660156,
      554.6710052490234,
      327.20599365234375,
      555.0690002441406
    ],
    "caption": "Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau-mann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023a.",
    "file_name": [
      "figures/fileoutpart115.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_178.png"
  },
  {
    "page": 12,
    "bbox": [
      372.2449951171875,
      554.6710052490234,
      375.73699951171875,
      555.0690002441406
    ],
    "caption": "Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau-mann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023a.",
    "file_name": [
      "figures/fileoutpart116.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_179.png"
  },
  {
    "page": 12,
    "bbox": [
      384.82200622558594,
      554.6710052490234,
      388.3139953613281,
      555.0690002441406
    ],
    "caption": "Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau-mann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023a.",
    "file_name": [
      "figures/fileoutpart117.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_180.png"
  },
  {
    "page": 12,
    "bbox": [
      427.28599548339844,
      554.6710052490234,
      430.7779998779297,
      555.0690002441406
    ],
    "caption": "Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau-mann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023a.",
    "file_name": [
      "figures/fileoutpart118.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_181.png"
  },
  {
    "page": 12,
    "bbox": [
      455.8730010986328,
      554.6710052490234,
      459.36500549316406,
      555.0690002441406
    ],
    "caption": "Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau-mann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023a.",
    "file_name": [
      "figures/fileoutpart119.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_182.png"
  },
  {
    "page": 12,
    "bbox": [
      472.8730010986328,
      554.6710052490234,
      476.36500549316406,
      555.0690002441406
    ],
    "caption": "Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau-mann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023a.",
    "file_name": [
      "figures/fileoutpart120.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_183.png"
  },
  {
    "page": 12,
    "bbox": [
      317.16200256347656,
      503.3679962158203,
      320.5709991455078,
      503.76600646972656
    ],
    "caption": "Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, and Huaxiu Yao. Lite: Modeling environmental ecosystems with multimodal large language models. arXiv preprint arXiv:2404.01165, 2024a.",
    "file_name": [
      "figures/fileoutpart121.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_184.png"
  },
  {
    "page": 12,
    "bbox": [
      365.61000061035156,
      503.3679962158203,
      369.0189971923828,
      503.76600646972656
    ],
    "caption": "Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, and Huaxiu Yao. Lite: Modeling environmental ecosystems with multimodal large language models. arXiv preprint arXiv:2404.01165, 2024a.",
    "file_name": [
      "figures/fileoutpart122.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_185.png"
  },
  {
    "page": 12,
    "bbox": [
      378.1029968261719,
      503.3679962158203,
      381.5119934082031,
      503.76600646972656
    ],
    "caption": "Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, and Huaxiu Yao. Lite: Modeling environmental ecosystems with multimodal large language models. arXiv preprint arXiv:2404.01165, 2024a.",
    "file_name": [
      "figures/fileoutpart123.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_186.png"
  },
  {
    "page": 12,
    "bbox": [
      407.7409973144531,
      503.3679962158203,
      411.1499938964844,
      503.76600646972656
    ],
    "caption": "Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, and Huaxiu Yao. Lite: Modeling environmental ecosystems with multimodal large language models. arXiv preprint arXiv:2404.01165, 2024a.",
    "file_name": [
      "figures/fileoutpart124.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_187.png"
  },
  {
    "page": 12,
    "bbox": [
      457.8630065917969,
      503.3679962158203,
      461.2720031738281,
      503.76600646972656
    ],
    "caption": "Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, and Huaxiu Yao. Lite: Modeling environmental ecosystems with multimodal large language models. arXiv preprint arXiv:2404.01165, 2024a.",
    "file_name": [
      "figures/fileoutpart125.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_188.png"
  },
  {
    "page": 12,
    "bbox": [
      150.73899841308594,
      492.40899658203125,
      154.10800170898438,
      492.8070068359375
    ],
    "caption": "Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, and Huaxiu Yao. Lite: Modeling environmental ecosystems with multimodal large language models. arXiv preprint arXiv:2404.01165, 2024a.",
    "file_name": [
      "figures/fileoutpart126.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_189.png"
  },
  {
    "page": 12,
    "bbox": [
      186.98199462890625,
      492.40899658203125,
      190.3509979248047,
      492.8070068359375
    ],
    "caption": "Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, and Huaxiu Yao. Lite: Modeling environmental ecosystems with multimodal large language models. arXiv preprint arXiv:2404.01165, 2024a.",
    "file_name": [
      "figures/fileoutpart127.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_190.png"
  },
  {
    "page": 12,
    "bbox": [
      203.85899353027344,
      492.40899658203125,
      207.22799682617188,
      492.8070068359375
    ],
    "caption": "Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, and Huaxiu Yao. Lite: Modeling environmental ecosystems with multimodal large language models. arXiv preprint arXiv:2404.01165, 2024a.",
    "file_name": [
      "figures/fileoutpart128.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_191.png"
  },
  {
    "page": 12,
    "bbox": [
      256.15199279785156,
      492.40899658203125,
      259.52099609375,
      492.8070068359375
    ],
    "caption": "Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, and Huaxiu Yao. Lite: Modeling environmental ecosystems with multimodal large language models. arXiv preprint arXiv:2404.01165, 2024a.",
    "file_name": [
      "figures/fileoutpart129.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_192.png"
  },
  {
    "page": 12,
    "bbox": [
      468.04400634765625,
      463.0240020751953,
      472.8979949951172,
      463.4219970703125
    ],
    "caption": "Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, et al. Rag-ddr: Optimizing retrieval-augmented generation using differentiable data rewards. arXiv preprint arXiv:2410.13509, 2024b.",
    "file_name": [
      "figures/fileoutpart130.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_193.png"
  },
  {
    "page": 12,
    "bbox": [
      177.218994140625,
      411.7209930419922,
      180.58799743652344,
      412.11900329589844
    ],
    "caption": "Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, and Luping Zhou. A comprehensive study of gpt-4v’s multimodal capabilities in medical imaging. arXiv preprint arXiv:2310.20381, 2023b.",
    "file_name": [
      "figures/fileoutpart131.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_194.png"
  },
  {
    "page": 12,
    "bbox": [
      211.25100708007812,
      411.7209930419922,
      214.6199951171875,
      412.11900329589844
    ],
    "caption": "Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, and Luping Zhou. A comprehensive study of gpt-4v’s multimodal capabilities in medical imaging. arXiv preprint arXiv:2310.20381, 2023b.",
    "file_name": [
      "figures/fileoutpart132.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_195.png"
  },
  {
    "page": 12,
    "bbox": [
      222.9669952392578,
      371.3769989013672,
      226.33599853515625,
      371.7749938964844
    ],
    "caption": "Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74–81, 2004.",
    "file_name": [
      "figures/fileoutpart133.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_196.png"
  },
  {
    "page": 12,
    "bbox": [
      256.9989929199219,
      371.3769989013672,
      260.3679962158203,
      371.7749938964844
    ],
    "caption": "Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74–81, 2004.",
    "file_name": [
      "figures/fileoutpart134.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_197.png"
  },
  {
    "page": 12,
    "bbox": [
      441.4320068359375,
      352.9499969482422,
      445.2209930419922,
      353.34800720214844
    ],
    "caption": "Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 525–536. Springer, 2023a.",
    "file_name": [
      "figures/fileoutpart135.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_198.png"
  },
  {
    "page": 12,
    "bbox": [
      152.93099975585938,
      341.9909973144531,
      156.3000030517578,
      342.3890075683594
    ],
    "caption": "Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 525–536. Springer, 2023a.",
    "file_name": [
      "figures/fileoutpart136.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_199.png"
  },
  {
    "page": 12,
    "bbox": [
      163.4409942626953,
      301.6470031738281,
      168.2570037841797,
      302.0449981689453
    ],
    "caption": "Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi Wu, Mingguang He, and Zongyuan Ge. Medical visual question answering: A survey. Artificial Intelligence in Medicine, 143:102611, 2023b.",
    "file_name": [
      "figures/fileoutpart137.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_200.png"
  },
  {
    "page": 12,
    "bbox": [
      177.3419952392578,
      301.6470031738281,
      182.1580047607422,
      302.0449981689453
    ],
    "caption": "Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi Wu, Mingguang He, and Zongyuan Ge. Medical visual question answering: A survey. Artificial Intelligence in Medicine, 143:102611, 2023b.",
    "file_name": [
      "figures/fileoutpart138.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_201.png"
  },
  {
    "page": 12,
    "bbox": [
      213.927001953125,
      301.6470031738281,
      218.7429962158203,
      302.0449981689453
    ],
    "caption": "Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi Wu, Mingguang He, and Zongyuan Ge. Medical visual question answering: A survey. Artificial Intelligence in Medicine, 143:102611, 2023b.",
    "file_name": [
      "figures/fileoutpart139.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_202.png"
  },
  {
    "page": 12,
    "bbox": [
      242.76100158691406,
      301.6470031738281,
      247.57699584960938,
      302.0449981689453
    ],
    "caption": "Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi Wu, Mingguang He, and Zongyuan Ge. Medical visual question answering: A survey. Artificial Intelligence in Medicine, 143:102611, 2023b.",
    "file_name": [
      "figures/fileoutpart140.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_203.png"
  },
  {
    "page": 12,
    "bbox": [
      291.54100036621094,
      301.6470031738281,
      296.35699462890625,
      302.0449981689453
    ],
    "caption": "Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi Wu, Mingguang He, and Zongyuan Ge. Medical visual question answering: A survey. Artificial Intelligence in Medicine, 143:102611, 2023b.",
    "file_name": [
      "figures/fileoutpart141.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_204.png"
  },
  {
    "page": 12,
    "bbox": [
      309.86500549316406,
      301.6470031738281,
      314.6809997558594,
      302.0449981689453
    ],
    "caption": "Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi Wu, Mingguang He, and Zongyuan Ge. Medical visual question answering: A survey. Artificial Intelligence in Medicine, 143:102611, 2023b.",
    "file_name": [
      "figures/fileoutpart142.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_205.png"
  },
  {
    "page": 12,
    "bbox": [
      390.53399658203125,
      301.6470031738281,
      395.3500061035156,
      302.0449981689453
    ],
    "caption": "Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi Wu, Mingguang He, and Zongyuan Ge. Medical visual question answering: A survey. Artificial Intelligence in Medicine, 143:102611, 2023b.",
    "file_name": [
      "figures/fileoutpart143.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_206.png"
  },
  {
    "page": 12,
    "bbox": [
      442.6199951171875,
      261.30299377441406,
      446.5769958496094,
      261.7010040283203
    ],
    "caption": "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 26296–26306, 2024a.",
    "file_name": [
      "figures/fileoutpart144.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_207.png"
  },
  {
    "page": 12,
    "bbox": [
      492.7310028076172,
      261.30299377441406,
      496.68800354003906,
      261.7010040283203
    ],
    "caption": "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 26296–26306, 2024a.",
    "file_name": [
      "figures/fileoutpart145.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_208.png"
  },
  {
    "page": 12,
    "bbox": [
      248.42799377441406,
      220.95899963378906,
      252.281005859375,
      221.35699462890625
    ],
    "caption": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b.",
    "file_name": [
      "figures/fileoutpart146.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_209.png"
  },
  {
    "page": 12,
    "bbox": [
      259.7010040283203,
      220.95899963378906,
      263.5540008544922,
      221.35699462890625
    ],
    "caption": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b.",
    "file_name": [
      "figures/fileoutpart147.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_210.png"
  },
  {
    "page": 12,
    "bbox": [
      274.8500061035156,
      220.95899963378906,
      278.7030029296875,
      221.35699462890625
    ],
    "caption": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b.",
    "file_name": [
      "figures/fileoutpart148.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_211.png"
  },
  {
    "page": 12,
    "bbox": [
      321.5509948730469,
      220.95899963378906,
      325.4029998779297,
      221.35699462890625
    ],
    "caption": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b.",
    "file_name": [
      "figures/fileoutpart149.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_212.png"
  },
  {
    "page": 12,
    "bbox": [
      370.44200134277344,
      220.95899963378906,
      374.2949981689453,
      221.35699462890625
    ],
    "caption": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b.",
    "file_name": [
      "figures/fileoutpart150.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_213.png"
  },
  {
    "page": 12,
    "bbox": [
      383.3789978027344,
      220.95899963378906,
      387.23199462890625,
      221.35699462890625
    ],
    "caption": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b.",
    "file_name": [
      "figures/fileoutpart151.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_214.png"
  },
  {
    "page": 12,
    "bbox": [
      426.20399475097656,
      220.95899963378906,
      430.0570068359375,
      221.35699462890625
    ],
    "caption": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b.",
    "file_name": [
      "figures/fileoutpart152.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_215.png"
  },
  {
    "page": 12,
    "bbox": [
      455.1510009765625,
      220.95899963378906,
      459.0039978027344,
      221.35699462890625
    ],
    "caption": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b.",
    "file_name": [
      "figures/fileoutpart153.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_216.png"
  },
  {
    "page": 12,
    "bbox": [
      472.5119934082031,
      220.95899963378906,
      476.36500549316406,
      221.35699462890625
    ],
    "caption": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b.",
    "file_name": [
      "figures/fileoutpart154.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_217.png"
  },
  {
    "page": 12,
    "bbox": [
      125.27400207519531,
      180.61500549316406,
      128.64300537109375,
      181.01300048828125
    ],
    "caption": "Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, et al. Fairclip: Harnessing fairness in vision-language learning. arXiv preprint arXiv:2403.19949, 2024.",
    "file_name": [
      "figures/fileoutpart155.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_218.png"
  },
  {
    "page": 12,
    "bbox": [
      152.66200256347656,
      180.61500549316406,
      156.031005859375,
      181.01300048828125
    ],
    "caption": "Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, et al. Fairclip: Harnessing fairness in vision-language learning. arXiv preprint arXiv:2403.19949, 2024.",
    "file_name": [
      "figures/fileoutpart156.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_219.png"
  },
  {
    "page": 12,
    "bbox": [
      202.19500732421875,
      180.61500549316406,
      205.56399536132812,
      181.01300048828125
    ],
    "caption": "Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, et al. Fairclip: Harnessing fairness in vision-language learning. arXiv preprint arXiv:2403.19949, 2024.",
    "file_name": [
      "figures/fileoutpart157.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_220.png"
  },
  {
    "page": 12,
    "bbox": [
      247.29600524902344,
      180.61500549316406,
      250.6649932861328,
      181.01300048828125
    ],
    "caption": "Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, et al. Fairclip: Harnessing fairness in vision-language learning. arXiv preprint arXiv:2403.19949, 2024.",
    "file_name": [
      "figures/fileoutpart158.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_221.png"
  },
  {
    "page": 12,
    "bbox": [
      244.81500244140625,
      140.27099609375,
      248.1840057373047,
      140.66900634765625
    ],
    "caption": "Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718, 2024.",
    "file_name": [
      "figures/fileoutpart159.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_222.png"
  },
  {
    "page": 12,
    "bbox": [
      278.8470001220703,
      140.27099609375,
      282.21600341796875,
      140.66900634765625
    ],
    "caption": "Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718, 2024.",
    "file_name": [
      "figures/fileoutpart160.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_223.png"
  },
  {
    "page": 12,
    "bbox": [
      240.9499969482422,
      99.927001953125,
      244.31900024414062,
      100.32499694824219
    ],
    "caption": "Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: a multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pp. 353–367. PMLR, 2023.",
    "file_name": [
      "figures/fileoutpart161.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_224.png"
  },
  {
    "page": 12,
    "bbox": [
      274.98199462890625,
      99.927001953125,
      278.3509979248047,
      100.32499694824219
    ],
    "caption": "Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: a multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pp. 353–367. PMLR, 2023.",
    "file_name": [
      "figures/fileoutpart162.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_225.png"
  },
  {
    "page": 12,
    "bbox": [
      215.4550018310547,
      59.58299255371094,
      218.82400512695312,
      59.98100280761719
    ],
    "caption": "Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718, 2024.",
    "file_name": [
      "figures/fileoutpart163.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_226.png"
  },
  {
    "page": 12,
    "bbox": [
      253.91099548339844,
      59.58299255371094,
      257.2799987792969,
      59.98100280761719
    ],
    "caption": "Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718, 2024.",
    "file_name": [
      "figures/fileoutpart164.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_227.png"
  },
  {
    "page": 12,
    "bbox": [
      268.01800537109375,
      59.58299255371094,
      271.3869934082031,
      59.98100280761719
    ],
    "caption": "Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718, 2024.",
    "file_name": [
      "figures/fileoutpart165.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_228.png"
  },
  {
    "page": 12,
    "bbox": [
      297.0690002441406,
      59.58299255371094,
      300.43800354003906,
      59.98100280761719
    ],
    "caption": "Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718, 2024.",
    "file_name": [
      "figures/fileoutpart166.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_229.png"
  },
  {
    "page": 13,
    "bbox": [
      317.3820037841797,
      666.7740020751953,
      320.98699951171875,
      667.1719970703125
    ],
    "caption": "Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. arXiv preprint arXiv:2408.00555, 2024.",
    "file_name": [
      "figures/fileoutpart167.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_230.png"
  },
  {
    "page": 13,
    "bbox": [
      328.4080047607422,
      666.7740020751953,
      332.01300048828125,
      667.1719970703125
    ],
    "caption": "Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. arXiv preprint arXiv:2408.00555, 2024.",
    "file_name": [
      "figures/fileoutpart168.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_231.png"
  },
  {
    "page": 13,
    "bbox": [
      343.3090057373047,
      666.7740020751953,
      346.91400146484375,
      667.1719970703125
    ],
    "caption": "Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. arXiv preprint arXiv:2408.00555, 2024.",
    "file_name": [
      "figures/fileoutpart169.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_232.png"
  },
  {
    "page": 13,
    "bbox": [
      363.75,
      666.7740020751953,
      367.35499572753906,
      667.1719970703125
    ],
    "caption": "Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. arXiv preprint arXiv:2408.00555, 2024.",
    "file_name": [
      "figures/fileoutpart170.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_233.png"
  },
  {
    "page": 13,
    "bbox": [
      393.03700256347656,
      666.7740020751953,
      396.6419982910156,
      667.1719970703125
    ],
    "caption": "Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. arXiv preprint arXiv:2408.00555, 2024.",
    "file_name": [
      "figures/fileoutpart171.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_234.png"
  },
  {
    "page": 13,
    "bbox": [
      427.8630065917969,
      666.7740020751953,
      431.46800231933594,
      667.1719970703125
    ],
    "caption": "Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. arXiv preprint arXiv:2408.00555, 2024.",
    "file_name": [
      "figures/fileoutpart172.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_235.png"
  },
  {
    "page": 13,
    "bbox": [
      438.8890075683594,
      666.7740020751953,
      442.49400329589844,
      667.1719970703125
    ],
    "caption": "Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. arXiv preprint arXiv:2408.00555, 2024.",
    "file_name": [
      "figures/fileoutpart173.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_236.png"
  },
  {
    "page": 13,
    "bbox": [
      453.79100036621094,
      666.7740020751953,
      457.39599609375,
      667.1719970703125
    ],
    "caption": "Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. arXiv preprint arXiv:2408.00555, 2024.",
    "file_name": [
      "figures/fileoutpart174.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_237.png"
  },
  {
    "page": 13,
    "bbox": [
      129.13999938964844,
      655.8150024414062,
      132.50900268554688,
      656.2129974365234
    ],
    "caption": "Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. arXiv preprint arXiv:2408.00555, 2024.",
    "file_name": [
      "figures/fileoutpart175.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_238.png"
  },
  {
    "page": 13,
    "bbox": [
      190.85800170898438,
      655.8150024414062,
      194.2270050048828,
      656.2129974365234
    ],
    "caption": "Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. arXiv preprint arXiv:2408.00555, 2024.",
    "file_name": [
      "figures/fileoutpart176.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_239.png"
  },
  {
    "page": 13,
    "bbox": [
      467.01100158691406,
      624.968994140625,
      472.8970031738281,
      625.3670043945312
    ],
    "caption": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.",
    "file_name": [
      "figures/fileoutpart177.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_240.png"
  },
  {
    "page": 13,
    "bbox": [
      175.78500366210938,
      530.3999938964844,
      179.1540069580078,
      530.7980041503906
    ],
    "caption": "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gem-ini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.",
    "file_name": [
      "figures/fileoutpart178.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_241.png"
  },
  {
    "page": 13,
    "bbox": [
      224.1929931640625,
      530.3999938964844,
      227.56199645996094,
      530.7980041503906
    ],
    "caption": "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gem-ini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.",
    "file_name": [
      "figures/fileoutpart179.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_242.png"
  },
  {
    "page": 13,
    "bbox": [
      236.64599609375,
      530.3999938964844,
      240.01499938964844,
      530.7980041503906
    ],
    "caption": "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gem-ini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.",
    "file_name": [
      "figures/fileoutpart180.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_243.png"
  },
  {
    "page": 13,
    "bbox": [
      266.2449951171875,
      530.3999938964844,
      269.61399841308594,
      530.7980041503906
    ],
    "caption": "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gem-ini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.",
    "file_name": [
      "figures/fileoutpart181.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_244.png"
  },
  {
    "page": 13,
    "bbox": [
      316.3260040283203,
      530.3999938964844,
      319.69500732421875,
      530.7980041503906
    ],
    "caption": "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gem-ini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.",
    "file_name": [
      "figures/fileoutpart182.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_245.png"
  },
  {
    "page": 13,
    "bbox": [
      361.98500061035156,
      530.3999938964844,
      365.35400390625,
      530.7980041503906
    ],
    "caption": "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gem-ini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.",
    "file_name": [
      "figures/fileoutpart183.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_246.png"
  },
  {
    "page": 13,
    "bbox": [
      469.64500427246094,
      488.593994140625,
      472.8979949951172,
      488.99200439453125
    ],
    "caption": "Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre´ Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:8583–8595, 2021.",
    "file_name": [
      "figures/fileoutpart184.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_247.png"
  },
  {
    "page": 13,
    "bbox": [
      156.5570068359375,
      435.8300018310547,
      159.92599487304688,
      436.2279968261719
    ],
    "caption": "Corentin Royer, Bjoern Menze, and Anjany Sekuboyina. Multimedeval: A benchmark and a toolkit for evaluating medical vision-language models. arXiv preprint arXiv:2402.09262, 2024.",
    "file_name": [
      "figures/fileoutpart185.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_248.png"
  },
  {
    "page": 13,
    "bbox": [
      166.79800415039062,
      435.8300018310547,
      170.16700744628906,
      436.2279968261719
    ],
    "caption": "Corentin Royer, Bjoern Menze, and Anjany Sekuboyina. Multimedeval: A benchmark and a toolkit for evaluating medical vision-language models. arXiv preprint arXiv:2402.09262, 2024.",
    "file_name": [
      "figures/fileoutpart186.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_249.png"
  },
  {
    "page": 13,
    "bbox": [
      196.39700317382812,
      435.8300018310547,
      199.76600646972656,
      436.2279968261719
    ],
    "caption": "Corentin Royer, Bjoern Menze, and Anjany Sekuboyina. Multimedeval: A benchmark and a toolkit for evaluating medical vision-language models. arXiv preprint arXiv:2402.09262, 2024.",
    "file_name": [
      "figures/fileoutpart187.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_250.png"
  },
  {
    "page": 13,
    "bbox": [
      246.47900390625,
      435.8300018310547,
      249.84800720214844,
      436.2279968261719
    ],
    "caption": "Corentin Royer, Bjoern Menze, and Anjany Sekuboyina. Multimedeval: A benchmark and a toolkit for evaluating medical vision-language models. arXiv preprint arXiv:2402.09262, 2024.",
    "file_name": [
      "figures/fileoutpart188.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_251.png"
  },
  {
    "page": 13,
    "bbox": [
      292.1369934082031,
      435.8300018310547,
      295.50599670410156,
      436.2279968261719
    ],
    "caption": "Corentin Royer, Bjoern Menze, and Anjany Sekuboyina. Multimedeval: A benchmark and a toolkit for evaluating medical vision-language models. arXiv preprint arXiv:2402.09262, 2024.",
    "file_name": [
      "figures/fileoutpart189.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_252.png"
  },
  {
    "page": 13,
    "bbox": [
      332.5850067138672,
      404.98399353027344,
      335.95399475097656,
      405.3820037841797
    ],
    "caption": "Ma Guadalupe Sanchez, Ma Guadalupe Sanchez,´ Vicente Vidal, Gumersindo Verdu, Gumersindo Verdu,´ Patricia Mayo, and Francisco Rodenas. Medical image restoration with different types of noise. In 2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, pp. 4382–4385. IEEE, 2012.",
    "file_name": [
      "figures/fileoutpart190.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_253.png"
  },
  {
    "page": 13,
    "bbox": [
      366.61700439453125,
      404.98399353027344,
      369.9859924316406,
      405.3820037841797
    ],
    "caption": "Ma Guadalupe Sanchez, Ma Guadalupe Sanchez,´ Vicente Vidal, Gumersindo Verdu, Gumersindo Verdu,´ Patricia Mayo, and Francisco Rodenas. Medical image restoration with different types of noise. In 2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, pp. 4382–4385. IEEE, 2012.",
    "file_name": [
      "figures/fileoutpart191.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_254.png"
  },
  {
    "page": 13,
    "bbox": [
      191.55799865722656,
      363.1790008544922,
      195.968994140625,
      363.5769958496094
    ],
    "caption": "Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. Advances in Neural Information Processing Systems, 35:19304–19318, 2022.",
    "file_name": [
      "figures/fileoutpart192.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_255.png"
  },
  {
    "page": 13,
    "bbox": [
      224.42100524902344,
      363.1790008544922,
      228.83200073242188,
      363.5769958496094
    ],
    "caption": "Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. Advances in Neural Information Processing Systems, 35:19304–19318, 2022.",
    "file_name": [
      "figures/fileoutpart193.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_256.png"
  },
  {
    "page": 13,
    "bbox": [
      278.8630065917969,
      363.1790008544922,
      283.2740020751953,
      363.5769958496094
    ],
    "caption": "Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. Advances in Neural Information Processing Systems, 35:19304–19318, 2022.",
    "file_name": [
      "figures/fileoutpart194.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_257.png"
  },
  {
    "page": 13,
    "bbox": [
      328.3139953613281,
      363.1790008544922,
      332.7250061035156,
      363.5769958496094
    ],
    "caption": "Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. Advances in Neural Information Processing Systems, 35:19304–19318, 2022.",
    "file_name": [
      "figures/fileoutpart195.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_258.png"
  },
  {
    "page": 13,
    "bbox": [
      340.14599609375,
      363.1790008544922,
      344.5570068359375,
      363.5769958496094
    ],
    "caption": "Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. Advances in Neural Information Processing Systems, 35:19304–19318, 2022.",
    "file_name": [
      "figures/fileoutpart196.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_259.png"
  },
  {
    "page": 13,
    "bbox": [
      355.85400390625,
      363.1790008544922,
      360.26499938964844,
      363.5769958496094
    ],
    "caption": "Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. Advances in Neural Information Processing Systems, 35:19304–19318, 2022.",
    "file_name": [
      "figures/fileoutpart197.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_260.png"
  },
  {
    "page": 13,
    "bbox": [
      380.96600341796875,
      363.1790008544922,
      385.3760070800781,
      363.5769958496094
    ],
    "caption": "Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. Advances in Neural Information Processing Systems, 35:19304–19318, 2022.",
    "file_name": [
      "figures/fileoutpart198.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_261.png"
  },
  {
    "page": 13,
    "bbox": [
      433.1959991455078,
      363.1790008544922,
      437.60699462890625,
      363.5769958496094
    ],
    "caption": "Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. Advances in Neural Information Processing Systems, 35:19304–19318, 2022.",
    "file_name": [
      "figures/fileoutpart199.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_262.png"
  },
  {
    "page": 13,
    "bbox": [
      444.47999572753906,
      363.1790008544922,
      448.89100646972656,
      363.5769958496094
    ],
    "caption": "Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. Advances in Neural Information Processing Systems, 35:19304–19318, 2022.",
    "file_name": [
      "figures/fileoutpart200.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_263.png"
  },
  {
    "page": 13,
    "bbox": [
      485.6419982910156,
      363.1790008544922,
      490.05299377441406,
      363.5769958496094
    ],
    "caption": "Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. Advances in Neural Information Processing Systems, 35:19304–19318, 2022.",
    "file_name": [
      "figures/fileoutpart201.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_264.png"
  },
  {
    "page": 13,
    "bbox": [
      149.63299560546875,
      352.2200012207031,
      153.0019989013672,
      352.6179962158203
    ],
    "caption": "Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. Advances in Neural Information Processing Systems, 35:19304–19318, 2022.",
    "file_name": [
      "figures/fileoutpart202.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_265.png"
  },
  {
    "page": 13,
    "bbox": [
      492.83299255371094,
      310.4149932861328,
      496.68800354003906,
      310.81300354003906
    ],
    "caption": "Mehmet Saygin Seyfioglu, Wisdom O Ikezogwo, Fatemeh Ghezloo, Ranjay Krishna, and Linda Shapiro. Quilt-llava: Visual instruction tuning by extracting localized narratives from open-source histopathology videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13183–13192, 2024.",
    "file_name": [
      "figures/fileoutpart203.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_266.png"
  },
  {
    "page": 13,
    "bbox": [
      144.6320037841797,
      299.45599365234375,
      148.00100708007812,
      299.85400390625
    ],
    "caption": "Mehmet Saygin Seyfioglu, Wisdom O Ikezogwo, Fatemeh Ghezloo, Ranjay Krishna, and Linda Shapiro. Quilt-llava: Visual instruction tuning by extracting localized narratives from open-source histopathology videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13183–13192, 2024.",
    "file_name": [
      "figures/fileoutpart204.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_267.png"
  },
  {
    "page": 13,
    "bbox": [
      194.71299743652344,
      299.45599365234375,
      198.08200073242188,
      299.85400390625
    ],
    "caption": "Mehmet Saygin Seyfioglu, Wisdom O Ikezogwo, Fatemeh Ghezloo, Ranjay Krishna, and Linda Shapiro. Quilt-llava: Visual instruction tuning by extracting localized narratives from open-source histopathology videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13183–13192, 2024.",
    "file_name": [
      "figures/fileoutpart205.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_268.png"
  },
  {
    "page": 13,
    "bbox": [
      240.3719940185547,
      299.45599365234375,
      243.74099731445312,
      299.85400390625
    ],
    "caption": "Mehmet Saygin Seyfioglu, Wisdom O Ikezogwo, Fatemeh Ghezloo, Ranjay Krishna, and Linda Shapiro. Quilt-llava: Visual instruction tuning by extracting localized narratives from open-source histopathology videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13183–13192, 2024.",
    "file_name": [
      "figures/fileoutpart206.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_269.png"
  },
  {
    "page": 13,
    "bbox": [
      275.7539978027344,
      257.6499938964844,
      280.07200622558594,
      258.0480041503906
    ],
    "caption": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.",
    "file_name": [
      "figures/fileoutpart207.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_270.png"
  },
  {
    "page": 13,
    "bbox": [
      287.4929962158203,
      257.6499938964844,
      291.8110046386719,
      258.0480041503906
    ],
    "caption": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.",
    "file_name": [
      "figures/fileoutpart208.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_271.png"
  },
  {
    "page": 13,
    "bbox": [
      303.10699462890625,
      257.6499938964844,
      307.4250030517578,
      258.0480041503906
    ],
    "caption": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.",
    "file_name": [
      "figures/fileoutpart209.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_272.png"
  },
  {
    "page": 13,
    "bbox": [
      350.2729949951172,
      257.6499938964844,
      354.5899963378906,
      258.0480041503906
    ],
    "caption": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.",
    "file_name": [
      "figures/fileoutpart210.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_273.png"
  },
  {
    "page": 13,
    "bbox": [
      399.6300048828125,
      257.6499938964844,
      403.947998046875,
      258.0480041503906
    ],
    "caption": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.",
    "file_name": [
      "figures/fileoutpart211.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_274.png"
  },
  {
    "page": 13,
    "bbox": [
      413.03199768066406,
      257.6499938964844,
      417.3500061035156,
      258.0480041503906
    ],
    "caption": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.",
    "file_name": [
      "figures/fileoutpart212.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_275.png"
  },
  {
    "page": 13,
    "bbox": [
      456.322998046875,
      257.6499938964844,
      460.64100646972656,
      258.0480041503906
    ],
    "caption": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.",
    "file_name": [
      "figures/fileoutpart213.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_276.png"
  },
  {
    "page": 13,
    "bbox": [
      485.73500061035156,
      257.6499938964844,
      490.05299377441406,
      258.0480041503906
    ],
    "caption": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.",
    "file_name": [
      "figures/fileoutpart214.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_277.png"
  },
  {
    "page": 13,
    "bbox": [
      145.59800720214844,
      246.69200134277344,
      148.9669952392578,
      247.08999633789062
    ],
    "caption": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.",
    "file_name": [
      "figures/fileoutpart215.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_278.png"
  },
  {
    "page": 13,
    "bbox": [
      139.95899963378906,
      204.88600158691406,
      143.3280029296875,
      205.28399658203125
    ],
    "caption": "Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, and Xiaoxiao Li. A survey on trustworthiness in foundation models for medical image analysis. arXiv preprint arXiv:2407.15851, 2024.",
    "file_name": [
      "figures/fileoutpart216.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_279.png"
  },
  {
    "page": 13,
    "bbox": [
      173.99099731445312,
      204.88600158691406,
      177.36000061035156,
      205.28399658203125
    ],
    "caption": "Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, and Xiaoxiao Li. A survey on trustworthiness in foundation models for medical image analysis. arXiv preprint arXiv:2407.15851, 2024.",
    "file_name": [
      "figures/fileoutpart217.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_280.png"
  },
  {
    "page": 13,
    "bbox": [
      330.04400634765625,
      174.0399932861328,
      333.4129943847656,
      174.43800354003906
    ],
    "caption": "Jiashuo Sun, Jihai Zhang, Yucheng Zhou, Zhaochen Su, Xiaoye Qu, and Yu Cheng. Surf: Teaching large vision-language models to selectively utilize retrieved information. arXiv preprint arXiv:2409.14083, 2024a.",
    "file_name": [
      "figures/fileoutpart218.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_281.png"
  },
  {
    "page": 13,
    "bbox": [
      364.0760040283203,
      174.0399932861328,
      367.44500732421875,
      174.43800354003906
    ],
    "caption": "Jiashuo Sun, Jihai Zhang, Yucheng Zhou, Zhaochen Su, Xiaoye Qu, and Yu Cheng. Surf: Teaching large vision-language models to selectively utilize retrieved information. arXiv preprint arXiv:2409.14083, 2024a.",
    "file_name": [
      "figures/fileoutpart219.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_282.png"
  },
  {
    "page": 13,
    "bbox": [
      467.7460021972656,
      143.19400024414062,
      472.8979949951172,
      143.5919952392578
    ],
    "caption": "Liwen Sun, James Zhao, Megan Han, and Chenyan Xiong. Fact-aware multimodal retrieval augmentation for accurate medical radiology report generation. arXiv preprint arXiv:2407.15268, 2024b.",
    "file_name": [
      "figures/fileoutpart220.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_283.png"
  },
  {
    "page": 13,
    "bbox": [
      390.08799743652344,
      101.38800048828125,
      394.4230041503906,
      101.78599548339844
    ],
    "caption": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104–3112, 2014.",
    "file_name": [
      "figures/fileoutpart221.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_284.png"
  },
  {
    "page": 13,
    "bbox": [
      425.08599853515625,
      101.38800048828125,
      429.42100524902344,
      101.78599548339844
    ],
    "caption": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104–3112, 2014.",
    "file_name": [
      "figures/fileoutpart222.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_285.png"
  },
  {
    "page": 13,
    "bbox": [
      167.3459930419922,
      59.58299255371094,
      170.71499633789062,
      59.98100280761719
    ],
    "caption": "Liwen Sun, James Zhao, Megan Han, and Chenyan Xiong. Fact-aware multimodal retrieval augmentation for accurate medical radiology report generation. arXiv preprint arXiv:2407.15268, 2024b.",
    "file_name": [
      "figures/fileoutpart223.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_286.png"
  },
  {
    "page": 13,
    "bbox": [
      177.58799743652344,
      59.58299255371094,
      180.95700073242188,
      59.98100280761719
    ],
    "caption": "Liwen Sun, James Zhao, Megan Han, and Chenyan Xiong. Fact-aware multimodal retrieval augmentation for accurate medical radiology report generation. arXiv preprint arXiv:2407.15268, 2024b.",
    "file_name": [
      "figures/fileoutpart224.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_287.png"
  },
  {
    "page": 13,
    "bbox": [
      204.97500610351562,
      59.58299255371094,
      208.343994140625,
      59.98100280761719
    ],
    "caption": "Liwen Sun, James Zhao, Megan Han, and Chenyan Xiong. Fact-aware multimodal retrieval augmentation for accurate medical radiology report generation. arXiv preprint arXiv:2407.15268, 2024b.",
    "file_name": [
      "figures/fileoutpart225.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_288.png"
  },
  {
    "page": 13,
    "bbox": [
      254.50900268554688,
      59.58299255371094,
      257.8780059814453,
      59.98100280761719
    ],
    "caption": "Liwen Sun, James Zhao, Megan Han, and Chenyan Xiong. Fact-aware multimodal retrieval augmentation for accurate medical radiology report generation. arXiv preprint arXiv:2407.15268, 2024b.",
    "file_name": [
      "figures/fileoutpart226.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_289.png"
  },
  {
    "page": 13,
    "bbox": [
      299.60899353027344,
      59.58299255371094,
      302.9779968261719,
      59.98100280761719
    ],
    "caption": "Liwen Sun, James Zhao, Megan Han, and Chenyan Xiong. Fact-aware multimodal retrieval augmentation for accurate medical radiology report generation. arXiv preprint arXiv:2407.15268, 2024b.",
    "file_name": [
      "figures/fileoutpart227.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_290.png"
  },
  {
    "page": 14,
    "bbox": [
      310.0740051269531,
      686.6620025634766,
      314.3780059814453,
      687.0599975585938
    ],
    "caption": "Alexandra-Maria T˘aut¸an, Bogdan Ionescu, and Emiliano Santarnecchi. Artificial intelligence in neu-rodegenerative diseases: A review of available tools with a focus on machine learning techniques. Artificial Intelligence in Medicine, 117:102081, 2021.",
    "file_name": [
      "figures/fileoutpart228.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_291.png"
  },
  {
    "page": 14,
    "bbox": [
      342.82899475097656,
      686.6620025634766,
      347.13299560546875,
      687.0599975585938
    ],
    "caption": "Alexandra-Maria T˘aut¸an, Bogdan Ionescu, and Emiliano Santarnecchi. Artificial intelligence in neu-rodegenerative diseases: A review of available tools with a focus on machine learning techniques. Artificial Intelligence in Medicine, 117:102081, 2021.",
    "file_name": [
      "figures/fileoutpart229.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_292.png"
  },
  {
    "page": 14,
    "bbox": [
      354.5540008544922,
      686.6620025634766,
      358.8580017089844,
      687.0599975585938
    ],
    "caption": "Alexandra-Maria T˘aut¸an, Bogdan Ionescu, and Emiliano Santarnecchi. Artificial intelligence in neu-rodegenerative diseases: A review of available tools with a focus on machine learning techniques. Artificial Intelligence in Medicine, 117:102081, 2021.",
    "file_name": [
      "figures/fileoutpart230.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_293.png"
  },
  {
    "page": 14,
    "bbox": [
      403.9179992675781,
      686.6620025634766,
      408.2220001220703,
      687.0599975585938
    ],
    "caption": "Alexandra-Maria T˘aut¸an, Bogdan Ionescu, and Emiliano Santarnecchi. Artificial intelligence in neu-rodegenerative diseases: A review of available tools with a focus on machine learning techniques. Artificial Intelligence in Medicine, 117:102081, 2021.",
    "file_name": [
      "figures/fileoutpart231.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_294.png"
  },
  {
    "page": 14,
    "bbox": [
      421.72999572753906,
      686.6620025634766,
      426.03399658203125,
      687.0599975585938
    ],
    "caption": "Alexandra-Maria T˘aut¸an, Bogdan Ionescu, and Emiliano Santarnecchi. Artificial intelligence in neu-rodegenerative diseases: A review of available tools with a focus on machine learning techniques. Artificial Intelligence in Medicine, 117:102081, 2021.",
    "file_name": [
      "figures/fileoutpart232.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_295.png"
  },
  {
    "page": 14,
    "bbox": [
      451.7169952392578,
      686.6620025634766,
      456.02099609375,
      687.0599975585938
    ],
    "caption": "Alexandra-Maria T˘aut¸an, Bogdan Ionescu, and Emiliano Santarnecchi. Artificial intelligence in neu-rodegenerative diseases: A review of available tools with a focus on machine learning techniques. Artificial Intelligence in Medicine, 117:102081, 2021.",
    "file_name": [
      "figures/fileoutpart233.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_296.png"
  },
  {
    "page": 14,
    "bbox": [
      153.4980010986328,
      634.6280059814453,
      156.86700439453125,
      635.0260009765625
    ],
    "caption": "Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muham-mad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radio-graphs summarization using medical vision-language models. arXiv preprint arXiv:2306.07971, 2023.",
    "file_name": [
      "figures/fileoutpart234.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_297.png"
  },
  {
    "page": 14,
    "bbox": [
      203.02200317382812,
      634.6280059814453,
      206.39100646972656,
      635.0260009765625
    ],
    "caption": "Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muham-mad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radio-graphs summarization using medical vision-language models. arXiv preprint arXiv:2306.07971, 2023.",
    "file_name": [
      "figures/fileoutpart235.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_298.png"
  },
  {
    "page": 14,
    "bbox": [
      213.26300048828125,
      634.6280059814453,
      216.6320037841797,
      635.0260009765625
    ],
    "caption": "Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muham-mad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radio-graphs summarization using medical vision-language models. arXiv preprint arXiv:2306.07971, 2023.",
    "file_name": [
      "figures/fileoutpart236.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_299.png"
  },
  {
    "page": 14,
    "bbox": [
      391.58799743652344,
      593.5529937744141,
      395.1719970703125,
      593.9510040283203
    ],
    "caption": "Robert Tibshirani, Guenther Walther, and Trevor Hastie. Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2):411–423, 2001.",
    "file_name": [
      "figures/fileoutpart237.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_300.png"
  },
  {
    "page": 14,
    "bbox": [
      425.83599853515625,
      593.5529937744141,
      429.4199981689453,
      593.9510040283203
    ],
    "caption": "Robert Tibshirani, Guenther Walther, and Trevor Hastie. Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2):411–423, 2001.",
    "file_name": [
      "figures/fileoutpart238.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_301.png"
  },
  {
    "page": 14,
    "bbox": [
      281.3269958496094,
      552.47900390625,
      286.0249938964844,
      552.8769989013672
    ],
    "caption": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.",
    "file_name": [
      "figures/fileoutpart239.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_302.png"
  },
  {
    "page": 14,
    "bbox": [
      293.4459991455078,
      552.47900390625,
      298.1439971923828,
      552.8769989013672
    ],
    "caption": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.",
    "file_name": [
      "figures/fileoutpart240.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_303.png"
  },
  {
    "page": 14,
    "bbox": [
      309.4409942626953,
      552.47900390625,
      314.1390075683594,
      552.8769989013672
    ],
    "caption": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.",
    "file_name": [
      "figures/fileoutpart241.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_304.png"
  },
  {
    "page": 14,
    "bbox": [
      336.9620056152344,
      552.47900390625,
      341.6600036621094,
      552.8769989013672
    ],
    "caption": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.",
    "file_name": [
      "figures/fileoutpart242.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_305.png"
  },
  {
    "page": 14,
    "bbox": [
      380.08399963378906,
      552.47900390625,
      384.78199768066406,
      552.8769989013672
    ],
    "caption": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.",
    "file_name": [
      "figures/fileoutpart243.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_306.png"
  },
  {
    "page": 14,
    "bbox": [
      416.5610046386719,
      552.47900390625,
      423.18699645996094,
      552.8769989013672
    ],
    "caption": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.",
    "file_name": [
      "figures/fileoutpart244.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_307.png"
  },
  {
    "page": 14,
    "bbox": [
      446.65699768066406,
      552.47900390625,
      451.35499572753906,
      552.8769989013672
    ],
    "caption": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.",
    "file_name": [
      "figures/fileoutpart245.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_308.png"
  },
  {
    "page": 14,
    "bbox": [
      457.1230010986328,
      552.47900390625,
      461.82000732421875,
      552.8769989013672
    ],
    "caption": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.",
    "file_name": [
      "figures/fileoutpart246.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_309.png"
  },
  {
    "page": 14,
    "bbox": [
      253.6719970703125,
      500.44500732421875,
      257.04100036621094,
      500.84300231933594
    ],
    "caption": "Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. NEJM AI, 1(3):AIoa2300138, 2024.",
    "file_name": [
      "figures/fileoutpart247.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_310.png"
  },
  {
    "page": 14,
    "bbox": [
      287.70399475097656,
      500.44500732421875,
      291.072998046875,
      500.84300231933594
    ],
    "caption": "Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. NEJM AI, 1(3):AIoa2300138, 2024.",
    "file_name": [
      "figures/fileoutpart248.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_311.png"
  },
  {
    "page": 14,
    "bbox": [
      143.53599548339844,
      459.3710021972656,
      146.90499877929688,
      459.7689971923828
    ],
    "caption": "Chunhao Wang, Xiaofeng Zhu, Julian C Hong, and Dandan Zheng. Artificial intelligence in radiotherapy treatment planning: present and future. Technology in cancer research & treatment, 18: 1533033819873922, 2019.",
    "file_name": [
      "figures/fileoutpart249.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_312.png"
  },
  {
    "page": 14,
    "bbox": [
      358.84300231933594,
      429.2550048828125,
      362.5709991455078,
      429.6529998779297
    ],
    "caption": "Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: A comprehensive benchmark for mul-timodal large language model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024.",
    "file_name": [
      "figures/fileoutpart250.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_313.png"
  },
  {
    "page": 14,
    "bbox": [
      369.4429931640625,
      429.2550048828125,
      373.17100524902344,
      429.6529998779297
    ],
    "caption": "Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: A comprehensive benchmark for mul-timodal large language model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024.",
    "file_name": [
      "figures/fileoutpart251.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_314.png"
  },
  {
    "page": 14,
    "bbox": [
      398.2850036621094,
      429.2550048828125,
      402.01300048828125,
      429.6529998779297
    ],
    "caption": "Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: A comprehensive benchmark for mul-timodal large language model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024.",
    "file_name": [
      "figures/fileoutpart252.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_315.png"
  },
  {
    "page": 14,
    "bbox": [
      434.32000732421875,
      429.2550048828125,
      438.0480041503906,
      429.6529998779297
    ],
    "caption": "Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: A comprehensive benchmark for mul-timodal large language model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024.",
    "file_name": [
      "figures/fileoutpart253.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_316.png"
  },
  {
    "page": 14,
    "bbox": [
      444.92100524902344,
      429.2550048828125,
      448.6490020751953,
      429.6529998779297
    ],
    "caption": "Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: A comprehensive benchmark for mul-timodal large language model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024.",
    "file_name": [
      "figures/fileoutpart254.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_317.png"
  },
  {
    "page": 14,
    "bbox": [
      392.625,
      377.2209930419922,
      395.6909942626953,
      377.61900329589844
    ],
    "caption": "Zhen Wang, Mingxiao Li, Peng Xia, Chao Jiang, Ting Shen, Jiaming Ma, Yu Bai, Suhui Zhang, Yiwei Lai, Sitong Li, et al. Screening cognitive impairment in patients with atrial fibrillation: a deep learning model based on retinal fundus photographs. Heart Rhythm O2, 2025.",
    "file_name": [
      "figures/fileoutpart255.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_318.png"
  },
  {
    "page": 14,
    "bbox": [
      426.35499572753906,
      377.2209930419922,
      429.42100524902344,
      377.61900329589844
    ],
    "caption": "Zhen Wang, Mingxiao Li, Peng Xia, Chao Jiang, Ting Shen, Jiaming Ma, Yu Bai, Suhui Zhang, Yiwei Lai, Sitong Li, et al. Screening cognitive impairment in patients with atrial fibrillation: a deep learning model based on retinal fundus photographs. Heart Rhythm O2, 2025.",
    "file_name": [
      "figures/fileoutpart256.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_319.png"
  },
  {
    "page": 14,
    "bbox": [
      374.0,
      325.18800354003906,
      377.36900329589844,
      325.58599853515625
    ],
    "caption": "Zhepei Wei, Wei-Lin Chen, and Yu Meng. Instructrag: Instructing retrieval-augmented generation with explicit denoising. arXiv preprint arXiv:2406.13629, 2024.",
    "file_name": [
      "figures/fileoutpart257.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_320.png"
  },
  {
    "page": 14,
    "bbox": [
      408.5500030517578,
      325.18800354003906,
      411.91900634765625,
      325.58599853515625
    ],
    "caption": "Zhepei Wei, Wei-Lin Chen, and Yu Meng. Instructrag: Instructing retrieval-augmented generation with explicit denoising. arXiv preprint arXiv:2406.13629, 2024.",
    "file_name": [
      "figures/fileoutpart258.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_321.png"
  },
  {
    "page": 14,
    "bbox": [
      237.21299743652344,
      295.07200622558594,
      240.58200073242188,
      295.4700012207031
    ],
    "caption": "Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis. arXiv preprint arXiv:2310.09909, 2023a.",
    "file_name": [
      "figures/fileoutpart259.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_322.png"
  },
  {
    "page": 14,
    "bbox": [
      271.2460021972656,
      295.07200622558594,
      274.61500549316406,
      295.4700012207031
    ],
    "caption": "Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis. arXiv preprint arXiv:2310.09909, 2023a.",
    "file_name": [
      "figures/fileoutpart260.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_323.png"
  },
  {
    "page": 14,
    "bbox": [
      351.3739929199219,
      253.9969940185547,
      354.7429962158203,
      254.39500427246094
    ],
    "caption": "Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology. arXiv preprint arXiv:2308.02463, 2023b.",
    "file_name": [
      "figures/fileoutpart261.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_324.png"
  },
  {
    "page": 14,
    "bbox": [
      385.406005859375,
      253.9969940185547,
      388.7749938964844,
      254.39500427246094
    ],
    "caption": "Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology. arXiv preprint arXiv:2308.02463, 2023b.",
    "file_name": [
      "figures/fileoutpart262.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_325.png"
  },
  {
    "page": 14,
    "bbox": [
      252.4759979248047,
      223.8820037841797,
      255.84500122070312,
      224.27999877929688
    ],
    "caption": "Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Cheng Niu, Randy Zhong, Juntong Song, and Tong Zhang. Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented language models. arXiv preprint arXiv:2401.00396, 2023c.",
    "file_name": [
      "figures/fileoutpart263.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_326.png"
  },
  {
    "page": 14,
    "bbox": [
      286.5090026855469,
      223.8820037841797,
      289.8780059814453,
      224.27999877929688
    ],
    "caption": "Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Cheng Niu, Randy Zhong, Juntong Song, and Tong Zhang. Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented language models. arXiv preprint arXiv:2401.00396, 2023c.",
    "file_name": [
      "figures/fileoutpart264.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_327.png"
  },
  {
    "page": 14,
    "bbox": [
      213.2740020751953,
      182.8070068359375,
      216.64300537109375,
      183.2050018310547
    ],
    "caption": "Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: A comprehensive benchmark of trustworthiness in medical vision language models. arXiv preprint arXiv:2406.06007, 2024a.",
    "file_name": [
      "figures/fileoutpart265.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_328.png"
  },
  {
    "page": 14,
    "bbox": [
      247.30599975585938,
      182.8070068359375,
      250.6750030517578,
      183.2050018310547
    ],
    "caption": "Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: A comprehensive benchmark of trustworthiness in medical vision language models. arXiv preprint arXiv:2406.06007, 2024a.",
    "file_name": [
      "figures/fileoutpart266.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_329.png"
  },
  {
    "page": 14,
    "bbox": [
      274.1549987792969,
      141.73199462890625,
      277.5240020751953,
      142.1300048828125
    ],
    "caption": "Peng Xia, Ming Hu, Feilong Tang, Wenxue Li, Wenhao Zheng, Lie Ju, Peibo Duan, Huaxiu Yao, and Zongyuan Ge. Generalizing to unseen domains in diabetic retinopathy with disentangled representations. In arXiv preprint arXiv:2406.06384, 2024b.",
    "file_name": [
      "figures/fileoutpart267.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_330.png"
  },
  {
    "page": 14,
    "bbox": [
      308.18699645996094,
      141.73199462890625,
      311.5559997558594,
      142.1300048828125
    ],
    "caption": "Peng Xia, Ming Hu, Feilong Tang, Wenxue Li, Wenhao Zheng, Lie Ju, Peibo Duan, Huaxiu Yao, and Zongyuan Ge. Generalizing to unseen domains in diabetic retinopathy with disentangled representations. In arXiv preprint arXiv:2406.06384, 2024b.",
    "file_name": [
      "figures/fileoutpart268.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_331.png"
  },
  {
    "page": 14,
    "bbox": [
      217.13900756835938,
      100.65800476074219,
      220.50799560546875,
      101.05599975585938
    ],
    "caption": "Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao. Rule: Reliable multimodal rag for factuality in medical vision language models. arXiv preprint arXiv:2407.05131, 2024c.",
    "file_name": [
      "figures/fileoutpart269.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_332.png"
  },
  {
    "page": 14,
    "bbox": [
      251.17100524902344,
      100.65800476074219,
      254.5399932861328,
      101.05599975585938
    ],
    "caption": "Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao. Rule: Reliable multimodal rag for factuality in medical vision language models. arXiv preprint arXiv:2407.05131, 2024c.",
    "file_name": [
      "figures/fileoutpart270.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_333.png"
  },
  {
    "page": 14,
    "bbox": [
      469.07000732421875,
      70.54200744628906,
      472.8979949951172,
      70.94000244140625
    ],
    "caption": "Peng Xia, Ming Hu, Feilong Tang, Wenxue Li, Wenhao Zheng, Lie Ju, Peibo Duan, Huaxiu Yao, and Zongyuan Ge. Generalizing to unseen domains in diabetic retinopathy with disentangled representations. In arXiv preprint arXiv:2406.06384, 2024b.",
    "file_name": [
      "figures/fileoutpart271.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_334.png"
  },
  {
    "page": 15,
    "bbox": [
      264.7100067138672,
      675.7030029296875,
      268.07899475097656,
      676.1009979248047
    ],
    "caption": "Zheng Yuan, Qiao Jin, Chuanqi Tan, Zhengyun Zhao, Hongyi Yuan, Fei Huang, and Songfang Huang. Ramm: Retrieval-augmented biomedical visual question answering with multi-modal pre-training. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 547– 556, 2023.",
    "file_name": [
      "figures/fileoutpart272.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_335.png"
  },
  {
    "page": 15,
    "bbox": [
      230.41200256347656,
      630.0330047607422,
      233.85000610351562,
      630.4309997558594
    ],
    "caption": "Jihai Zhang, Xiaoye Qu, Tong Zhu, and Yu Cheng. Clip-moe: Towards building mixture of experts for clip with diversified multiplet upcycling. arXiv preprint arXiv:2409.19291, 2024.",
    "file_name": [
      "figures/fileoutpart273.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_336.png"
  },
  {
    "page": 15,
    "bbox": [
      241.27099609375,
      630.0330047607422,
      244.70899963378906,
      630.4309997558594
    ],
    "caption": "Jihai Zhang, Xiaoye Qu, Tong Zhu, and Yu Cheng. Clip-moe: Towards building mixture of experts for clip with diversified multiplet upcycling. arXiv preprint arXiv:2409.19291, 2024.",
    "file_name": [
      "figures/fileoutpart274.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_337.png"
  },
  {
    "page": 15,
    "bbox": [
      256.00599670410156,
      630.0330047607422,
      259.4440002441406,
      630.4309997558594
    ],
    "caption": "Jihai Zhang, Xiaoye Qu, Tong Zhu, and Yu Cheng. Clip-moe: Towards building mixture of experts for clip with diversified multiplet upcycling. arXiv preprint arXiv:2409.19291, 2024.",
    "file_name": [
      "figures/fileoutpart275.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_338.png"
  },
  {
    "page": 15,
    "bbox": [
      275.1730041503906,
      630.0330047607422,
      278.6109924316406,
      630.4309997558594
    ],
    "caption": "Jihai Zhang, Xiaoye Qu, Tong Zhu, and Yu Cheng. Clip-moe: Towards building mixture of experts for clip with diversified multiplet upcycling. arXiv preprint arXiv:2409.19291, 2024.",
    "file_name": [
      "figures/fileoutpart276.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_339.png"
  },
  {
    "page": 15,
    "bbox": [
      300.0299987792969,
      630.0330047607422,
      303.46800231933594,
      630.4309997558594
    ],
    "caption": "Jihai Zhang, Xiaoye Qu, Tong Zhu, and Yu Cheng. Clip-moe: Towards building mixture of experts for clip with diversified multiplet upcycling. arXiv preprint arXiv:2409.19291, 2024.",
    "file_name": [
      "figures/fileoutpart277.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_340.png"
  },
  {
    "page": 15,
    "bbox": [
      353.4980010986328,
      630.0330047607422,
      356.9360046386719,
      630.4309997558594
    ],
    "caption": "Jihai Zhang, Xiaoye Qu, Tong Zhu, and Yu Cheng. Clip-moe: Towards building mixture of experts for clip with diversified multiplet upcycling. arXiv preprint arXiv:2409.19291, 2024.",
    "file_name": [
      "figures/fileoutpart278.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_341.png"
  },
  {
    "page": 15,
    "bbox": [
      401.9759979248047,
      630.0330047607422,
      405.41400146484375,
      630.4309997558594
    ],
    "caption": "Jihai Zhang, Xiaoye Qu, Tong Zhu, and Yu Cheng. Clip-moe: Towards building mixture of experts for clip with diversified multiplet upcycling. arXiv preprint arXiv:2409.19291, 2024.",
    "file_name": [
      "figures/fileoutpart279.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_342.png"
  },
  {
    "page": 15,
    "bbox": [
      414.4989929199219,
      630.0330047607422,
      417.93699645996094,
      630.4309997558594
    ],
    "caption": "Jihai Zhang, Xiaoye Qu, Tong Zhu, and Yu Cheng. Clip-moe: Towards building mixture of experts for clip with diversified multiplet upcycling. arXiv preprint arXiv:2409.19291, 2024.",
    "file_name": [
      "figures/fileoutpart280.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_343.png"
  },
  {
    "page": 15,
    "bbox": [
      319.5540008544922,
      584.3639984130859,
      322.9230041503906,
      584.7619934082031
    ],
    "caption": "Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Pre-ston, Rajesh Rao, Mu Wei, Naveen Valluri, et al. Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915, 2023a.",
    "file_name": [
      "figures/fileoutpart281.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_344.png"
  },
  {
    "page": 15,
    "bbox": [
      353.58599853515625,
      584.3639984130859,
      356.9550018310547,
      584.7619934082031
    ],
    "caption": "Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Pre-ston, Rajesh Rao, Mu Wei, Naveen Valluri, et al. Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915, 2023a.",
    "file_name": [
      "figures/fileoutpart282.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_345.png"
  },
  {
    "page": 15,
    "bbox": [
      467.5870056152344,
      538.6940002441406,
      472.8970031738281,
      539.0919952392578
    ],
    "caption": "Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023b.",
    "file_name": [
      "figures/fileoutpart283.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_346.png"
  },
  {
    "page": 15,
    "bbox": [
      468.9709930419922,
      493.0249938964844,
      472.8979949951172,
      493.4230041503906
    ],
    "caption": "Kangyu Zhu, Peng Xia, Yun Li, Hongtu Zhu, Sheng Wang, and Huaxiu Yao. Mmedpo: Aligning medical vision-language models with clinical-aware multimodal preference optimization. arXiv preprint arXiv:2412.06141, 2024.",
    "file_name": [
      "figures/fileoutpart284.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_347.png"
  },
  {
    "page": 15,
    "bbox": [
      149.06500244140625,
      436.39599609375,
      152.4340057373047,
      436.79400634765625
    ],
    "caption": "A EXPERIMENT",
    "file_name": [
      "figures/fileoutpart285.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_348.png"
  },
  {
    "page": 18,
    "bbox": [
      122.60699462890625,
      598.8820037841797,
      489.39300537109375,
      610.4059600830078
    ],
    "caption": "[QA PAIRS R1]",
    "file_name": [
      "figures/fileoutpart292.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_349.png"
  },
  {
    "page": 18,
    "bbox": [
      122.60699462890625,
      598.8820037841797,
      489.39300537109375,
      610.4059600830078
    ],
    "caption": "[QA PAIRS R1]",
    "file_name": ""
  },
  {
    "page": 24,
    "bbox": [
      61.729095458984375,
      498.37257385253906,
      567.5685424804688,
      783.0224761962891
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart317.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_351.png"
  },
  {
    "page": 25,
    "bbox": [
      161.85699462890625,
      553.6749114990234,
      452.6375427246094,
      593.0670318603516
    ],
    "caption": "where U(t) = log(1 + exp(−t)). Define the weight of xv with respect to log πθ(y|x) as",
    "file_name": [
      "figures/fileoutpart318.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_352.png"
  },
  {
    "page": 25,
    "bbox": [
      161.85699462890625,
      553.6749114990234,
      452.6375427246094,
      593.0670318603516
    ],
    "caption": "where U(t) = log(1 + exp(−t)). Define the weight of xv with respect to log πθ(y|x) as",
    "file_name": ""
  },
  {
    "page": 25,
    "bbox": [
      214.11300659179688,
      500.3576965332031,
      399.1365203857422,
      538.7934112548828
    ],
    "caption": "Assumption B.1 (Large parameter space) Assume that π(xv, y|xt, xr) lies in the optimization space {πθ, θ ∈ Θ} such that π(xv, y|xt, xr) ∝ πo(xv, y|xt, xr)  qw (xv ,y|xt,xr ) ql(xv ,y|xt,xr ) 1 α",
    "file_name": [
      "figures/fileoutpart319.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_354.png"
  },
  {
    "page": 25,
    "bbox": [
      214.11300659179688,
      500.3576965332031,
      399.1365203857422,
      538.7934112548828
    ],
    "caption": "Assumption B.1 (Large parameter space) Assume that π(xv, y|xt, xr) lies in the optimization space {πθ, θ ∈ Θ} such that π(xv, y|xt, xr) ∝ πo(xv, y|xt, xr)  qw (xv ,y|xt,xr ) ql(xv ,y|xt,xr ) 1 α",
    "file_name": ""
  },
  {
    "page": 25,
    "bbox": [
      107.99626159667969,
      425.6881866455078,
      506.4903106689453,
      472.0520935058594
    ],
    "caption": "Assumption B.2 Let h(x, y), abbreviate as h, be",
    "file_name": [
      "figures/fileoutpart320.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_356.png"
  },
  {
    "page": 25,
    "bbox": [
      107.99626159667969,
      425.6881866455078,
      506.4903106689453,
      472.0520935058594
    ],
    "caption": "Assumption B.2 Let h(x, y), abbreviate as h, be",
    "file_name": ""
  },
  {
    "page": 25,
    "bbox": [
      208.68699645996094,
      338.65846252441406,
      402.8632507324219,
      380.02943420410156
    ],
    "caption": "Assume that wt(xv, πo) < c2 , where",
    "file_name": [
      "figures/fileoutpart321.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_358.png"
  },
  {
    "page": 25,
    "bbox": [
      208.68699645996094,
      338.65846252441406,
      402.8632507324219,
      380.02943420410156
    ],
    "caption": "Assume that wt(xv, πo) < c2 , where",
    "file_name": ""
  },
  {
    "page": 25,
    "bbox": [
      152.70700073242188,
      268.64337158203125,
      460.28871154785156,
      322.97357177734375
    ],
    "caption": "h1 :=   y πo(y|x)  q 1 w(y|xv ,xt,xr) + q 2 w (y|xv ,xt) q1 l (y|xv ,xt) + q2 l (y|xv ,xt,xr ) 1 α −1  q 1 w (y|xv ,xt,xr ) + q 2 w (y|xv ,xt) q1 l (y|xv ,xt) + q2 l (y|xv ,xt,xr ) 1 α",
    "file_name": [
      "figures/fileoutpart322.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_360.png"
  },
  {
    "page": 25,
    "bbox": [
      152.70700073242188,
      268.64337158203125,
      460.28871154785156,
      322.97357177734375
    ],
    "caption": "h1 :=   y πo(y|x)  q 1 w(y|xv ,xt,xr) + q 2 w (y|xv ,xt) q1 l (y|xv ,xt) + q2 l (y|xv ,xt,xr ) 1 α −1  q 1 w (y|xv ,xt,xr ) + q 2 w (y|xv ,xt) q1 l (y|xv ,xt) + q2 l (y|xv ,xt,xr ) 1 α",
    "file_name": ""
  },
  {
    "page": 25,
    "bbox": [
      117.71200561523438,
      220.87747192382812,
      493.8382568359375,
      262.2484436035156
    ],
    "caption": "Assume that wt(xr, πo) < c12 and wt(˜xr, πo) > c22, where",
    "file_name": [
      "figures/fileoutpart323.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_362.png"
  },
  {
    "page": 25,
    "bbox": [
      117.71200561523438,
      220.87747192382812,
      493.8382568359375,
      262.2484436035156
    ],
    "caption": "Assume that wt(xr, πo) < c12 and wt(˜xr, πo) > c22, where",
    "file_name": ""
  },
  {
    "page": 25,
    "bbox": [
      160.36099243164062,
      119.56986999511719,
      452.6347198486328,
      205.1605682373047
    ],
    "caption": "Lemma B.1 Suppose that Assumption (<>)B.1 hold, optimizing equation (<>)14 gives",
    "file_name": [
      "figures/fileoutpart324.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_364.png"
  },
  {
    "page": 25,
    "bbox": [
      160.36099243164062,
      119.56986999511719,
      452.6347198486328,
      205.1605682373047
    ],
    "caption": "Lemma B.1 Suppose that Assumption (<>)B.1 hold, optimizing equation (<>)14 gives",
    "file_name": ""
  },
  {
    "page": 25,
    "bbox": [
      238.5050048828125,
      50.223907470703125,
      373.0452575683594,
      89.61468505859375
    ],
    "caption": "Lemma B.1 Suppose that Assumption (<>)B.1 hold, optimizing equation (<>)14 gives",
    "file_name": [
      "figures/fileoutpart325.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_366.png"
  },
  {
    "page": 25,
    "bbox": [
      238.5050048828125,
      50.223907470703125,
      373.0452575683594,
      89.61468505859375
    ],
    "caption": "Lemma B.1 Suppose that Assumption (<>)B.1 hold, optimizing equation (<>)14 gives",
    "file_name": ""
  },
  {
    "page": 26,
    "bbox": [
      166.14605712890625,
      548.9459075927734,
      454.62310791015625,
      578.4154205322266
    ],
    "caption": "and abbreviated as p1 and p2 for notational convenience. Then,",
    "file_name": [
      "figures/fileoutpart326.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_368.png"
  },
  {
    "page": 26,
    "bbox": [
      166.14605712890625,
      548.9459075927734,
      454.62310791015625,
      578.4154205322266
    ],
    "caption": "and abbreviated as p1 and p2 for notational convenience. Then,",
    "file_name": ""
  },
  {
    "page": 26,
    "bbox": [
      192.23049926757812,
      471.9161376953125,
      422.2608337402344,
      529.7985992431641
    ],
    "caption": "f(x, y) = g(x) + log qw(xv, y|xt, xr) ql(xv, y|xt, xr)",
    "file_name": [
      "figures/fileoutpart327.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_370.png"
  },
  {
    "page": 26,
    "bbox": [
      192.23049926757812,
      471.9161376953125,
      422.2608337402344,
      529.7985992431641
    ],
    "caption": "f(x, y) = g(x) + log qw(xv, y|xt, xr) ql(xv, y|xt, xr)",
    "file_name": ""
  },
  {
    "page": 26,
    "bbox": [
      228.3800048828125,
      432.91790771484375,
      384.9161071777344,
      465.0024108886719
    ],
    "caption": "2ED [U (f(xw, yw,o, xt, xr) − f(xl, yl,o, xt, xr))] =  q(xt, xr) · p1 · U (f(xw, yw,o, xt, xr) − f(xl, yl,o, xt, xr)) dxdy ",
    "file_name": [
      "figures/fileoutpart328.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_372.png"
  },
  {
    "page": 26,
    "bbox": [
      228.3800048828125,
      432.91790771484375,
      384.9161071777344,
      465.0024108886719
    ],
    "caption": "2ED [U (f(xw, yw,o, xt, xr) − f(xl, yl,o, xt, xr))] =  q(xt, xr) · p1 · U (f(xw, yw,o, xt, xr) − f(xl, yl,o, xt, xr)) dxdy ",
    "file_name": ""
  },
  {
    "page": 26,
    "bbox": [
      155.94149780273438,
      321.70286560058594,
      437.05369567871094,
      393.75054931640625
    ],
    "caption": "=2 log 2 +  q(xt, xr)  p1 · log  p1 + p2 2p1  + p2 · log  p1 + p2 2p2  dxdy =2 log 2 − KL  p1  p1 + p2 2  − KL  p2  p1 + p2 2 ",
    "file_name": [
      "figures/fileoutpart329.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_374.png"
  },
  {
    "page": 26,
    "bbox": [
      155.94149780273438,
      321.70286560058594,
      437.05369567871094,
      393.75054931640625
    ],
    "caption": "=2 log 2 +  q(xt, xr)  p1 · log  p1 + p2 2p1  + p2 · log  p1 + p2 2p2  dxdy =2 log 2 − KL  p1  p1 + p2 2  − KL  p2  p1 + p2 2 ",
    "file_name": ""
  },
  {
    "page": 26,
    "bbox": [
      155.93450927734375,
      294.69219970703125,
      447.56968688964844,
      346.57948303222656
    ],
    "caption": "where the first inequality follows from Lemma (<>)B.2. For equivalence,",
    "file_name": [
      "figures/fileoutpart330.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_376.png"
  },
  {
    "page": 26,
    "bbox": [
      155.93450927734375,
      294.69219970703125,
      447.56968688964844,
      346.57948303222656
    ],
    "caption": "where the first inequality follows from Lemma (<>)B.2. For equivalence,",
    "file_name": ""
  },
  {
    "page": 26,
    "bbox": [
      155.9409942626953,
      227.83114624023438,
      458.19847106933594,
      305.7186737060547
    ],
    "caption": "f(x, yw,o, xt, xr) − f(xl, yl,o, xt, xr) = log qw(xw, yw,o|xt, xr) · ql(xl, yl,o|xt, xr) ql(xw, yw,o|xt, xr) · qw(xl, yl,o|xt, xr)",
    "file_name": [
      "figures/fileoutpart331.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_378.png"
  },
  {
    "page": 26,
    "bbox": [
      155.9409942626953,
      227.83114624023438,
      458.19847106933594,
      305.7186737060547
    ],
    "caption": "f(x, yw,o, xt, xr) − f(xl, yl,o, xt, xr) = log qw(xw, yw,o|xt, xr) · ql(xl, yl,o|xt, xr) ql(xw, yw,o|xt, xr) · qw(xl, yl,o|xt, xr)",
    "file_name": ""
  },
  {
    "page": 26,
    "bbox": [
      124.218994140625,
      188.83290100097656,
      462.51710510253906,
      220.91741943359375
    ],
    "caption": "f(xw, yw,o, xt, xr) − log qw(xw, yw,o|xt, xr) ql(xw, yw,o|xt, xr) = f(xl, yl,o, xt, xr) − log qw(xl, yl,o|xt, xr) ql(xl, yl,o|xt, xr) (25)",
    "file_name": [
      "figures/fileoutpart332.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_380.png"
  },
  {
    "page": 26,
    "bbox": [
      124.218994140625,
      188.83290100097656,
      462.51710510253906,
      220.91741943359375
    ],
    "caption": "f(xw, yw,o, xt, xr) − log qw(xw, yw,o|xt, xr) ql(xw, yw,o|xt, xr) = f(xl, yl,o, xt, xr) − log qw(xl, yl,o|xt, xr) ql(xl, yl,o|xt, xr) (25)",
    "file_name": ""
  },
  {
    "page": 26,
    "bbox": [
      113.33000183105469,
      138.22190856933594,
      506.4899444580078,
      170.30641174316406
    ],
    "caption": "Therefore, equation (<>)25 holds if and only if there exists some g(xv, xt, xr) such that",
    "file_name": [
      "figures/fileoutpart333.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_382.png"
  },
  {
    "page": 26,
    "bbox": [
      113.33000183105469,
      138.22190856933594,
      506.4899444580078,
      170.30641174316406
    ],
    "caption": "Therefore, equation (<>)25 holds if and only if there exists some g(xv, xt, xr) such that",
    "file_name": ""
  },
  {
    "page": 26,
    "bbox": [
      202.91200256347656,
      87.31790161132812,
      410.38409423828125,
      119.40242004394531
    ],
    "caption": "Lemma (<>)B.3 provides a closed-form solution to equation (<>)14 if the parameter space is sufficiently large. This lemma is crucial for the proof Lemma (<>)B.1, which follows below",
    "file_name": [
      "figures/fileoutpart334.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_384.png"
  },
  {
    "page": 26,
    "bbox": [
      202.91200256347656,
      87.31790161132812,
      410.38409423828125,
      119.40242004394531
    ],
    "caption": "Lemma (<>)B.3 provides a closed-form solution to equation (<>)14 if the parameter space is sufficiently large. This lemma is crucial for the proof Lemma (<>)B.1, which follows below",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      172.93899536132812,
      656.0989074707031,
      438.6112518310547,
      695.4896850585938
    ],
    "caption": "α log π(xv, y|xt, xr) πo(xv, y|xt, xr)  = α log[ˆg(xt, xr)] + log qw(xv, y|xt, xr) ql(xv, y|xt, xr)",
    "file_name": [
      "figures/fileoutpart335.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_386.png"
  },
  {
    "page": 27,
    "bbox": [
      172.93899536132812,
      656.0989074707031,
      438.6112518310547,
      695.4896850585938
    ],
    "caption": "α log π(xv, y|xt, xr) πo(xv, y|xt, xr)  = α log[ˆg(xt, xr)] + log qw(xv, y|xt, xr) ql(xv, y|xt, xr)",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      168.58883666992188,
      607.8079071044922,
      444.7001037597656,
      647.2000274658203
    ],
    "caption": "arg min f ED [U (f(xw, yw,o, xt, xr) − f(xl, yl,o, xt, xr))]",
    "file_name": [
      "figures/fileoutpart336.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_388.png"
  },
  {
    "page": 27,
    "bbox": [
      168.58883666992188,
      607.8079071044922,
      444.7001037597656,
      647.2000274658203
    ],
    "caption": "arg min f ED [U (f(xw, yw,o, xt, xr) − f(xl, yl,o, xt, xr))]",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      191.24378967285156,
      572.2725524902344,
      423.24615478515625,
      591.6215972900391
    ],
    "caption": "by Lemma (<>)B.3. Since π(xv, y|xt, xr) ∈ {πθ, θ ∈ Θ} lies in the optimization space, we have",
    "file_name": [
      "figures/fileoutpart337.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_390.png"
  },
  {
    "page": 27,
    "bbox": [
      191.24378967285156,
      572.2725524902344,
      423.24615478515625,
      591.6215972900391
    ],
    "caption": "by Lemma (<>)B.3. Since π(xv, y|xt, xr) ∈ {πθ, θ ∈ Θ} lies in the optimization space, we have",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      166.94400024414062,
      494.84190368652344,
      447.54393005371094,
      547.6925964355469
    ],
    "caption": "α log πθ(xv, y|xt, xr) πo(xv, y|xt, xr) = g(xt, xr) + log qw(xv, y|xt, xr) ql(xv, y|xt, xr) =⇒πθ(xv, y|xt, xr) = πo(xv, y|xt, xr)  qw(xv, y|xt, xr) ql(xv, y|xt, xr) 1 α exp  1 α g(xt, xr) ",
    "file_name": [
      "figures/fileoutpart338.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_392.png"
  },
  {
    "page": 27,
    "bbox": [
      166.94400024414062,
      494.84190368652344,
      447.54393005371094,
      547.6925964355469
    ],
    "caption": "α log πθ(xv, y|xt, xr) πo(xv, y|xt, xr) = g(xt, xr) + log qw(xv, y|xt, xr) ql(xv, y|xt, xr) =⇒πθ(xv, y|xt, xr) = πo(xv, y|xt, xr)  qw(xv, y|xt, xr) ql(xv, y|xt, xr) 1 α exp  1 α g(xt, xr) ",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      141.83639526367188,
      413.9259033203125,
      472.6565704345703,
      477.3304138183594
    ],
    "caption": "min f EDU (f(xw, yw,o, xt, xr) − f(xl, yl,o, xt, xr)) = min πθ EDU  α log πθ(yw,o|xw, xt, xr) πo(yw,o|xw, xt, xr) − α log πθ(yl,o|xl, xt, xr) πo(yl,o|xl, xt, xr) ",
    "file_name": [
      "figures/fileoutpart339.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_394.png"
  },
  {
    "page": 27,
    "bbox": [
      141.83639526367188,
      413.9259033203125,
      472.6565704345703,
      477.3304138183594
    ],
    "caption": "min f EDU (f(xw, yw,o, xt, xr) − f(xl, yl,o, xt, xr)) = min πθ EDU  α log πθ(yw,o|xw, xt, xr) πo(yw,o|xw, xt, xr) − α log πθ(yl,o|xl, xt, xr) πo(yl,o|xl, xt, xr) ",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      108.0,
      287.9167022705078,
      479.0549774169922,
      415.4934387207031
    ],
    "caption": "θ = θo + 1 α(β − ˜β)",
    "file_name": [
      "figures/fileoutpart340.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_396.png"
  },
  {
    "page": 27,
    "bbox": [
      108.0,
      287.9167022705078,
      479.0549774169922,
      415.4934387207031
    ],
    "caption": "θ = θo + 1 α(β − ˜β)",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      265.71600341796875,
      227.87924194335938,
      348.7630920410156,
      253.13409423828125
    ],
    "caption": "Corollary (<>)B.1 is a direct application of Lemma (<>)B.1, indicating that the model updates coefficient θo towards the direction of β for preferred responses and away from β˜ for dispreferred responses.",
    "file_name": [
      "figures/fileoutpart341.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_398.png"
  },
  {
    "page": 27,
    "bbox": [
      265.71600341796875,
      227.87924194335938,
      348.7630920410156,
      253.13409423828125
    ],
    "caption": "Corollary (<>)B.1 is a direct application of Lemma (<>)B.1, indicating that the model updates coefficient θo towards the direction of β for preferred responses and away from β˜ for dispreferred responses.",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      160.3800811767578,
      54.42713928222656,
      454.0874938964844,
      170.5986785888672
    ],
    "caption": "Corollary (<>)B.1 is a direct application of Lemma (<>)B.1, indicating that the model updates coefficient θo towards the direction of β for preferred responses and away from β˜ for dispreferred responses.",
    "file_name": [
      "figures/fileoutpart342.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_400.png"
  },
  {
    "page": 27,
    "bbox": [
      160.3800811767578,
      54.42713928222656,
      454.0874938964844,
      170.5986785888672
    ],
    "caption": "Corollary (<>)B.1 is a direct application of Lemma (<>)B.1, indicating that the model updates coefficient θo towards the direction of β for preferred responses and away from β˜ for dispreferred responses.",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      158.64300537109375,
      605.7497100830078,
      455.4932861328125,
      671.6826782226562
    ],
    "caption": "Theorem B.2 Suppose that Assumption (<>)B.2 holds, then cross-modality increase the weight of xv.",
    "file_name": [
      "figures/fileoutpart343.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_402.png"
  },
  {
    "page": 28,
    "bbox": [
      158.64300537109375,
      605.7497100830078,
      455.4932861328125,
      671.6826782226562
    ],
    "caption": "Theorem B.2 Suppose that Assumption (<>)B.2 holds, then cross-modality increase the weight of xv.",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      203.61500549316406,
      552.6576995849609,
      415.10462951660156,
      617.4094085693359
    ],
    "caption": "Theorem B.2 Suppose that Assumption (<>)B.2 holds, then cross-modality increase the weight of xv.",
    "file_name": [
      "figures/fileoutpart344.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_404.png"
  },
  {
    "page": 28,
    "bbox": [
      203.61500549316406,
      552.6576995849609,
      415.10462951660156,
      617.4094085693359
    ],
    "caption": "Theorem B.2 Suppose that Assumption (<>)B.2 holds, then cross-modality increase the weight of xv.",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      257.19427490234375,
      517.3865509033203,
      357.3000946044922,
      529.8304443359375
    ],
    "caption": "πθ(y|x) = πo(y|x) · h(x, y),  πo(y|x) · h(x, y)dy = 1",
    "file_name": [
      "figures/fileoutpart345.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_406.png"
  },
  {
    "page": 28,
    "bbox": [
      257.19427490234375,
      517.3865509033203,
      357.3000946044922,
      529.8304443359375
    ],
    "caption": "πθ(y|x) = πo(y|x) · h(x, y),  πo(y|x) · h(x, y)dy = 1",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      190.2372589111328,
      456.55780029296875,
      424.24810791015625,
      494.99351501464844
    ],
    "caption": "Abbreviate h(x, y) and πo(y|xv, xt) as h and πo respectively, we have",
    "file_name": [
      "figures/fileoutpart346.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_408.png"
  },
  {
    "page": 28,
    "bbox": [
      190.2372589111328,
      456.55780029296875,
      424.24810791015625,
      494.99351501464844
    ],
    "caption": "Abbreviate h(x, y) and πo(y|xv, xt) as h and πo respectively, we have",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      115.15699768066406,
      349.6417999267578,
      506.48724365234375,
      441.51646423339844
    ],
    "caption": "(39)",
    "file_name": [
      "figures/fileoutpart347.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_410.png"
  },
  {
    "page": 28,
    "bbox": [
      115.15699768066406,
      349.6417999267578,
      506.48724365234375,
      441.51646423339844
    ],
    "caption": "(39)",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      108.0,
      294.9527893066406,
      473.55751037597656,
      363.11143493652344
    ],
    "caption": "the last term in equation (<>)38 is equivalent to",
    "file_name": [
      "figures/fileoutpart348.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_412.png"
  },
  {
    "page": 28,
    "bbox": [
      108.0,
      294.9527893066406,
      473.55751037597656,
      363.11143493652344
    ],
    "caption": "the last term in equation (<>)38 is equivalent to",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      173.23899841308594,
      239.8177947998047,
      440.009521484375,
      300.4800720214844
    ],
    "caption": "Thus, wt(xv, πθ) > wt(xv, πo) if wt(xv, πo) < c.",
    "file_name": [
      "figures/fileoutpart349.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_414.png"
  },
  {
    "page": 28,
    "bbox": [
      173.23899841308594,
      239.8177947998047,
      440.009521484375,
      300.4800720214844
    ],
    "caption": "Thus, wt(xv, πθ) > wt(xv, πo) if wt(xv, πo) < c.",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      178.35800170898438,
      170.0497283935547,
      436.1321258544922,
      242.76641845703125
    ],
    "caption": "wt(xr, πθ) > wt(xr, πo), wt(˜xr, πθ) < wt(˜xr, πo)",
    "file_name": [
      "figures/fileoutpart350.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_416.png"
  },
  {
    "page": 28,
    "bbox": [
      178.35800170898438,
      170.0497283935547,
      436.1321258544922,
      242.76641845703125
    ],
    "caption": "wt(xr, πθ) > wt(xr, πo), wt(˜xr, πθ) < wt(˜xr, πo)",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      203.07550048828125,
      134.34254455566406,
      411.4141082763672,
      146.78643798828125
    ],
    "caption": "Proof B.6 The distribution of preferred responses can be considered as a mixture distribution: qw1 (xv, yw,o|xt, xr) + qw2 (xv, yw,o|xt). Similarly, for dispreferred responses, the distribution is represented as ql1(xv, yl,o|xt) + ql2(xv, yl,o|xt, xr). By Lemma (<>)B.1,",
    "file_name": [
      "figures/fileoutpart351.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_418.png"
  },
  {
    "page": 28,
    "bbox": [
      203.07550048828125,
      134.34254455566406,
      411.4141082763672,
      146.78643798828125
    ],
    "caption": "Proof B.6 The distribution of preferred responses can be considered as a mixture distribution: qw1 (xv, yw,o|xt, xr) + qw2 (xv, yw,o|xt). Similarly, for dispreferred responses, the distribution is represented as ql1(xv, yl,o|xt) + ql2(xv, yl,o|xt, xr). By Lemma (<>)B.1,",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      185.77117919921875,
      49.80580139160156,
      428.7231750488281,
      88.24151611328125
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart352.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_420.png"
  },
  {
    "page": 28,
    "bbox": [
      185.77117919921875,
      49.80580139160156,
      428.7231750488281,
      88.24151611328125
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 29,
    "bbox": [
      108.0,
      611.8677978515625,
      506.49085998535156,
      709.3814392089844
    ],
    "caption": "(45)",
    "file_name": [
      "figures/fileoutpart353.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_422.png"
  },
  {
    "page": 29,
    "bbox": [
      422.8818054199219,
      697.4860992431641,
      436.4408264160156,
      709.3814392089844
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 29,
    "bbox": [
      108.0,
      611.8677978515625,
      506.49085998535156,
      709.3814392089844
    ],
    "caption": "(45)",
    "file_name": ""
  },
  {
    "page": 29,
    "bbox": [
      124.02999877929688,
      537.5607299804688,
      472.62152099609375,
      614.2560729980469
    ],
    "caption": "where",
    "file_name": [
      "figures/fileoutpart354.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_425.png"
  },
  {
    "page": 29,
    "bbox": [
      124.02999877929688,
      537.5607299804688,
      472.62152099609375,
      614.2560729980469
    ],
    "caption": "where",
    "file_name": ""
  },
  {
    "page": 29,
    "bbox": [
      129.24224853515625,
      454.7327880859375,
      468.6481018066406,
      539.0956878662109
    ],
    "caption": "Thus, wt(xr,πθ) < wt(xr,πo) if wt(xr,πo) > c2.",
    "file_name": [
      "figures/fileoutpart355.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_427.png"
  },
  {
    "page": 29,
    "bbox": [
      129.24224853515625,
      454.7327880859375,
      468.6481018066406,
      539.0956878662109
    ],
    "caption": "Thus, wt(xr,πθ) < wt(xr,πo) if wt(xr,πo) > c2.",
    "file_name": ""
  },
  {
    "page": 29,
    "bbox": [
      141.59800720214844,
      389.27272033691406,
      471.39772033691406,
      462.10552978515625
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart356.png"
    ],
    "output_file": "assets/images/paper/MMed-RAG-Versatile-Multimodal-RAG-System-for-Medical-Vision-Language-Models/fig_429.png"
  },
  {
    "page": 29,
    "bbox": [
      141.59800720214844,
      389.27272033691406,
      471.39772033691406,
      462.10552978515625
    ],
    "caption": "",
    "file_name": ""
  }
]