[
  {
    "page": 1,
    "bbox": [
      134.76499938964844,
      525.1940002441406,
      480.58470153808594,
      676.1647491455078
    ],
    "caption": "Fig. 1: The comparison between V2T-CoT and existing Med-VQA methods. A employs a combined vision and text encoding strategy with regional attention for medical diagnosis. In contrast, previous methods (B & C) either lack reasoning or utilize it in a text-only context. D demonstrates the pipeline of V2T-CoT.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_01.png"
  },
  {
    "page": 3,
    "bbox": [
      134.76499938964844,
      539.1199951171875,
      480.59002685546875,
      676.1646728515625
    ],
    "caption": "Fig. 2: Implementation of V2T-CoT for medical diagnosis. The model leverages dynamic fusion of image and text encodings, alignment scoring, and regional attention mechanisms to generate answers and rationale.",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_02.png"
  },
  {
    "page": 3,
    "bbox": [
      200.9990234375,
      385.04151916503906,
      416.84193420410156,
      404.9816589355469
    ],
    "caption": "T i+1 = BERTLayer(T i + Ti i2t), T =T L ,",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_03.png"
  },
  {
    "page": 3,
    "bbox": [
      200.9990234375,
      385.04151916503906,
      416.84193420410156,
      404.9816589355469
    ],
    "caption": "T i+1 = BERTLayer(T i + Ti i2t), T =T L ,",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      200.98358154296875,
      373.46083068847656,
      405.49346923828125,
      388.5316467285156
    ],
    "caption": "T i+1 = BERTLayer(T i + Ti i2t), T =T L ,",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_05.png"
  },
  {
    "page": 3,
    "bbox": [
      200.98358154296875,
      373.46083068847656,
      405.49346923828125,
      388.5316467285156
    ],
    "caption": "T i+1 = BERTLayer(T i + Ti i2t), T =T L ,",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      200.992919921875,
      357.0098419189453,
      386.06646728515625,
      372.0806579589844
    ],
    "caption": "where L is the number of DyHeadModules, Vt2i i and Ti2t i are the context vectors generated by the X-MHA module.",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_07.png"
  },
  {
    "page": 3,
    "bbox": [
      200.992919921875,
      357.0098419189453,
      386.06646728515625,
      372.0806579589844
    ],
    "caption": "where L is the number of DyHeadModules, Vt2i i and Ti2t i are the context vectors generated by the X-MHA module.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      141.25399780273438,
      176.6625213623047,
      483.09181213378906,
      216.00640869140625
    ],
    "caption": "Regional pixel-level attention is implemented by assigning higher weights to key pixels using convolutional layers and attention modules. To balance regional and global attention, we introduce a weight factor α, where α ∈ [0, 1]. The combined attention Acombined is computed as:",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_09.png"
  },
  {
    "page": 3,
    "bbox": [
      141.25399780273438,
      176.6625213623047,
      483.09181213378906,
      216.00640869140625
    ],
    "caption": "Regional pixel-level attention is implemented by assigning higher weights to key pixels using convolutional layers and attention modules. To balance regional and global attention, we introduce a weight factor α, where α ∈ [0, 1]. The combined attention Acombined is computed as:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      431.33168029785156,
      499.6472473144531,
      481.8389434814453,
      519.0874786376953
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_11.png"
  },
  {
    "page": 4,
    "bbox": [
      462.66099548339844,
      506.1929168701172,
      481.8389434814453,
      519.0874786376953
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      431.33168029785156,
      499.6472473144531,
      481.8389434814453,
      519.0874786376953
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      381.91717529296875,
      487.69090270996094,
      429.3301086425781,
      507.1324768066406
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_14.png"
  },
  {
    "page": 4,
    "bbox": [
      408.86199951171875,
      494.2379150390625,
      429.3301086425781,
      507.1324768066406
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      381.91717529296875,
      487.69090270996094,
      429.3301086425781,
      507.1324768066406
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      241.9814910888672,
      452.38970947265625,
      261.8968048095703,
      494.6348419189453
    ],
    "caption": "T i+1 = MultiHeadAttention(T i ,V regional, V global),",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_17.png"
  },
  {
    "page": 4,
    "bbox": [
      241.9814910888672,
      452.38970947265625,
      261.8968048095703,
      494.6348419189453
    ],
    "caption": "T i+1 = MultiHeadAttention(T i ,V regional, V global),",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      287.42091369628906,
      452.38970947265625,
      319.24139404296875,
      494.6348419189453
    ],
    "caption": "T i+1 = MultiHeadAttention(T i ,V regional, V global),",
    "file_name": [
      "figures/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_19.png"
  },
  {
    "page": 4,
    "bbox": [
      287.42091369628906,
      452.38970947265625,
      319.24139404296875,
      494.6348419189453
    ],
    "caption": "T i+1 = MultiHeadAttention(T i ,V regional, V global),",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      163.15740966796875,
      464.341552734375,
      306.3516845703125,
      483.3802032470703
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_21.png"
  },
  {
    "page": 4,
    "bbox": [
      183.7429962158203,
      467.0943908691406,
      199.44102478027344,
      475.89532470703125
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      254.3179931640625,
      473.2213134765625,
      272.59193420410156,
      481.13134765625
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      273.09100341796875,
      474.1575469970703,
      279.785888671875,
      481.13134765625
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      280.2740020751953,
      474.20465087890625,
      286.91407775878906,
      483.3802032470703
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      279.78582763671875,
      472.2220916748047,
      291.7857666015625,
      478.6180877685547
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      267.9550018310547,
      465.40391540527344,
      274.5033721923828,
      477.48951721191406
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      163.15740966796875,
      464.341552734375,
      306.3516845703125,
      483.3802032470703
    ],
    "caption": "To further enhance the interaction between modalities, we employ a multi-head cross-attention mechanism in the VLM, where the textual features are iteratively refined through layers of self-attention:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      199.6455078125,
      395.72654724121094,
      418.20570373535156,
      409.34046936035156
    ],
    "caption": "where T i represents the textual features at layer i. This iterative refinement ensures that the textual reasoning aligns with the visual clues.",
    "file_name": [
      "figures/fileoutpart11.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_29.png"
  },
  {
    "page": 4,
    "bbox": [
      199.6455078125,
      395.72654724121094,
      418.20570373535156,
      409.34046936035156
    ],
    "caption": "where T i represents the textual features at layer i. This iterative refinement ensures that the textual reasoning aligns with the visual clues.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      134.76499938964844,
      557.9400024414062,
      480.58868408203125,
      676.1642761230469
    ],
    "caption": "Fig. 3: (a) A radar plot compares the performance of V2T-CoT and Med-Think across three datasets: VQA-RAD(VR), SLAKE(SK ), PathVQA(PV ) using open-ended evaluation metrics. (b) A quantitative heatmap visualization showing that incorporating Vision CoT improves attention to the organ (liver).",
    "file_name": [
      "figures/fileoutpart16.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_31.png"
  },
  {
    "page": 8,
    "bbox": [
      134.76499938964844,
      586.4819946289062,
      480.57965087890625,
      676.1642150878906
    ],
    "caption": "Fig. 4: LLM and Human evaluation for rationale from R-Med 39K. (a) A bar chart illustrating rationale quality scores using different LLMs and human on four Med-VQA datasets, highlighting that higher rationale quality correlates with better Med-VQA performance. (b) Sample rationale quality assessments demonstrating Text CoT’s role in medical diagnostics.",
    "file_name": [
      "figures/fileoutpart21.png"
    ],
    "output_file": "assets/images/paper/V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis/fig_32.png"
  }
]