[
  {
    "page": 1,
    "bbox": [
      97.46461486816406,
      612.1788177490234,
      211.17796325683594,
      719.7576293945312
    ],
    "caption": "(a) Textual-modal fine-tuning methods",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_01.png"
  },
  {
    "page": 1,
    "bbox": [
      97.46461486816406,
      612.1788177490234,
      211.17796325683594,
      719.7576293945312
    ],
    "caption": "(a) Textual-modal fine-tuning methods",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      248.91429138183594,
      612.0612640380859,
      359.23692321777344,
      718.8134613037109
    ],
    "caption": "(b) Visual-modal fine-tuning methods",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_03.png"
  },
  {
    "page": 1,
    "bbox": [
      248.91429138183594,
      612.0612640380859,
      359.23692321777344,
      718.8134613037109
    ],
    "caption": "(b) Visual-modal fine-tuning methods",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      386.2360534667969,
      610.3433685302734,
      530.2751007080078,
      720.1002807617188
    ],
    "caption": "(c) Dual-modal fine-tuning methods",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_05.png"
  },
  {
    "page": 1,
    "bbox": [
      386.2360534667969,
      610.3433685302734,
      530.2751007080078,
      720.1002807617188
    ],
    "caption": "(c) Dual-modal fine-tuning methods",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      189.29164123535156,
      560.6405487060547,
      421.8914337158203,
      719.99853515625
    ],
    "caption": "Figure 2: Effect of CLIP pre-tained on natural datasets vs BiomedCLIP pre-tained on biomedical datasets on visual saliency maps. Red lines indicate foreground regions, yellow lines represent background regions associated with object.",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_07.png"
  },
  {
    "page": 2,
    "bbox": [
      189.29164123535156,
      560.6405487060547,
      421.8914337158203,
      719.99853515625
    ],
    "caption": "Figure 2: Effect of CLIP pre-tained on natural datasets vs BiomedCLIP pre-tained on biomedical datasets on visual saliency maps. Red lines indicate foreground regions, yellow lines represent background regions associated with object.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      81.33457946777344,
      648.1096343994141,
      139.07191467285156,
      707.1399536132812
    ],
    "caption": "OCTMNIST",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/table_97.png"
  },
  {
    "page": 3,
    "bbox": [
      313.1475830078125,
      648.5665740966797,
      370.8849182128906,
      706.6820526123047
    ],
    "caption": "Kvasir",
    "file_name": [
      "figures/fileoutpart11.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/table_147.png"
  },
  {
    "page": 3,
    "bbox": [
      313.14756774902344,
      557.0194702148438,
      370.8849182128906,
      614.5657043457031
    ],
    "caption": "Figure 3: Comparison of classification accuracies (%) of different initialization methods. [CLS] denotes [CLASS].",
    "file_name": [
      "figures/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/table_171.png"
  },
  {
    "page": 5,
    "bbox": [
      76.58772277832031,
      413.46380615234375,
      535.4147186279297,
      720.0940246582031
    ],
    "caption": "Figure 4: Overview of the Biomed-DPT framework, which combines LLM prompt generation, fixed clinical prompt templates, learnable context, zero vector as a soft prompt, and BiomedCLIP to construct a unified multimodal representation space. In this method, the prompt ensemble strategy is used to integrate text and image features, and the cross-entropy, L1 constraint, and KL divergence loss are minimized to achieve effective few-shot learning.",
    "file_name": [
      "figures/fileoutpart13.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_12.png"
  },
  {
    "page": 5,
    "bbox": [
      76.58772277832031,
      413.46380615234375,
      535.4147186279297,
      720.0940246582031
    ],
    "caption": "Figure 4: Overview of the Biomed-DPT framework, which combines LLM prompt generation, fixed clinical prompt templates, learnable context, zero vector as a soft prompt, and BiomedCLIP to construct a unified multimodal representation space. In this method, the prompt ensemble strategy is used to integrate text and image features, and the cross-entropy, L1 constraint, and KL divergence loss are minimized to achieve effective few-shot learning.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      150.29800415039062,
      205.84490966796875,
      211.57420349121094,
      225.28646850585938
    ],
    "caption": "e j 0 = Embed(I j ), e j 0 ∈ Rd , j = 1, 2, . . . ,N p.",
    "file_name": [
      "figures/fileoutpart16.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_14.png"
  },
  {
    "page": 6,
    "bbox": [
      174.01699829101562,
      212.3919219970703,
      206.73109436035156,
      225.28646850585938
    ],
    "caption": "e j 0 = Embed(I j ), e j 0 ∈ Rd , j = 1, 2, . . . ,N p.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      150.29800415039062,
      205.84490966796875,
      211.57420349121094,
      225.28646850585938
    ],
    "caption": "e j 0 = Embed(I j ), e j 0 ∈ Rd , j = 1, 2, . . . ,N p.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      376.05299377441406,
      205.84490966796875,
      542.4925537109375,
      225.28646850585938
    ],
    "caption": "e j 0 = Embed(I j ), e j 0 ∈ Rd , j = 1, 2, . . . ,N p.",
    "file_name": [
      "figures/fileoutpart17.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_17.png"
  },
  {
    "page": 6,
    "bbox": [
      385.3789978027344,
      212.17054748535156,
      390.4210510253906,
      220.1764678955078
    ],
    "caption": "e j 0 = Embed(I j ), e j 0 ∈ Rd , j = 1, 2, . . . ,N p.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      408.9510040283203,
      212.3919219970703,
      437.5435791015625,
      225.28646850585938
    ],
    "caption": "Let and denote the image embeddings and visual prompts at the l-th Transformer layer respectively, where xl represents a class token in the image encoder. We introduce zero vector as a soft prompt in each transformer block Φl:",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      492.89599609375,
      212.17054748535156,
      497.00360107421875,
      219.14434814453125
    ],
    "caption": "Let and denote the image embeddings and visual prompts at the l-th Transformer layer respectively, where xl represents a class token in the image encoder. We introduce zero vector as a soft prompt in each transformer block Φl:",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      524.8780059814453,
      212.17054748535156,
      530.7290191650391,
      220.1764678955078
    ],
    "caption": "(1)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      376.05299377441406,
      205.84490966796875,
      542.4925537109375,
      225.28646850585938
    ],
    "caption": "e j 0 = Embed(I j ), e j 0 ∈ Rd , j = 1, 2, . . . ,N p.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.6490478515625,
      165.74090576171875,
      407.84825134277344,
      186.35546875
    ],
    "caption": "E= {e∈ R| j ∈ N, 1 ≤ j ≤ N}",
    "file_name": [
      "figures/fileoutpart18.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_23.png"
  },
  {
    "page": 6,
    "bbox": [
      206.6490478515625,
      165.74090576171875,
      407.84825134277344,
      186.35546875
    ],
    "caption": "E= {e∈ R| j ∈ N, 1 ≤ j ≤ N}",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      87.71499633789062,
      143.60189819335938,
      242.3229522705078,
      164.2164764404297
    ],
    "caption": "[xl, Pl, El] = Φl([xl−1, Pl−1, El−1]), l = 1, 2, · · · , L.",
    "file_name": [
      "figures/fileoutpart19.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_25.png"
  },
  {
    "page": 6,
    "bbox": [
      95.06900024414062,
      149.9285430908203,
      99.32998657226562,
      157.93447875976562
    ],
    "caption": "Given an image I and its label y, we employ a visual encoder ϕ(·) to extract visual embeddings. The entire image feature extraction process can be simply expressed as:",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      121.031005859375,
      156.21055603027344,
      126.07305908203125,
      164.2164764404297
    ],
    "caption": "l",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      121.031005859375,
      148.4208221435547,
      123.54849243164062,
      155.39462280273438
    ],
    "caption": "Given an image I and its label y, we employ a visual encoder ϕ(·) to extract visual embeddings. The entire image feature extraction process can be simply expressed as:",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      144.60000610351562,
      155.03854370117188,
      150.49285888671875,
      163.0444793701172
    ],
    "caption": "Given an image I and its label y, we employ a visual encoder ϕ(·) to extract visual embeddings. The entire image feature extraction process can be simply expressed as:",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      230.24600219726562,
      149.9285430908203,
      234.35360717773438,
      156.90234375
    ],
    "caption": "[xl, Pl, El] = Φl([xl−1, Pl−1, El−1]), l = 1, 2, · · · , L.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      87.71499633789062,
      143.60189819335938,
      242.3229522705078,
      164.2164764404297
    ],
    "caption": "[xl, Pl, El] = Φl([xl−1, Pl−1, El−1]), l = 1, 2, · · · , L.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      259.13600158691406,
      143.60189819335938,
      404.9463806152344,
      163.0444793701172
    ],
    "caption": "[xl, Pl, El] = Φl([xl−1, Pl−1, El−1]), l = 1, 2, · · · , L.",
    "file_name": [
      "figures/fileoutpart20.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_32.png"
  },
  {
    "page": 6,
    "bbox": [
      265.53199768066406,
      149.9285430908203,
      269.7929992675781,
      157.93447875976562
    ],
    "caption": "[xl, Pl, El] = Φl([xl−1, Pl−1, El−1]), l = 1, 2, · · · , L.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      291.8370056152344,
      155.03854370117188,
      296.3978576660156,
      163.0444793701172
    ],
    "caption": "[xl, Pl, El] = Φl([xl−1, Pl−1, El−1]), l = 1, 2, · · · , L.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      291.8370056152344,
      148.6017303466797,
      294.3544921875,
      155.57553100585938
    ],
    "caption": "[xl, Pl, El] = Φl([xl−1, Pl−1, El−1]), l = 1, 2, · · · , L.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      314.5249938964844,
      155.03854370117188,
      320.41786193847656,
      163.0444793701172
    ],
    "caption": "[xl, Pl, El] = Φl([xl−1, Pl−1, El−1]), l = 1, 2, · · · , L.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      259.13600158691406,
      143.60189819335938,
      404.9463806152344,
      163.0444793701172
    ],
    "caption": "[xl, Pl, El] = Φl([xl−1, Pl−1, El−1]), l = 1, 2, · · · , L.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      192.7666015625,
      107.46389770507812,
      421.7277374267578,
      125.97441101074219
    ],
    "caption": "Given an image I and its label y, we employ a visual encoder ϕ(·) to extract visual embeddings. The entire image feature extraction process can be simply expressed as:",
    "file_name": [
      "figures/fileoutpart21.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_38.png"
  },
  {
    "page": 6,
    "bbox": [
      192.7666015625,
      107.46389770507812,
      421.7277374267578,
      125.97441101074219
    ],
    "caption": "Given an image I and its label y, we employ a visual encoder ϕ(·) to extract visual embeddings. The entire image feature extraction process can be simply expressed as:",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      249.3542022705078,
      68.76254272460938,
      365.1427001953125,
      80.95016479492188
    ],
    "caption": "Given an image I and its label y, we employ a visual encoder ϕ(·) to extract visual embeddings. The entire image feature extraction process can be simply expressed as:",
    "file_name": [
      "figures/fileoutpart22.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_40.png"
  },
  {
    "page": 6,
    "bbox": [
      249.3542022705078,
      68.76254272460938,
      365.1427001953125,
      80.95016479492188
    ],
    "caption": "Given an image I and its label y, we employ a visual encoder ϕ(·) to extract visual embeddings. The entire image feature extraction process can be simply expressed as:",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      222.18600463867188,
      647.7447052001953,
      391.09947204589844,
      694.1320953369141
    ],
    "caption": "where f ∈ Rd represents the feature vector extracted by the image encoder, wi ∈ Rd denotes the weight vector generated by the text encoder for the extended text of the i-th class, sim(·) indicates the similarity function, K is the total number of classes, and τ is the temperature parameter learned by CLIP to adjust the smoothness of the probability distribution. The final prediction result of the model is given by:",
    "file_name": [
      "figures/fileoutpart23.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_42.png"
  },
  {
    "page": 7,
    "bbox": [
      222.18600463867188,
      647.7447052001953,
      391.09947204589844,
      694.1320953369141
    ],
    "caption": "where f ∈ Rd represents the feature vector extracted by the image encoder, wi ∈ Rd denotes the weight vector generated by the text encoder for the extended text of the i-th class, sim(·) indicates the similarity function, K is the total number of classes, and τ is the temperature parameter learned by CLIP to adjust the smoothness of the probability distribution. The final prediction result of the model is given by:",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      255.97799682617188,
      625.5608978271484,
      357.3203125,
      657.7675170898438
    ],
    "caption": "where f ∈ Rd represents the feature vector extracted by the image encoder, wi ∈ Rd denotes the weight vector generated by the text encoder for the extended text of the i-th class, sim(·) indicates the similarity function, K is the total number of classes, and τ is the temperature parameter learned by CLIP to adjust the smoothness of the probability distribution. The final prediction result of the model is given by:",
    "file_name": [
      "figures/fileoutpart24.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_44.png"
  },
  {
    "page": 7,
    "bbox": [
      255.97799682617188,
      625.5608978271484,
      357.3203125,
      657.7675170898438
    ],
    "caption": "where f ∈ Rd represents the feature vector extracted by the image encoder, wi ∈ Rd denotes the weight vector generated by the text encoder for the extended text of the i-th class, sim(·) indicates the similarity function, K is the total number of classes, and τ is the temperature parameter learned by CLIP to adjust the smoothness of the probability distribution. The final prediction result of the model is given by:",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      254.26576232910156,
      558.4479064941406,
      360.2284240722656,
      576.9600219726562
    ],
    "caption": "In the computation process of the text encoder, the dynamically generated learnable context vectors can automatically extract semantic features highly relevant to the current task, avoiding the prior bias introduced by manual prompt design. In the computation process of the image encoder, zero vectors dynamically adjust the attention weight distribution among image patches in the self-attention layer, reducing excessive reliance on local features. In the context of biomedical image analysis, this Dual Modality Prompt Tuning enables the model to simultaneously capture fine-grained visual features of lesions and clinically relevant semantic contexts, thereby achieving precise alignment of vision-language representations.",
    "file_name": [
      "figures/fileoutpart25.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_46.png"
  },
  {
    "page": 7,
    "bbox": [
      254.26576232910156,
      558.4479064941406,
      360.2284240722656,
      576.9600219726562
    ],
    "caption": "In the computation process of the text encoder, the dynamically generated learnable context vectors can automatically extract semantic features highly relevant to the current task, avoiding the prior bias introduced by manual prompt design. In the computation process of the image encoder, zero vectors dynamically adjust the attention weight distribution among image patches in the self-attention layer, reducing excessive reliance on local features. In the context of biomedical image analysis, this Dual Modality Prompt Tuning enables the model to simultaneously capture fine-grained visual features of lesions and clinically relevant semantic contexts, thereby achieving precise alignment of vision-language representations.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      258.6109924316406,
      290.28770446777344,
      355.8812561035156,
      328.7234191894531
    ],
    "caption": "At the distribution level, to further enhance prediction-level consistency, we define the KL divergence loss:",
    "file_name": [
      "figures/fileoutpart26.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_48.png"
  },
  {
    "page": 7,
    "bbox": [
      258.6109924316406,
      290.28770446777344,
      355.8812561035156,
      328.7234191894531
    ],
    "caption": "At the distribution level, to further enhance prediction-level consistency, we define the KL divergence loss:",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      238.0970001220703,
      202.34471130371094,
      373.94549560546875,
      240.78041076660156
    ],
    "caption": "where the teacher distribution pt and student distribution ps are calculated by Equation (<>)4. By optimizing the KL divergence loss function, we maintain consistency between the GPT-prompted logits distribution and the pre-trained BiomedCLIP logits distribution.",
    "file_name": [
      "figures/fileoutpart27.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_50.png"
  },
  {
    "page": 7,
    "bbox": [
      238.0970001220703,
      202.34471130371094,
      373.94549560546875,
      240.78041076660156
    ],
    "caption": "where the teacher distribution pt and student distribution ps are calculated by Equation (<>)4. By optimizing the KL divergence loss function, we maintain consistency between the GPT-prompted logits distribution and the pre-trained BiomedCLIP logits distribution.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      245.29942321777344,
      101.97270202636719,
      367.94049072265625,
      140.40841674804688
    ],
    "caption": "At the task level, to ensure model classification performance, we define the cross-entropy loss between predictions ps and ground truth labels y as:",
    "file_name": [
      "figures/fileoutpart28.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_52.png"
  },
  {
    "page": 7,
    "bbox": [
      245.29942321777344,
      101.97270202636719,
      367.94049072265625,
      140.40841674804688
    ],
    "caption": "At the task level, to ensure model classification performance, we define the cross-entropy loss between predictions ps and ground truth labels y as:",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      247.73899841308594,
      684.701904296875,
      365.50791931152344,
      703.2124176025391
    ],
    "caption": "where λ1 and λ2 are weighting coefficients used to balance the contributions of each loss term.",
    "file_name": [
      "figures/fileoutpart29.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_54.png"
  },
  {
    "page": 8,
    "bbox": [
      247.73899841308594,
      684.701904296875,
      365.50791931152344,
      703.2124176025391
    ],
    "caption": "where λ1 and λ2 are weighting coefficients used to balance the contributions of each loss term.",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      100.46580505371094,
      442.5222625732422,
      513.0596618652344,
      694.8703002929688
    ],
    "caption": "Figure 5: Comparison with hand-crafted prompts “a photo of a [CLASS]”.",
    "file_name": [
      "figures/fileoutpart36.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_56.png"
  },
  {
    "page": 11,
    "bbox": [
      100.46580505371094,
      442.5222625732422,
      513.0596618652344,
      694.8703002929688
    ],
    "caption": "Figure 5: Comparison with hand-crafted prompts “a photo of a [CLASS]”.",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      80.74252319335938,
      415.3115539550781,
      528.6748657226562,
      720.1356811523438
    ],
    "caption": "Figure 6: Effect of various text prompt techniques on visual saliency maps. Columns (b)-(f) represent different prompt methods.",
    "file_name": [
      "figures/fileoutpart45.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_58.png"
  },
  {
    "page": 14,
    "bbox": [
      80.74252319335938,
      415.3115539550781,
      528.6748657226562,
      720.1356811523438
    ],
    "caption": "Figure 6: Effect of various text prompt techniques on visual saliency maps. Columns (b)-(f) represent different prompt methods.",
    "file_name": ""
  },
  {
    "page": 17,
    "bbox": [
      163.607421875,
      466.84674072265625,
      448.32872009277344,
      720.0011138916016
    ],
    "caption": "Figure 1: Average classification accuracy (%) of various fewshot adaptation methods across different numbers of training shots per class.",
    "file_name": [
      "figures/fileoutpart46.png"
    ],
    "output_file": "assets/images/paper/Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models/fig_60.png"
  }
]