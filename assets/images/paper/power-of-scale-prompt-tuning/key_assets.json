{
  "architecture": {
    "page": 4,
    "bbox": [
      103.39723205566406,
      628.7255554199219,
      273.4548645019531,
      772.7052154541016
    ],
    "caption": "108 109 1010 Model Parameters 10 20 30 40 50 60 70 80 90 100 SuperGLUE Score Span Corruption Span Corruption + Sentinel LM Adaptation (100K) (c) Pre-training method",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/Power-of-Scale-Prompt-Tuning/fig_06.png"
  },
  "results": {
    "page": 6,
    "bbox": [
      323.9228057861328,
      687.2742462158203,
      508.50987243652344,
      769.1190338134766
    ],
    "caption": "Table 1: F1 mean and stddev for models trained on SQuAD and evaluated on out-of-domain datasets from the MRQA 2019 shared task. Prompt tuning tends to give stronger zero-shot performance than model tuning, especially on datasets with large domain shifts like TextbookQA.",
    "file_name": [
      "tables/fileoutpart9.xlsx",
      "tables/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/Power-of-Scale-Prompt-Tuning/table_01.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/Power-of-Scale-Prompt-Tuning/fig_06.png)\n캡션: 108 109 1010 Model Parameters 10 20 30 40 50 60 70 80 90 100 SuperGLUE Score Span Corruption Span Corruption + Sentinel LM Adaptation (100K) (c) Pre-training method\n\n### Main Results Table\n![Results](/assets/images/paper/Power-of-Scale-Prompt-Tuning/table_01.png)\n캡션: Table 1: F1 mean and stddev for models trained on SQuAD and evaluated on out-of-domain datasets from the MRQA 2019 shared task. Prompt tuning tends to give stronger zero-shot performance than model tuning, especially on datasets with large domain shifts like TextbookQA."
}