[
  {
    "page": -1,
    "bbox": [
      312.8758239746094,
      458.6111755371094,
      509.9000549316406,
      629.3762664794922
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/Power-of-Scale-Prompt-Tuning/fig_01.png"
  },
  {
    "page": -1,
    "bbox": [
      312.8758239746094,
      458.6111755371094,
      509.9000549316406,
      629.3762664794922
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      66.31343078613281,
      660.6093902587891,
      289.0672302246094,
      771.2112579345703
    ],
    "caption": "Figure 2: Model tuning requires making a task-specific copy of the entire pre-trained model for each downstream task and inference must be performed in separate batches. Prompt tuning only requires storing a small task-specific prompt for each task, and enables mixed-task inference using the original pre-trained model. With a T5 “XXL” model, each copy of the tuned model requires 11 billion parameters. By contrast, our tuned prompts would only require 20,480 parameters per task—a reduction of over five orders of magnitude—assuming a prompt length of 5 tokens.",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/Power-of-Scale-Prompt-Tuning/fig_03.png"
  },
  {
    "page": 1,
    "bbox": [
      66.31343078613281,
      660.6093902587891,
      289.0672302246094,
      771.2112579345703
    ],
    "caption": "Figure 2: Model tuning requires making a task-specific copy of the entire pre-trained model for each downstream task and inference must be performed in separate batches. Prompt tuning only requires storing a small task-specific prompt for each task, and enables mixed-task inference using the original pre-trained model. With a T5 “XXL” model, each copy of the tuned model requires 11 billion parameters. By contrast, our tuned prompts would only require 20,480 parameters per task—a reduction of over five orders of magnitude—assuming a prompt length of 5 tokens.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      91.25700378417969,
      454.32899475097656,
      104.80632019042969,
      459.1786193847656
    ],
    "caption": "Our frozen models are built on top of pre-trained T5 checkpoints of all sizes (Small, Base, Large, XL, XXL). We leverage the public T5 1.1 checkpoints, which include improvements over the original T5.2 (<>)",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/Power-of-Scale-Prompt-Tuning/fig_05.png"
  },
  {
    "page": 4,
    "bbox": [
      103.39723205566406,
      628.7255554199219,
      273.4548645019531,
      772.7052154541016
    ],
    "caption": "108 109 1010 Model Parameters 10 20 30 40 50 60 70 80 90 100 SuperGLUE Score Span Corruption Span Corruption + Sentinel LM Adaptation (100K) (c) Pre-training method",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/Power-of-Scale-Prompt-Tuning/fig_06.png"
  },
  {
    "page": 4,
    "bbox": [
      103.39723205566406,
      628.7255554199219,
      273.4548645019531,
      772.7052154541016
    ],
    "caption": "108 109 1010 Model Parameters 10 20 30 40 50 60 70 80 90 100 SuperGLUE Score Span Corruption Span Corruption + Sentinel LM Adaptation (100K) (c) Pre-training method",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      324.39222717285156,
      628.7255554199219,
      494.4498748779297,
      772.7052154541016
    ],
    "caption": "108 109 1010 Model Parameters 10 20 30 40 50 60 70 80 90 100 SuperGLUE Score 0K 10K 50K 100K (d) LM adaptation steps",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/Power-of-Scale-Prompt-Tuning/fig_08.png"
  },
  {
    "page": 4,
    "bbox": [
      324.39222717285156,
      628.7255554199219,
      494.4498748779297,
      772.7052154541016
    ],
    "caption": "108 109 1010 Model Parameters 10 20 30 40 50 60 70 80 90 100 SuperGLUE Score 0K 10K 50K 100K (d) LM adaptation steps",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      103.39723205566406,
      480.05255126953125,
      273.4548645019531,
      624.0332183837891
    ],
    "caption": "Figure 3: Ablations of various hyperparameters on prompt tuning performance (mean and stddev across 3 runs). In our “(<>)default” ( ) configuration, quality improves stably with model size. Across all ablations, the largest (XXL) model is the most robust to hyperparameter choice. (<>)(a) Prompt length: Increasing to 20+ tokens generally confers a large boost, but XXL performs well even with single-token prompts. (<>)(b) Prompt initialization: Random uniform initialization lags behind more “advanced” initializations using sampled vocabulary or class label embeddings, but the difference vanishes at XXL size. (<>)(c) Pre-training objective: LM adaptation outperforms span corruption, even when a sentinel is added to downstream task targets, but XXL works well with any method. (<>)(d) LM adaptation: Longer adaptation generally gives larger gains, but XXL is robust to even short adaptation.",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/Power-of-Scale-Prompt-Tuning/fig_10.png"
  },
  {
    "page": 4,
    "bbox": [
      103.39723205566406,
      480.05255126953125,
      273.4548645019531,
      624.0332183837891
    ],
    "caption": "Figure 3: Ablations of various hyperparameters on prompt tuning performance (mean and stddev across 3 runs). In our “(<>)default” ( ) configuration, quality improves stably with model size. Across all ablations, the largest (XXL) model is the most robust to hyperparameter choice. (<>)(a) Prompt length: Increasing to 20+ tokens generally confers a large boost, but XXL performs well even with single-token prompts. (<>)(b) Prompt initialization: Random uniform initialization lags behind more “advanced” initializations using sampled vocabulary or class label embeddings, but the difference vanishes at XXL size. (<>)(c) Pre-training objective: LM adaptation outperforms span corruption, even when a sentinel is added to downstream task targets, but XXL works well with any method. (<>)(d) LM adaptation: Longer adaptation generally gives larger gains, but XXL is robust to even short adaptation.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      324.39222717285156,
      480.05255126953125,
      494.4498748779297,
      624.0332183837891
    ],
    "caption": "Figure 3: Ablations of various hyperparameters on prompt tuning performance (mean and stddev across 3 runs). In our “(<>)default” ( ) configuration, quality improves stably with model size. Across all ablations, the largest (XXL) model is the most robust to hyperparameter choice. (<>)(a) Prompt length: Increasing to 20+ tokens generally confers a large boost, but XXL performs well even with single-token prompts. (<>)(b) Prompt initialization: Random uniform initialization lags behind more “advanced” initializations using sampled vocabulary or class label embeddings, but the difference vanishes at XXL size. (<>)(c) Pre-training objective: LM adaptation outperforms span corruption, even when a sentinel is added to downstream task targets, but XXL works well with any method. (<>)(d) LM adaptation: Longer adaptation generally gives larger gains, but XXL is robust to even short adaptation.",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/Power-of-Scale-Prompt-Tuning/fig_12.png"
  },
  {
    "page": 4,
    "bbox": [
      324.39222717285156,
      480.05255126953125,
      494.4498748779297,
      624.0332183837891
    ],
    "caption": "Figure 3: Ablations of various hyperparameters on prompt tuning performance (mean and stddev across 3 runs). In our “(<>)default” ( ) configuration, quality improves stably with model size. Across all ablations, the largest (XXL) model is the most robust to hyperparameter choice. (<>)(a) Prompt length: Increasing to 20+ tokens generally confers a large boost, but XXL performs well even with single-token prompts. (<>)(b) Prompt initialization: Random uniform initialization lags behind more “advanced” initializations using sampled vocabulary or class label embeddings, but the difference vanishes at XXL size. (<>)(c) Pre-training objective: LM adaptation outperforms span corruption, even when a sentinel is added to downstream task targets, but XXL works well with any method. (<>)(d) LM adaptation: Longer adaptation generally gives larger gains, but XXL is robust to even short adaptation.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      129.27699279785156,
      449.2969970703125,
      141.22689819335938,
      453.5741424560547
    ],
    "caption": "108 109 1010 Model Parameters 10 20 30 40 50 60 70 80 90 100 SuperGLUE Score Span Corruption Span Corruption + Sentinel LM Adaptation (100K) (c) Pre-training method",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/Power-of-Scale-Prompt-Tuning/fig_14.png"
  },
  {
    "page": 5,
    "bbox": [
      316.5731201171875,
      597.0995483398438,
      513.9768676757812,
      771.1064453125
    ],
    "caption": "Figure 4: Parameter usage of various adaptation techniques, fixing architecture to T5 1.1 and prompt/prefix length to 1–100 tokens (bands show mean and stddev). Model Tuning: All parameters are task-specific. Prefix Tuning: Activations are tuned in the prefix of each layer, requiring 0.1–1% task-specific parameters for inference, but more are used for training. WARP: Task parameters are reduced to under 0.1% by only tuning input and output layers. Prompt Tuning: Only prompt embeddings are tuned, reaching under 0.01% for most model sizes. Prompt Design: Only a sequence of prompt IDs (500–2000 tokens) is required.",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/Power-of-Scale-Prompt-Tuning/fig_15.png"
  },
  {
    "page": 5,
    "bbox": [
      316.5731201171875,
      597.0995483398438,
      513.9768676757812,
      771.1064453125
    ],
    "caption": "Figure 4: Parameter usage of various adaptation techniques, fixing architecture to T5 1.1 and prompt/prefix length to 1–100 tokens (bands show mean and stddev). Model Tuning: All parameters are task-specific. Prefix Tuning: Activations are tuned in the prefix of each layer, requiring 0.1–1% task-specific parameters for inference, but more are used for training. WARP: Task parameters are reduced to under 0.1% by only tuning input and output layers. Prompt Tuning: Only prompt embeddings are tuned, reaching under 0.01% for most model sizes. Prompt Design: Only a sequence of prompt IDs (500–2000 tokens) is required.",
    "file_name": ""
  }
]