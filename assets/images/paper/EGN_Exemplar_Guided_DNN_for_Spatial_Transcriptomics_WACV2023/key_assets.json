{
  "architecture": {
    "page": 2,
    "bbox": [
      76.71612548828125,
      524.8915100097656,
      519.4253540039062,
      719.1414642333984
    ],
    "caption": "Figure 2: EGN framework. Our networks are trained in a two-stage manner. In the stage of unsupervised exemplar retrieval (Sec. 3.1), we learn an extractor E(·) and a decoder G(·) with image reconstruction objectives. After convergence, we use the extractor E(·) with a distance metric for unsupervised exemplar retrieval. For example, given Xi, we obtain the global view of Xi, i.e., ei and ei = E(Xi), and construct the nearest exemplar set KXi = {ej , yj }, where ej is the exemplar global view, and yj is the exemplar gene expression. In the stage of exemplar learning (Sec. 3.2), we train a network C(·, ·, ·) to predict gene expression yi from Xi, ei, and KXi . We use a vision transformer (ViT) as our backbone. The proposed EB block is interleaved with the vision transformer blocks. With a projector, we refine ei and KXi = {ej , yj} to h0 , {r0}, and {s0}. Then, they are used by the EB block to revise the ViT patch representation and are updated to ht , {rt}, and {st}, where 1 ≤ t ≤ T, and T is the number of layers. Finally, we have a prediction block that concatenates the refined global view hT of Xi and the attention-pooled ViT patch representation, to achieve the gene expression prediction task.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/EGN_Exemplar_Guided_DNN_for_Spatial_Transcriptomics_WACV2023/fig_06.png"
  },
  "results": {
    "page": 7,
    "bbox": [
      351.51829528808594,
      597.4037628173828,
      520.4046783447266,
      671.3178405761719
    ],
    "caption": "Table 4: Ablation study on model architectures.",
    "file_name": [
      "tables/fileoutpart36.xlsx",
      "tables/fileoutpart37.png"
    ],
    "output_file": "assets/images/paper/EGN_Exemplar_Guided_DNN_for_Spatial_Transcriptomics_WACV2023/table_205.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/EGN_Exemplar_Guided_DNN_for_Spatial_Transcriptomics_WACV2023/fig_06.png)\n캡션: Figure 2: EGN framework. Our networks are trained in a two-stage manner. In the stage of unsupervised exemplar retrieval (Sec. 3.1), we learn an extractor E(·) and a decoder G(·) with image reconstruction objectives. After convergence, we use the extractor E(·) with a distance metric for unsupervised exemplar retrieval. For example, given Xi, we obtain the global view of Xi, i.e., ei and ei = E(Xi…\n\n### Main Results Table\n![Results](/assets/images/paper/EGN_Exemplar_Guided_DNN_for_Spatial_Transcriptomics_WACV2023/table_205.png)\n캡션: Table 4: Ablation study on model architectures."
}