{
  "architecture": {
    "page": 1,
    "bbox": [
      56.80509948730469,
      553.4217071533203,
      541.0757904052734,
      725.2994232177734
    ],
    "caption": "Figure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classifier to predict some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_01.png"
  },
  "results": {
    "page": 45,
    "bbox": [
      60.11004638671875,
      370.2334442138672,
      285.28562927246094,
      482.19395446777344
    ],
    "caption": "Table 14. OCR performance on 5 datasets. All metrics are accuracy on the test set except for Hateful Memes which reports ROC AUC on the dev set. Single model SOTA reported to best of knowledge. ES Best reports the best performance across the 56 non-CLIP models in our evaluation suite. a((<>)Assiri, (<>)2020) b((<>)Jaderberg et al., (<>)2015) c((<>)Wang et al., (<>)2020) d((<>)Lippe et al., (<>)2020) f ((<>)Jaderberg et al., (<>)2014) g ((<>)Wang et al., (<>)2018) h((<>)Xie et al., (<>)2020) i((<>)Mahajan et al., (<>)2018)",
    "file_name": [
      "tables/fileoutpart59.xlsx",
      "tables/fileoutpart60.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/table_5523.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_01.png)\n캡션: Figure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classifier to predict some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descript…\n\n### Main Results Table\n![Results](/assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/table_5523.png)\n캡션: Table 14. OCR performance on 5 datasets. All metrics are accuracy on the test set except for Hateful Memes which reports ROC AUC on the dev set. Single model SOTA reported to best of knowledge. ES Best reports the best performance across the 56 non-CLIP models in our evaluation suite. a((<>)Assiri, (<>)2020) b((<>)Jaderberg et al., (<>)2015) c((<>)Wang et al., (<>)2020) d((<>)Lippe et al., (<>)202…"
}