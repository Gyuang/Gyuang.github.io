[
  {
    "page": 1,
    "bbox": [
      56.80509948730469,
      553.4217071533203,
      541.0757904052734,
      725.2994232177734
    ],
    "caption": "Figure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classifier to predict some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_01.png"
  },
  {
    "page": 1,
    "bbox": [
      56.80509948730469,
      553.4217071533203,
      541.0757904052734,
      725.2994232177734
    ],
    "caption": "Figure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classifier to predict some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      52.94061279296875,
      571.7252349853516,
      290.53623962402344,
      725.0925750732422
    ],
    "caption": "Figure 2. CLIP is much more efficient at zero-shot transfer than our image caption baseline. Although highly expressive, we found that transformer-based language models are relatively weak at zero-shot ImageNet classification. Here, we see that it learns 3x slower than a baseline which predicts a bag-of-words (BoW) encoding of the text ((<>)Joulin et al., (<>)2016). Swapping the prediction objective for the contrastive objective of CLIP further improves efficiency another 4x.",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_03.png"
  },
  {
    "page": 2,
    "bbox": [
      52.94061279296875,
      571.7252349853516,
      290.53623962402344,
      725.0925750732422
    ],
    "caption": "Figure 2. CLIP is much more efficient at zero-shot transfer than our image caption baseline. Although highly expressive, we found that transformer-based language models are relatively weak at zero-shot ImageNet classification. Here, we see that it learns 3x slower than a baseline which predicts a bag-of-words (BoW) encoding of the text ((<>)Joulin et al., (<>)2016). Swapping the prediction objective for the contrastive objective of CLIP further improves efficiency another 4x.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      55.851104736328125,
      562.4678344726562,
      260.3179016113281,
      581.6179046630859
    ],
    "caption": "# symmetric loss function labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss   = (loss_i + loss_t)/2",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_05.png"
  },
  {
    "page": 4,
    "bbox": [
      55.851104736328125,
      562.4678344726562,
      260.3179016113281,
      581.6179046630859
    ],
    "caption": "# symmetric loss function labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss   = (loss_i + loss_t)/2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      304.41114807128906,
      496.2886047363281,
      542.7569274902344,
      727.9685516357422
    ],
    "caption": "Figure 4. Prompt engineering and ensembling improve zero-shot performance. Compared to the baseline of using contextless class names, prompt engineering and ensembling boost zero-shot classification performance by almost 5 points on average across 36 datasets. This improvement is similar to the gain from using 4 times more compute with the baseline zero-shot method but is “free” when amortized over many predictions.",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_07.png"
  },
  {
    "page": 6,
    "bbox": [
      304.41114807128906,
      496.2886047363281,
      542.7569274902344,
      727.9685516357422
    ],
    "caption": "Figure 4. Prompt engineering and ensembling improve zero-shot performance. Compared to the baseline of using contextless class names, prompt engineering and ensembling boost zero-shot classification performance by almost 5 points on average across 36 datasets. This improvement is similar to the gain from using 4 times more compute with the baseline zero-shot method but is “free” when amortized over many predictions.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      307.2313995361328,
      460.73548889160156,
      541.6510009765625,
      727.3275299072266
    ],
    "caption": "Figure 5. Zero-shot CLIP is competitive with a fully supervised baseline. Across a 27 dataset eval suite, a zero-shot CLIP classifier outperforms a fully supervised linear classifier fitted on ResNet-50 features on 16 datasets, including ImageNet.",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_09.png"
  },
  {
    "page": 7,
    "bbox": [
      307.2313995361328,
      460.73548889160156,
      541.6510009765625,
      727.3275299072266
    ],
    "caption": "Figure 5. Zero-shot CLIP is competitive with a fully supervised baseline. Across a 27 dataset eval suite, a zero-shot CLIP classifier outperforms a fully supervised linear classifier fitted on ResNet-50 features on 16 datasets, including ImageNet.",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      51.90461730957031,
      496.8518981933594,
      292.9315185546875,
      728.4748840332031
    ],
    "caption": "Figure 6. Zero-shot CLIP outperforms few-shot linear probes. Zero-shot CLIP matches the average performance of a 4-shot linear classifier trained on the same feature space and nearly matches the best results of a 16-shot linear classifier across publicly available models. For both BiT-M and SimCLRv2, the best performing model is highlighted. Light gray lines are other models in the eval suite. The 20 datasets with at least 16 examples per class were used in this analysis.",
    "file_name": [
      "figures/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_11.png"
  },
  {
    "page": 8,
    "bbox": [
      51.90461730957031,
      496.8518981933594,
      292.9315185546875,
      728.4748840332031
    ],
    "caption": "Figure 6. Zero-shot CLIP outperforms few-shot linear probes. Zero-shot CLIP matches the average performance of a 4-shot linear classifier trained on the same feature space and nearly matches the best results of a 16-shot linear classifier across publicly available models. For both BiT-M and SimCLRv2, the best performing model is highlighted. Light gray lines are other models in the eval suite. The 20 datasets with at least 16 examples per class were used in this analysis.",
    "file_name": ""
  },
  {
    "page": 9,
    "bbox": [
      55.28094482421875,
      523.9513702392578,
      291.0343017578125,
      727.1168975830078
    ],
    "caption": "Figure 7. The data efficiency of zero-shot transfer varies widely. Calculating the number of labeled examples per class a linear classifier on the same CLIP feature space requires to match the performance of the zero-shot classifier contextualizes the effectiveness of zero-shot transfer. Values are estimated based on log-linear interpolation of 1, 2, 4, 8, 16-shot and fully supervised results. Performance varies widely from still underperforming a one-shot classifier on two datasets to matching an estimated 184 labeled examples per class.",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_13.png"
  },
  {
    "page": 9,
    "bbox": [
      55.28094482421875,
      523.9513702392578,
      291.0343017578125,
      727.1168975830078
    ],
    "caption": "Figure 7. The data efficiency of zero-shot transfer varies widely. Calculating the number of labeled examples per class a linear classifier on the same CLIP feature space requires to match the performance of the zero-shot classifier contextualizes the effectiveness of zero-shot transfer. Values are estimated based on log-linear interpolation of 1, 2, 4, 8, 16-shot and fully supervised results. Performance varies widely from still underperforming a one-shot classifier on two datasets to matching an estimated 184 labeled examples per class.",
    "file_name": ""
  },
  {
    "page": 9,
    "bbox": [
      287.3677978515625,
      307.66172790527344,
      541.6050262451172,
      728.1352386474609
    ],
    "caption": "If we assume that evaluation datasets are large enough that the parameters of linear classifiers trained on them are well estimated, then, because CLIP’s zero-shot classifier is also a linear classifier, the performance of the fully supervised classifiers roughly sets an upper bound for what zero-shot transfer can achieve. In Figure (<>)8 we compare CLIP’s zero-shot performance with fully supervised linear classifiers across datasets. The dashed, y = x line represents an “optimal” zero-shot classifier that matches the performance of its fully supervised equivalent. For most datasets, the performance of zero-shot classifiers still underperform fully supervised classifiers by 10% to 25%, suggesting that there is still plenty of headroom for improving CLIP’s task-learning and zero-shot transfer capabilities.",
    "file_name": [
      "figures/fileoutpart11.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_15.png"
  },
  {
    "page": 9,
    "bbox": [
      287.3677978515625,
      307.66172790527344,
      541.6050262451172,
      728.1352386474609
    ],
    "caption": "If we assume that evaluation datasets are large enough that the parameters of linear classifiers trained on them are well estimated, then, because CLIP’s zero-shot classifier is also a linear classifier, the performance of the fully supervised classifiers roughly sets an upper bound for what zero-shot transfer can achieve. In Figure (<>)8 we compare CLIP’s zero-shot performance with fully supervised linear classifiers across datasets. The dashed, y = x line represents an “optimal” zero-shot classifier that matches the performance of its fully supervised equivalent. For most datasets, the performance of zero-shot classifiers still underperform fully supervised classifiers by 10% to 25%, suggesting that there is still plenty of headroom for improving CLIP’s task-learning and zero-shot transfer capabilities.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      -21.933502197265625,
      570.1509094238281,
      352.0992431640625,
      765.4507751464844
    ],
    "caption": "Figure 9. Zero-shot CLIP performance scales smoothly as a function of model compute. Across 39 evals on 36 different datasets, average zero-shot error is well modeled by a log-log linear trend across a 44x range of compute spanning 5 different CLIP models. Lightly shaded lines are performance on individual evals, showing that performance is much more varied despite the smooth overall trend.",
    "file_name": [
      "figures/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_17.png"
  },
  {
    "page": 10,
    "bbox": [
      -21.933502197265625,
      570.1509094238281,
      352.0992431640625,
      765.4507751464844
    ],
    "caption": "Figure 9. Zero-shot CLIP performance scales smoothly as a function of model compute. Across 39 evals on 36 different datasets, average zero-shot error is well modeled by a log-log linear trend across a 44x range of compute spanning 5 different CLIP models. Lightly shaded lines are performance on individual evals, showing that performance is much more varied despite the smooth overall trend.",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      55.303375244140625,
      410.76637268066406,
      541.5765838623047,
      725.0771942138672
    ],
    "caption": "Figure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including EfficientNet ((<>)Tan & Le, (<>)2019; (<>)Xie et al., (<>)2020), MoCo ((<>)Chen et al., (<>)2020d), Instagram-pretrained ResNeXt models ((<>)Mahajan et al., (<>)2018; (<>)Touvron et al., (<>)2019), BiT ((<>)Kolesnikov et al., (<>)2019), ViT ((<>)Dosovitskiy et al., (<>)2020), SimCLRv2 ((<>)Chen et al., (<>)2020c), BYOL ((<>)Grill et al., (<>)2020), and the original ResNet models ((<>)He et al., (<>)2016b). (Left) Scores are averaged over 12 datasets studied by (<>)Kornblith et al. ((<>)2019). (Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models fine-tuned or evaluated on images at a higher-resolution than pre-training. See Table (<>)10 for individual scores and Figure (<>)20 for plots for each dataset.",
    "file_name": [
      "figures/fileoutpart13.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_19.png"
  },
  {
    "page": 11,
    "bbox": [
      55.303375244140625,
      410.76637268066406,
      541.5765838623047,
      725.0771942138672
    ],
    "caption": "Figure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including EfficientNet ((<>)Tan & Le, (<>)2019; (<>)Xie et al., (<>)2020), MoCo ((<>)Chen et al., (<>)2020d), Instagram-pretrained ResNeXt models ((<>)Mahajan et al., (<>)2018; (<>)Touvron et al., (<>)2019), BiT ((<>)Kolesnikov et al., (<>)2019), ViT ((<>)Dosovitskiy et al., (<>)2020), SimCLRv2 ((<>)Chen et al., (<>)2020c), BYOL ((<>)Grill et al., (<>)2020), and the original ResNet models ((<>)He et al., (<>)2016b). (Left) Scores are averaged over 12 datasets studied by (<>)Kornblith et al. ((<>)2019). (Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models fine-tuned or evaluated on images at a higher-resolution than pre-training. See Table (<>)10 for individual scores and Figure (<>)20 for plots for each dataset.",
    "file_name": ""
  },
  {
    "page": 12,
    "bbox": [
      55.23388671875,
      463.89019775390625,
      289.65155029296875,
      727.2887268066406
    ],
    "caption": "Figure 11. CLIP’s features outperform the features of the best ImageNet model on a wide variety of datasets. Fitting a linear classifier on CLIP’s features outperforms using the Noisy Student EfficientNet-L2 on 21 out of 27 datasets.",
    "file_name": [
      "figures/fileoutpart14.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_21.png"
  },
  {
    "page": 12,
    "bbox": [
      55.23388671875,
      463.89019775390625,
      289.65155029296875,
      727.2887268066406
    ],
    "caption": "Figure 11. CLIP’s features outperform the features of the best ImageNet model on a wide variety of datasets. Fitting a linear classifier on CLIP’s features outperforms using the Noisy Student EfficientNet-L2 on 21 out of 27 datasets.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      56.2158203125,
      486.88134765625,
      291.78936767578125,
      724.6877746582031
    ],
    "caption": "CLIP-ViT",
    "file_name": [
      "figures/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_23.png"
  },
  {
    "page": 13,
    "bbox": [
      56.2158203125,
      486.88134765625,
      291.78936767578125,
      724.6877746582031
    ],
    "caption": "CLIP-ViT",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      156.4975128173828,
      453.2325897216797,
      167.45164489746094,
      463.78358459472656
    ],
    "caption": "Figure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset splits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar ImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overfit to their task.",
    "file_name": [
      "figures/fileoutpart16.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_25.png"
  },
  {
    "page": 13,
    "bbox": [
      156.4975128173828,
      442.0991516113281,
      167.45164489746094,
      452.650146484375
    ],
    "caption": "Figure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset splits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar ImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overfit to their task.",
    "file_name": [
      "figures/fileoutpart17.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_26.png"
  },
  {
    "page": 13,
    "bbox": [
      158.45236206054688,
      432.3052215576172,
      165.49647521972656,
      439.3493347167969
    ],
    "caption": "Figure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset splits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar ImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overfit to their task.",
    "file_name": [
      "figures/fileoutpart18.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_27.png"
  },
  {
    "page": 13,
    "bbox": [
      158.45236206054688,
      421.1717834472656,
      165.49647521972656,
      428.2158966064453
    ],
    "caption": "Figure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset splits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar ImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overfit to their task.",
    "file_name": [
      "figures/fileoutpart19.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_28.png"
  },
  {
    "page": 13,
    "bbox": [
      291.4999542236328,
      419.1671447753906,
      539.1707611083984,
      724.6877746582031
    ],
    "caption": "Figure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset splits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar ImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overfit to their task.",
    "file_name": [
      "figures/fileoutpart20.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_29.png"
  },
  {
    "page": 13,
    "bbox": [
      291.4999542236328,
      419.1671447753906,
      539.1707611083984,
      724.6877746582031
    ],
    "caption": "Figure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset splits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar ImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overfit to their task.",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      -181.14878845214844,
      514.7723541259766,
      543.9281463623047,
      729.4214172363281
    ],
    "caption": "ImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch, and 1.9% on ImageNet-A. The change in accuracy on the two other datasets, Youtube-BB and ImageNet Vid, is insignificant.",
    "file_name": [
      "figures/fileoutpart21.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_31.png"
  },
  {
    "page": 14,
    "bbox": [
      -181.14878845214844,
      514.7723541259766,
      543.9281463623047,
      729.4214172363281
    ],
    "caption": "ImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch, and 1.9% on ImageNet-A. The change in accuracy on the two other datasets, Youtube-BB and ImageNet Vid, is insignificant.",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      -849.0648803710938,
      -698.1796264648438,
      859.8360595703125,
      1558.2283935546875
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart22.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_33.png"
  },
  {
    "page": 15,
    "bbox": [
      -849.0648803710938,
      -698.1796264648438,
      859.8360595703125,
      1558.2283935546875
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      315.2799987792969,
      607.7327117919922,
      531.1281280517578,
      728.6941223144531
    ],
    "caption": "Adapt to class shift",
    "file_name": [
      "figures/fileoutpart23.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_35.png"
  },
  {
    "page": 15,
    "bbox": [
      315.2799987792969,
      607.7327117919922,
      531.1281280517578,
      728.6941223144531
    ],
    "caption": "Adapt to class shift",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      303.6620330810547,
      486.3633117675781,
      532.0718841552734,
      593.1369018554688
    ],
    "caption": "Figure 14. While supervised adaptation to ImageNet increases ImageNet accuracy by 9.2%, it slightly reduces average robustness. (Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classifier and pooling predictions across similar classes as in (<>)Taori et al. ((<>)2020). CLIP models adapted to ImageNet have similar effective robustness as the best prior ImageNet models. (Right) Details of per dataset changes in accuracy for the two robustness interventions. Adapting to ImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions. Dataset specific zero-shot classifiers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don’t perfectly align with ImageNet categories.",
    "file_name": [
      "figures/fileoutpart24.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_37.png"
  },
  {
    "page": 15,
    "bbox": [
      303.6620330810547,
      486.3633117675781,
      532.0718841552734,
      593.1369018554688
    ],
    "caption": "Figure 14. While supervised adaptation to ImageNet increases ImageNet accuracy by 9.2%, it slightly reduces average robustness. (Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classifier and pooling predictions across similar classes as in (<>)Taori et al. ((<>)2020). CLIP models adapted to ImageNet have similar effective robustness as the best prior ImageNet models. (Right) Details of per dataset changes in accuracy for the two robustness interventions. Adapting to ImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions. Dataset specific zero-shot classifiers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don’t perfectly align with ImageNet categories.",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      -782.0634918212891,
      -47.7752685546875,
      827.2427215576172,
      1045.490982055664
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart27.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_39.png"
  },
  {
    "page": 16,
    "bbox": [
      -782.0634918212891,
      -47.7752685546875,
      827.2427215576172,
      1045.490982055664
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 17,
    "bbox": [
      55.3084716796875,
      505.0234680175781,
      290.402099609375,
      725.07080078125
    ],
    "caption": "Figure 16. The hardest problems for CLIP also tend to be the hardest problems for humans. Here we rank image categories by difficulty for CLIP as measured as probability of the correct label.",
    "file_name": [
      "figures/fileoutpart28.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_41.png"
  },
  {
    "page": 17,
    "bbox": [
      55.3084716796875,
      505.0234680175781,
      290.402099609375,
      725.07080078125
    ],
    "caption": "Figure 16. The hardest problems for CLIP also tend to be the hardest problems for humans. Here we rank image categories by difficulty for CLIP as measured as probability of the correct label.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      53.1943359375,
      552.5360870361328,
      542.4074554443359,
      725.6031036376953
    ],
    "caption": "Figure 17. Few statistically significant improvements in accuracy due to detected data overlap. (Left) While several datasets have up to ±20% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have 99.5% Clopper-Pearson confidence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data. (Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to overlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy improvements statistically significant when calculated using a one-sided binomial test.",
    "file_name": [
      "figures/fileoutpart29.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_43.png"
  },
  {
    "page": 18,
    "bbox": [
      53.1943359375,
      552.5360870361328,
      542.4074554443359,
      725.6031036376953
    ],
    "caption": "Figure 17. Few statistically significant improvements in accuracy due to detected data overlap. (Left) While several datasets have up to ±20% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have 99.5% Clopper-Pearson confidence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data. (Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to overlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy improvements statistically significant when calculated using a one-sided binomial test.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      298.2332305908203,
      552.5360870361328,
      542.4209289550781,
      724.3038635253906
    ],
    "caption": "Figure 17. Few statistically significant improvements in accuracy due to detected data overlap. (Left) While several datasets have up to ±20% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have 99.5% Clopper-Pearson confidence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data. (Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to overlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy improvements statistically significant when calculated using a one-sided binomial test.",
    "file_name": [
      "figures/fileoutpart30.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_45.png"
  },
  {
    "page": 18,
    "bbox": [
      298.2332305908203,
      552.5360870361328,
      542.4209289550781,
      724.3038635253906
    ],
    "caption": "Figure 17. Few statistically significant improvements in accuracy due to detected data overlap. (Left) While several datasets have up to ±20% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have 99.5% Clopper-Pearson confidence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data. (Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to overlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy improvements statistically significant when calculated using a one-sided binomial test.",
    "file_name": ""
  },
  {
    "page": 23,
    "bbox": [
      79.60272216796875,
      500.14971923828125,
      517.2740325927734,
      725.0794067382812
    ],
    "caption": "Figure 18. CLIP performance on Member of Congress images when given the combined returned label set for the images from Google Cloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were identified with χ2 tests with the threshold at 0.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a certain label by gender.",
    "file_name": [
      "figures/fileoutpart41.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_47.png"
  },
  {
    "page": 23,
    "bbox": [
      79.60272216796875,
      500.14971923828125,
      517.2740325927734,
      725.0794067382812
    ],
    "caption": "Figure 18. CLIP performance on Member of Congress images when given the combined returned label set for the images from Google Cloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were identified with χ2 tests with the threshold at 0.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a certain label by gender.",
    "file_name": ""
  },
  {
    "page": 37,
    "bbox": [
      88.2030029296875,
      527.1519927978516,
      282.6036376953125,
      721.5526428222656
    ],
    "caption": "Figure 19. Two example images from the Rendered SST2 dataset",
    "file_name": [
      "figures/fileoutpart44.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_49.png"
  },
  {
    "page": 37,
    "bbox": [
      311.7899932861328,
      527.1519927978516,
      506.1906433105469,
      721.5526428222656
    ],
    "caption": "Figure 19. Two example images from the Rendered SST2 dataset",
    "file_name": [
      "figures/fileoutpart45.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_50.png"
  },
  {
    "page": 40,
    "bbox": [
      67.4931640625,
      105.191162109375,
      529.3900299072266,
      725.0381927490234
    ],
    "caption": "Figure 20. Linear probe performance plotted for each of the 27 datasets, using the data from Table (<>)10.",
    "file_name": [
      "figures/fileoutpart50.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_51.png"
  },
  {
    "page": 40,
    "bbox": [
      67.4931640625,
      105.191162109375,
      529.3900299072266,
      725.0381927490234
    ],
    "caption": "Figure 20. Linear probe performance plotted for each of the 27 datasets, using the data from Table (<>)10.",
    "file_name": ""
  },
  {
    "page": 41,
    "bbox": [
      67.73222351074219,
      128.81988525390625,
      525.9907989501953,
      724.3084411621094
    ],
    "caption": "Figure 21. Visualization of predictions from 36 CLIP zero-shot classifiers. All examples are random with the exception of reselecting Hateful Memes to avoid offensive content. The predicted probability of the top 5 classes is shown along with the text used to represent the class. When more than one template is used, the first template is shown. The ground truth label is colored green while an incorrect prediction is colored orange.",
    "file_name": [
      "figures/fileoutpart51.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_53.png"
  },
  {
    "page": 41,
    "bbox": [
      67.73222351074219,
      128.81988525390625,
      525.9907989501953,
      724.3084411621094
    ],
    "caption": "Figure 21. Visualization of predictions from 36 CLIP zero-shot classifiers. All examples are random with the exception of reselecting Hateful Memes to avoid offensive content. The predicted probability of the top 5 classes is shown along with the text used to represent the class. When more than one template is used, the first template is shown. The ground truth label is colored green while an incorrect prediction is colored orange.",
    "file_name": ""
  },
  {
    "page": 42,
    "bbox": [
      55.325225830078125,
      145.53321838378906,
      541.5580902099609,
      523.2696228027344
    ],
    "caption": "Figure 22. CLIP’s zero-shot performance compared to linear-probe ResNet performance",
    "file_name": [
      "figures/fileoutpart54.png"
    ],
    "output_file": "assets/images/paper/CLIP-Learning-Transferable-Visual-Representations-from-Natural-Language/fig_55.png"
  },
  {
    "page": 42,
    "bbox": [
      55.325225830078125,
      145.53321838378906,
      541.5580902099609,
      523.2696228027344
    ],
    "caption": "Figure 22. CLIP’s zero-shot performance compared to linear-probe ResNet performance",
    "file_name": ""
  }
]