[
  {
    "page": 5,
    "bbox": [
      118.72825622558594,
      567.0544586181641,
      493.2732238769531,
      683.8302764892578
    ],
    "caption": "4.1 Chest CT",
    "file_name": [
      "tables/fileoutpart13.xlsx",
      "tables/fileoutpart14.png"
    ],
    "output_file": "assets/images/paper/HLIP-Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging/table_01.png"
  },
  {
    "page": 5,
    "bbox": [
      127.65499877929688,
      668.0404357910156,
      157.79107666015625,
      678.8449554443359
    ],
    "caption": "CT-CLIP [(<>)3]",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      127.65499877929688,
      668.0404357910156,
      157.79107666015625,
      678.8449554443359
    ],
    "caption": "CT-CLIP [(<>)3]",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      192.98419189453125,
      673.0257568359375,
      306.46295166015625,
      683.8302764892578
    ],
    "caption": "AUC",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      192.98419189453125,
      673.0257568359375,
      306.46295166015625,
      683.8302764892578
    ],
    "caption": "AUC",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      353.3572235107422,
      673.0257568359375,
      482.6078796386719,
      683.8302764892578
    ],
    "caption": "AUC",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      353.3572235107422,
      673.0257568359375,
      482.6078796386719,
      683.8302764892578
    ],
    "caption": "AUC",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      174.43699645996094,
      658.0534362792969,
      195.113525390625,
      668.8579559326172
    ],
    "caption": "73.3",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      174.43699645996094,
      658.0534362792969,
      195.113525390625,
      668.8579559326172
    ],
    "caption": "73.3",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      202.83358764648438,
      658.0534362792969,
      223.15145874023438,
      668.8579559326172
    ],
    "caption": "66.9",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      202.83358764648438,
      658.0534362792969,
      223.15145874023438,
      668.8579559326172
    ],
    "caption": "66.9",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      233.9828643798828,
      658.0534362792969,
      245.69297790527344,
      668.8579559326172
    ],
    "caption": "70.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      233.9828643798828,
      658.0534362792969,
      245.69297790527344,
      668.8579559326172
    ],
    "caption": "70.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      256.52439880371094,
      658.0534362792969,
      292.13893127441406,
      668.8579559326172
    ],
    "caption": "Supervised by Original Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      256.52439880371094,
      658.0534362792969,
      292.13893127441406,
      668.8579559326172
    ],
    "caption": "Supervised by Original Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      299.8590087890625,
      658.0534362792969,
      325.00975036621094,
      668.8579559326172
    ],
    "caption": "Supervised by Original Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      299.8590087890625,
      658.0534362792969,
      325.00975036621094,
      668.8579559326172
    ],
    "caption": "Supervised by Original Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      342.7004699707031,
      658.0534362792969,
      363.3769836425781,
      668.8579559326172
    ],
    "caption": "Supervised by Original Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      342.7004699707031,
      658.0534362792969,
      363.3769836425781,
      668.8579559326172
    ],
    "caption": "Supervised by Original Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      371.0970458984375,
      658.0534362792969,
      391.4149169921875,
      668.8579559326172
    ],
    "caption": "59.9",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      371.0970458984375,
      658.0534362792969,
      391.4149169921875,
      668.8579559326172
    ],
    "caption": "59.9",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      402.24632263183594,
      658.0534362792969,
      413.95643615722656,
      668.8579559326172
    ],
    "caption": "64.7",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      402.24632263183594,
      658.0534362792969,
      413.95643615722656,
      668.8579559326172
    ],
    "caption": "64.7",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      424.78785705566406,
      658.0534362792969,
      460.4023895263672,
      668.8579559326172
    ],
    "caption": "34.1",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      424.78785705566406,
      658.0534362792969,
      460.4023895263672,
      668.8579559326172
    ],
    "caption": "34.1",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      468.1224670410156,
      658.0534362792969,
      493.2732238769531,
      668.8579559326172
    ],
    "caption": "--",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      468.1224670410156,
      658.0534362792969,
      493.2732238769531,
      668.8579559326172
    ],
    "caption": "--",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      248.2209930419922,
      645.7980651855469,
      363.77996826171875,
      656.5039520263672
    ],
    "caption": "70.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      248.2209930419922,
      645.7980651855469,
      363.77996826171875,
      656.5039520263672
    ],
    "caption": "70.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      118.72825622558594,
      633.7472229003906,
      166.7164306640625,
      644.5517425537109
    ],
    "caption": "Merlin [(<>)4]",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      118.72825622558594,
      633.7472229003906,
      166.7164306640625,
      644.5517425537109
    ],
    "caption": "Merlin [(<>)4]",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      175.808349609375,
      633.7472229003906,
      193.74114990234375,
      644.5517425537109
    ],
    "caption": "72.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      175.808349609375,
      633.7472229003906,
      193.74114990234375,
      644.5517425537109
    ],
    "caption": "72.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      204.02561950683594,
      633.7472229003906,
      221.9584197998047,
      644.5517425537109
    ],
    "caption": "67.2",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      204.02561950683594,
      633.7472229003906,
      221.9584197998047,
      644.5517425537109
    ],
    "caption": "67.2",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      230.8710174560547,
      633.7472229003906,
      248.80381774902344,
      644.5517425537109
    ],
    "caption": "70.9",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      230.8710174560547,
      633.7472229003906,
      248.80381774902344,
      644.5517425537109
    ],
    "caption": "70.9",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      265.36476135253906,
      633.7472229003906,
      283.2975616455078,
      644.5517425537109
    ],
    "caption": "33.7",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      265.36476135253906,
      633.7472229003906,
      283.2975616455078,
      644.5517425537109
    ],
    "caption": "33.7",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      307.21095275878906,
      633.7472229003906,
      315.4241027832031,
      644.5517425537109
    ],
    "caption": "70.1",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      307.21095275878906,
      633.7472229003906,
      315.4241027832031,
      644.5517425537109
    ],
    "caption": "70.1",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      344.0718231201172,
      633.7472229003906,
      362.00462341308594,
      644.5517425537109
    ],
    "caption": "64.4",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      344.0718231201172,
      633.7472229003906,
      362.00462341308594,
      644.5517425537109
    ],
    "caption": "64.4",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      372.28907775878906,
      633.7472229003906,
      390.2218780517578,
      644.5517425537109
    ],
    "caption": "61.9",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      372.28907775878906,
      633.7472229003906,
      390.2218780517578,
      644.5517425537109
    ],
    "caption": "61.9",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      399.1344757080078,
      633.7472229003906,
      417.06727600097656,
      644.5517425537109
    ],
    "caption": "66.3",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      399.1344757080078,
      633.7472229003906,
      417.06727600097656,
      644.5517425537109
    ],
    "caption": "66.3",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      433.6282196044922,
      633.7472229003906,
      451.56101989746094,
      644.5517425537109
    ],
    "caption": "34.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      433.6282196044922,
      633.7472229003906,
      451.56101989746094,
      644.5517425537109
    ],
    "caption": "34.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      475.46543884277344,
      633.7472229003906,
      483.67860412597656,
      644.5517425537109
    ],
    "caption": "61.0",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      475.46543884277344,
      633.7472229003906,
      483.67860412597656,
      644.5517425537109
    ],
    "caption": "61.0",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      122.05477905273438,
      623.7855529785156,
      163.389892578125,
      634.5900726318359
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      122.05477905273438,
      623.7855529785156,
      163.389892578125,
      634.5900726318359
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      175.808349609375,
      623.7855529785156,
      193.74114990234375,
      634.5900726318359
    ],
    "caption": "77.7",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      175.808349609375,
      623.7855529785156,
      193.74114990234375,
      634.5900726318359
    ],
    "caption": "77.7",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      204.02561950683594,
      623.7855529785156,
      221.9584197998047,
      634.5900726318359
    ],
    "caption": "71.4",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      204.02561950683594,
      623.7855529785156,
      221.9584197998047,
      634.5900726318359
    ],
    "caption": "71.4",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      230.8710174560547,
      623.7855529785156,
      248.80381774902344,
      634.5900726318359
    ],
    "caption": "74.7",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      230.8710174560547,
      623.7855529785156,
      248.80381774902344,
      634.5900726318359
    ],
    "caption": "74.7",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      265.36476135253906,
      623.7855529785156,
      283.2975616455078,
      634.5900726318359
    ],
    "caption": "37.9",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      265.36476135253906,
      623.7855529785156,
      283.2975616455078,
      634.5900726318359
    ],
    "caption": "37.9",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      303.4719543457031,
      623.7855529785156,
      321.4047546386719,
      634.5900726318359
    ],
    "caption": "73.0",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      303.4719543457031,
      623.7855529785156,
      321.4047546386719,
      634.5900726318359
    ],
    "caption": "73.0",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      344.0718231201172,
      623.7855529785156,
      362.00462341308594,
      634.5900726318359
    ],
    "caption": "72.3",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      344.0718231201172,
      623.7855529785156,
      362.00462341308594,
      634.5900726318359
    ],
    "caption": "72.3",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      372.28907775878906,
      623.7855529785156,
      390.2218780517578,
      634.5900726318359
    ],
    "caption": "68.4",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      372.28907775878906,
      623.7855529785156,
      390.2218780517578,
      634.5900726318359
    ],
    "caption": "68.4",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      399.1344757080078,
      623.7855529785156,
      417.06727600097656,
      634.5900726318359
    ],
    "caption": "72.1",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      399.1344757080078,
      623.7855529785156,
      417.06727600097656,
      634.5900726318359
    ],
    "caption": "72.1",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      433.6282196044922,
      623.7855529785156,
      451.56101989746094,
      634.5900726318359
    ],
    "caption": "40.4",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      433.6282196044922,
      623.7855529785156,
      451.56101989746094,
      634.5900726318359
    ],
    "caption": "40.4",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      471.72645568847656,
      623.7855529785156,
      489.6592559814453,
      634.5900726318359
    ],
    "caption": "66.7",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      471.72645568847656,
      623.7855529785156,
      489.6592559814453,
      634.5900726318359
    ],
    "caption": "66.7",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      123.050048828125,
      613.8238830566406,
      162.39462280273438,
      624.6284027099609
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      123.050048828125,
      613.8238830566406,
      162.39462280273438,
      624.6284027099609
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      175.808349609375,
      613.8238830566406,
      193.74114990234375,
      624.6284027099609
    ],
    "caption": "77.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      175.808349609375,
      613.8238830566406,
      193.74114990234375,
      624.6284027099609
    ],
    "caption": "77.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      204.02561950683594,
      613.8238830566406,
      221.9584197998047,
      624.6284027099609
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      204.02561950683594,
      613.8238830566406,
      221.9584197998047,
      624.6284027099609
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      230.8710174560547,
      613.8238830566406,
      248.80381774902344,
      624.6284027099609
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      230.8710174560547,
      613.8238830566406,
      248.80381774902344,
      624.6284027099609
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      265.36476135253906,
      613.8238830566406,
      283.2975616455078,
      624.6284027099609
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      265.36476135253906,
      613.8238830566406,
      283.2975616455078,
      624.6284027099609
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      303.4719543457031,
      613.8238830566406,
      321.4047546386719,
      624.6284027099609
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      303.4719543457031,
      613.8238830566406,
      321.4047546386719,
      624.6284027099609
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      344.0718231201172,
      613.8238830566406,
      362.00462341308594,
      624.6284027099609
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      344.0718231201172,
      613.8238830566406,
      362.00462341308594,
      624.6284027099609
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      372.28907775878906,
      613.8238830566406,
      390.2218780517578,
      624.6284027099609
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      372.28907775878906,
      613.8238830566406,
      390.2218780517578,
      624.6284027099609
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      399.1344757080078,
      613.8238830566406,
      417.06727600097656,
      624.6284027099609
    ],
    "caption": "68.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      399.1344757080078,
      613.8238830566406,
      417.06727600097656,
      624.6284027099609
    ],
    "caption": "68.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      433.6282196044922,
      613.8238830566406,
      451.56101989746094,
      624.6284027099609
    ],
    "caption": "37.4",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      433.6282196044922,
      613.8238830566406,
      451.56101989746094,
      624.6284027099609
    ],
    "caption": "37.4",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      471.72645568847656,
      613.8238830566406,
      489.6592559814453,
      624.6284027099609
    ],
    "caption": "64.6",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      471.72645568847656,
      613.8238830566406,
      489.6592559814453,
      624.6284027099609
    ],
    "caption": "64.6",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      119.81300354003906,
      602.8604431152344,
      165.6313018798828,
      613.6649475097656
    ],
    "caption": "fVLM [(<>)6]",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      119.81300354003906,
      602.8604431152344,
      165.6313018798828,
      613.6649475097656
    ],
    "caption": "fVLM [(<>)6]",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      175.8090057373047,
      602.3224639892578,
      193.74180603027344,
      613.9877471923828
    ],
    "caption": "77.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      175.8090057373047,
      602.3224639892578,
      193.74180603027344,
      613.9877471923828
    ],
    "caption": "77.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      204.02699279785156,
      602.3224639892578,
      221.9597930908203,
      613.9877471923828
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      204.02699279785156,
      602.3224639892578,
      221.9597930908203,
      613.9877471923828
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      230.8730010986328,
      602.3224639892578,
      248.80580139160156,
      613.9877471923828
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      230.8730010986328,
      602.3224639892578,
      248.80580139160156,
      613.9877471923828
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      265.3679962158203,
      602.3224639892578,
      283.30079650878906,
      613.9877471923828
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      265.3679962158203,
      602.3224639892578,
      283.30079650878906,
      613.9877471923828
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      303.4709930419922,
      602.3224639892578,
      321.40379333496094,
      613.9877471923828
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      303.4709930419922,
      602.3224639892578,
      321.40379333496094,
      613.9877471923828
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      344.0679931640625,
      602.3224639892578,
      362.00079345703125,
      613.9877471923828
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      344.0679931640625,
      602.3224639892578,
      362.00079345703125,
      613.9877471923828
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      372.28599548339844,
      602.3224639892578,
      390.2187957763672,
      613.9877471923828
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      372.28599548339844,
      602.3224639892578,
      390.2187957763672,
      613.9877471923828
    ],
    "caption": "Supervised by Qwen [(<>)52] Summarized Reports",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      399.1320037841797,
      602.3224639892578,
      417.06480407714844,
      613.9877471923828
    ],
    "caption": "68.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      399.1320037841797,
      602.3224639892578,
      417.06480407714844,
      613.9877471923828
    ],
    "caption": "68.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      433.6269989013672,
      602.3224639892578,
      451.55979919433594,
      613.9877471923828
    ],
    "caption": "37.4",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      433.6269989013672,
      602.3224639892578,
      451.55979919433594,
      613.9877471923828
    ],
    "caption": "37.4",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      471.72999572753906,
      602.3224639892578,
      489.6627960205078,
      613.9877471923828
    ],
    "caption": "64.6",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      471.72999572753906,
      602.3224639892578,
      489.6627960205078,
      613.9877471923828
    ],
    "caption": "64.6",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      220.82899475097656,
      590.6050720214844,
      391.17266845703125,
      601.3109588623047
    ],
    "caption": "71.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      220.82899475097656,
      590.6050720214844,
      391.17266845703125,
      601.3109588623047
    ],
    "caption": "71.8",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      123.79461669921875,
      578.5542297363281,
      161.6417999267578,
      589.3587493896484
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      123.79461669921875,
      578.5542297363281,
      161.6417999267578,
      589.3587493896484
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      175.80870056152344,
      578.5542297363281,
      193.7415008544922,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      175.80870056152344,
      578.5542297363281,
      193.7415008544922,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      204.02597045898438,
      578.5542297363281,
      221.95877075195312,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      204.02597045898438,
      578.5542297363281,
      221.95877075195312,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      230.87136840820312,
      578.5542297363281,
      248.80416870117188,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      230.87136840820312,
      578.5542297363281,
      248.80416870117188,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      265.3651123046875,
      578.5542297363281,
      283.29791259765625,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      265.3651123046875,
      578.5542297363281,
      283.29791259765625,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      303.4633483886719,
      578.5542297363281,
      321.3961486816406,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      303.4633483886719,
      578.5542297363281,
      321.3961486816406,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      344.0632019042969,
      578.5542297363281,
      361.9960021972656,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      344.0632019042969,
      578.5542297363281,
      361.9960021972656,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      372.28045654296875,
      578.5542297363281,
      390.2132568359375,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      372.28045654296875,
      578.5542297363281,
      390.2132568359375,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      399.12586975097656,
      578.5542297363281,
      417.0586700439453,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      399.12586975097656,
      578.5542297363281,
      417.0586700439453,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      433.6195983886719,
      578.5542297363281,
      451.5523986816406,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      433.6195983886719,
      578.5542297363281,
      451.5523986816406,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      471.726806640625,
      578.5542297363281,
      489.65960693359375,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      471.726806640625,
      578.5542297363281,
      489.65960693359375,
      589.3587493896484
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      119.81300354003906,
      567.5924377441406,
      165.6313018798828,
      578.3969573974609
    ],
    "caption": "4.1 Chest CT",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      119.81300354003906,
      567.5924377441406,
      165.6313018798828,
      578.3969573974609
    ],
    "caption": "4.1 Chest CT",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      175.8090057373047,
      567.0544586181641,
      193.74180603027344,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      175.8090057373047,
      567.0544586181641,
      193.74180603027344,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      204.02699279785156,
      567.0544586181641,
      221.9597930908203,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      204.02699279785156,
      567.0544586181641,
      221.9597930908203,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      230.8730010986328,
      567.0544586181641,
      248.80580139160156,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      230.8730010986328,
      567.0544586181641,
      248.80580139160156,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      265.3679962158203,
      567.0544586181641,
      283.30079650878906,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      265.3679962158203,
      567.0544586181641,
      283.30079650878906,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      303.4709930419922,
      567.0544586181641,
      321.40379333496094,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      303.4709930419922,
      567.0544586181641,
      321.40379333496094,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      344.0679931640625,
      567.0544586181641,
      362.00079345703125,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      344.0679931640625,
      567.0544586181641,
      362.00079345703125,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      372.28599548339844,
      567.0544586181641,
      390.2187957763672,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      372.28599548339844,
      567.0544586181641,
      390.2187957763672,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      399.1320037841797,
      567.0544586181641,
      417.06480407714844,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      399.1320037841797,
      567.0544586181641,
      417.06480407714844,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      433.6269989013672,
      567.0544586181641,
      451.55979919433594,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      433.6269989013672,
      567.0544586181641,
      451.55979919433594,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      471.72999572753906,
      567.0544586181641,
      489.6627960205078,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      471.72999572753906,
      567.0544586181641,
      489.6627960205078,
      578.7197418212891
    ],
    "caption": "Implementation Details. Following CT-CLIP [(<>)3] and fVLM [(<>)6], we apply HLIP to the CT-RATE [(<>)3] training set, then perform internal validation on its test split and external validation on the full Rad-ChestCT dataset [(<>)22]. The implementation details differ from Section (<>)3.2 as these datasets are curated. During preprocessing, each scan is standardized with a spacing of (3mm,1mm,1mm) [(<>)6]. The HU values are truncated to [-1150,350] [(<>)36]. We apply a center crop of size (112,336,336) to construct mini-batch and a token size of (8,24,24). We use CXR-BERT [(<>)7] as the text encoder. HLIP is then trained for 20 epochs without patch dropout, which can be completed within 6 hours using a batch size of 512 on 4 A40 GPUs. More details are provided in Appendix (<>)B.1.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      125.97274780273438,
      522.3224639892578,
      483.78953552246094,
      662.0122680664062
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": [
      "tables/fileoutpart15.xlsx",
      "tables/fileoutpart16.png"
    ],
    "output_file": "assets/images/paper/HLIP-Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging/table_164.png"
  },
  {
    "page": 6,
    "bbox": [
      145.27200317382812,
      646.2224426269531,
      175.40806579589844,
      657.0269470214844
    ],
    "caption": "BiomedCLIP [(<>)23]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      145.27200317382812,
      646.2224426269531,
      175.40806579589844,
      657.0269470214844
    ],
    "caption": "BiomedCLIP [(<>)23]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      251.82870483398438,
      651.207763671875,
      319.0587615966797,
      662.0122680664062
    ],
    "caption": "Glioma",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      251.82870483398438,
      651.207763671875,
      319.0587615966797,
      662.0122680664062
    ],
    "caption": "Glioma",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      376.18370056152344,
      639.2555541992188,
      427.2473449707031,
      660.021728515625
    ],
    "caption": "46.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      388.59320068359375,
      649.2172241210938,
      414.8378448486328,
      660.021728515625
    ],
    "caption": "46.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      376.18370056152344,
      639.2555541992188,
      427.2473449707031,
      650.06005859375
    ],
    "caption": "46.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      434.96742248535156,
      639.2555541992188,
      483.78953552246094,
      660.021728515625
    ],
    "caption": "33.1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      445.43121337890625,
      649.2172241210938,
      475.5583038330078,
      660.021728515625
    ],
    "caption": "33.1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      434.96742248535156,
      639.2555541992188,
      483.78953552246094,
      650.06005859375
    ],
    "caption": "33.1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      202.4290008544922,
      636.2354431152344,
      227.99220275878906,
      647.0399475097656
    ],
    "caption": "66.7",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      202.4290008544922,
      636.2354431152344,
      227.99220275878906,
      647.0399475097656
    ],
    "caption": "66.7",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      235.7122802734375,
      636.2354431152344,
      264.8530731201172,
      647.0399475097656
    ],
    "caption": "88.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      235.7122802734375,
      636.2354431152344,
      264.8530731201172,
      647.0399475097656
    ],
    "caption": "88.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      272.5731506347656,
      636.2354431152344,
      320.6420135498047,
      647.0399475097656
    ],
    "caption": "Pub-Brain-5-GT",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      272.5731506347656,
      636.2354431152344,
      320.6420135498047,
      647.0399475097656
    ],
    "caption": "Pub-Brain-5-GT",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      328.3620910644531,
      636.2354431152344,
      368.45982360839844,
      647.0399475097656
    ],
    "caption": "Pub-Brain-5-GT",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      328.3620910644531,
      636.2354431152344,
      368.45982360839844,
      647.0399475097656
    ],
    "caption": "Pub-Brain-5-GT",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      274.7169952392578,
      623.9800720214844,
      336.59413146972656,
      634.6859588623047
    ],
    "caption": "63.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      274.7169952392578,
      623.9800720214844,
      336.59413146972656,
      634.6859588623047
    ],
    "caption": "63.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      126.71760559082031,
      611.9292297363281,
      193.96560668945312,
      622.7337493896484
    ],
    "caption": "ConceptCLIP [(<>)24]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      126.71760559082031,
      611.9292297363281,
      193.96560668945312,
      622.7337493896484
    ],
    "caption": "ConceptCLIP [(<>)24]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24957275390625,
      611.9292297363281,
      224.182373046875,
      622.7337493896484
    ],
    "caption": "69.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24957275390625,
      611.9292297363281,
      224.182373046875,
      622.7337493896484
    ],
    "caption": "69.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.31715393066406,
      611.9292297363281,
      259.2499542236328,
      622.7337493896484
    ],
    "caption": "92.1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.31715393066406,
      611.9292297363281,
      259.2499542236328,
      622.7337493896484
    ],
    "caption": "92.1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.64654541015625,
      611.9292297363281,
      305.579345703125,
      622.7337493896484
    ],
    "caption": "57.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.64654541015625,
      611.9292297363281,
      305.579345703125,
      622.7337493896484
    ],
    "caption": "57.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.4454345703125,
      611.9292297363281,
      357.37823486328125,
      622.7337493896484
    ],
    "caption": "69.5",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.4454345703125,
      611.9292297363281,
      357.37823486328125,
      622.7337493896484
    ],
    "caption": "69.5",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7506866455078,
      611.9292297363281,
      410.68348693847656,
      622.7337493896484
    ],
    "caption": "35.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7506866455078,
      611.9292297363281,
      410.68348693847656,
      622.7337493896484
    ],
    "caption": "35.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.53440856933594,
      611.9292297363281,
      469.4672088623047,
      622.7337493896484
    ],
    "caption": "31.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.53440856933594,
      611.9292297363281,
      469.4672088623047,
      622.7337493896484
    ],
    "caption": "31.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      146.40782165527344,
      601.2681732177734,
      192.21714782714844,
      612.7720794677734
    ],
    "caption": "ConceptCLIP [(<>)24]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      146.40782165527344,
      601.2681732177734,
      192.21714782714844,
      612.7720794677734
    ],
    "caption": "ConceptCLIP [(<>)24]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24957275390625,
      601.9675598144531,
      224.182373046875,
      612.7720794677734
    ],
    "caption": "69.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24957275390625,
      601.9675598144531,
      224.182373046875,
      612.7720794677734
    ],
    "caption": "69.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.3261260986328,
      601.9675598144531,
      259.25892639160156,
      612.7720794677734
    ],
    "caption": "92.1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.3261260986328,
      601.9675598144531,
      259.25892639160156,
      612.7720794677734
    ],
    "caption": "92.1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.64654541015625,
      601.9675598144531,
      305.579345703125,
      612.7720794677734
    ],
    "caption": "57.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.64654541015625,
      601.9675598144531,
      305.579345703125,
      612.7720794677734
    ],
    "caption": "57.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.45440673828125,
      601.9675598144531,
      357.38720703125,
      612.7720794677734
    ],
    "caption": "69.5",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.45440673828125,
      601.9675598144531,
      357.38720703125,
      612.7720794677734
    ],
    "caption": "69.5",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7506866455078,
      601.9675598144531,
      410.68348693847656,
      612.7720794677734
    ],
    "caption": "35.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7506866455078,
      601.9675598144531,
      410.68348693847656,
      612.7720794677734
    ],
    "caption": "35.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.53440856933594,
      601.9675598144531,
      469.4672088623047,
      612.7720794677734
    ],
    "caption": "31.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.53440856933594,
      601.9675598144531,
      469.4672088623047,
      612.7720794677734
    ],
    "caption": "31.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      125.973388671875,
      590.0153503417969,
      194.70980834960938,
      600.8198547363281
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      125.973388671875,
      590.0153503417969,
      194.70980834960938,
      600.8198547363281
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24957275390625,
      590.0153503417969,
      224.182373046875,
      600.8198547363281
    ],
    "caption": "95.0",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24957275390625,
      590.0153503417969,
      224.182373046875,
      600.8198547363281
    ],
    "caption": "95.0",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.31715393066406,
      590.0153503417969,
      259.2499542236328,
      600.8198547363281
    ],
    "caption": "89.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.31715393066406,
      590.0153503417969,
      259.2499542236328,
      600.8198547363281
    ],
    "caption": "89.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.64654541015625,
      590.0153503417969,
      305.579345703125,
      600.8198547363281
    ],
    "caption": "79.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.64654541015625,
      590.0153503417969,
      305.579345703125,
      600.8198547363281
    ],
    "caption": "79.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.4454345703125,
      590.0153503417969,
      357.37823486328125,
      600.8198547363281
    ],
    "caption": "73.4",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.4454345703125,
      590.0153503417969,
      357.37823486328125,
      600.8198547363281
    ],
    "caption": "73.4",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7506866455078,
      590.0153503417969,
      410.68348693847656,
      600.8198547363281
    ],
    "caption": "54.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7506866455078,
      590.0153503417969,
      410.68348693847656,
      600.8198547363281
    ],
    "caption": "54.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.53440856933594,
      590.0153503417969,
      469.4672088623047,
      600.8198547363281
    ],
    "caption": "61.3",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.53440856933594,
      590.0153503417969,
      469.4672088623047,
      600.8198547363281
    ],
    "caption": "61.3",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      146.40782165527344,
      579.3542938232422,
      192.21714782714844,
      590.8581848144531
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      146.40782165527344,
      579.3542938232422,
      192.21714782714844,
      590.8581848144531
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24957275390625,
      580.0536804199219,
      224.182373046875,
      590.8581848144531
    ],
    "caption": "95.0",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24957275390625,
      580.0536804199219,
      224.182373046875,
      590.8581848144531
    ],
    "caption": "95.0",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.3261260986328,
      579.5157012939453,
      259.25892639160156,
      591.1809844970703
    ],
    "caption": "89.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.3261260986328,
      579.5157012939453,
      259.25892639160156,
      591.1809844970703
    ],
    "caption": "89.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.655517578125,
      580.0536804199219,
      305.58831787109375,
      590.8581848144531
    ],
    "caption": "79.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.655517578125,
      580.0536804199219,
      305.58831787109375,
      590.8581848144531
    ],
    "caption": "79.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.46337890625,
      579.5157012939453,
      357.39617919921875,
      591.1809844970703
    ],
    "caption": "73.4",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.46337890625,
      579.5157012939453,
      357.39617919921875,
      591.1809844970703
    ],
    "caption": "73.4",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.75965881347656,
      580.0536804199219,
      410.6924591064453,
      590.8581848144531
    ],
    "caption": "54.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.75965881347656,
      580.0536804199219,
      410.6924591064453,
      590.8581848144531
    ],
    "caption": "54.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.5433807373047,
      580.0536804199219,
      469.47618103027344,
      590.8581848144531
    ],
    "caption": "61.3",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.5433807373047,
      580.0536804199219,
      469.47618103027344,
      590.8581848144531
    ],
    "caption": "61.3",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      137.43099975585938,
      568.0904388427734,
      183.24929809570312,
      578.8949584960938
    ],
    "caption": "BiomedCLIP [(<>)23]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      137.43099975585938,
      568.0904388427734,
      183.24929809570312,
      578.8949584960938
    ],
    "caption": "BiomedCLIP [(<>)23]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24400329589844,
      567.5524597167969,
      224.1768035888672,
      579.2177429199219
    ],
    "caption": "64.7",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24400329589844,
      567.5524597167969,
      224.1768035888672,
      579.2177429199219
    ],
    "caption": "64.7",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.31700134277344,
      568.0904388427734,
      259.2498016357422,
      578.8949584960938
    ],
    "caption": "87.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.31700134277344,
      568.0904388427734,
      259.2498016357422,
      578.8949584960938
    ],
    "caption": "87.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.6419982910156,
      567.5524597167969,
      305.5747985839844,
      579.2177429199219
    ],
    "caption": "Pub-Brain-5",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.6419982910156,
      567.5524597167969,
      305.5747985839844,
      579.2177429199219
    ],
    "caption": "Pub-Brain-5",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.4459991455078,
      568.0904388427734,
      357.37879943847656,
      578.8949584960938
    ],
    "caption": "59.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.4459991455078,
      568.0904388427734,
      357.37879943847656,
      578.8949584960938
    ],
    "caption": "59.8",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7460021972656,
      567.5524597167969,
      410.6788024902344,
      579.2177429199219
    ],
    "caption": "50.4",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7460021972656,
      567.5524597167969,
      410.6788024902344,
      579.2177429199219
    ],
    "caption": "50.4",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.5299987792969,
      567.5524597167969,
      469.4627990722656,
      579.2177429199219
    ],
    "caption": "31.5",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.5299987792969,
      567.5524597167969,
      469.4627990722656,
      579.2177429199219
    ],
    "caption": "31.5",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      282.28399658203125,
      555.8360748291016,
      329.7162628173828,
      566.5419464111328
    ],
    "caption": "63.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      282.28399658203125,
      555.8360748291016,
      329.7162628173828,
      566.5419464111328
    ],
    "caption": "63.6",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      126.71696472167969,
      543.7852325439453,
      193.9649658203125,
      554.5897369384766
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      126.71696472167969,
      543.7852325439453,
      193.9649658203125,
      554.5897369384766
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24893188476562,
      543.7852325439453,
      224.18173217773438,
      554.5897369384766
    ],
    "caption": "91.5",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24893188476562,
      543.7852325439453,
      224.18173217773438,
      554.5897369384766
    ],
    "caption": "91.5",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.31651306152344,
      543.7852325439453,
      259.2493133544922,
      554.5897369384766
    ],
    "caption": "89.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.31651306152344,
      543.7852325439453,
      259.2493133544922,
      554.5897369384766
    ],
    "caption": "89.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.6459045410156,
      543.7852325439453,
      305.5787048339844,
      554.5897369384766
    ],
    "caption": "79.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.6459045410156,
      543.7852325439453,
      305.5787048339844,
      554.5897369384766
    ],
    "caption": "79.2",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.4447937011719,
      543.7852325439453,
      357.3775939941406,
      554.5897369384766
    ],
    "caption": "78.1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.4447937011719,
      543.7852325439453,
      357.3775939941406,
      554.5897369384766
    ],
    "caption": "78.1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7500457763672,
      543.7852325439453,
      410.68284606933594,
      554.5897369384766
    ],
    "caption": "63.3",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7500457763672,
      543.7852325439453,
      410.68284606933594,
      554.5897369384766
    ],
    "caption": "63.3",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.5337677001953,
      543.7852325439453,
      469.46656799316406,
      554.5897369384766
    ],
    "caption": "63.9",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.5337677001953,
      543.7852325439453,
      469.46656799316406,
      554.5897369384766
    ],
    "caption": "63.9",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      125.97274780273438,
      533.8235626220703,
      194.70916748046875,
      544.6280670166016
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      125.97274780273438,
      533.8235626220703,
      194.70916748046875,
      544.6280670166016
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24893188476562,
      533.8235626220703,
      224.18173217773438,
      544.6280670166016
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24893188476562,
      533.8235626220703,
      224.18173217773438,
      544.6280670166016
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.31651306152344,
      533.2855834960938,
      259.2493133544922,
      544.9508666992188
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.31651306152344,
      533.2855834960938,
      259.2493133544922,
      544.9508666992188
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.6459045410156,
      533.8235626220703,
      305.5787048339844,
      544.6280670166016
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.6459045410156,
      533.8235626220703,
      305.5787048339844,
      544.6280670166016
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.4537658691406,
      533.8235626220703,
      357.3865661621094,
      544.6280670166016
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.4537658691406,
      533.8235626220703,
      357.3865661621094,
      544.6280670166016
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7500457763672,
      533.8235626220703,
      410.68284606933594,
      544.6280670166016
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7500457763672,
      533.8235626220703,
      410.68284606933594,
      544.6280670166016
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.5337677001953,
      533.8235626220703,
      469.46656799316406,
      544.6280670166016
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.5337677001953,
      533.8235626220703,
      469.46656799316406,
      544.6280670166016
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      137.43099975585938,
      522.8604431152344,
      183.24929809570312,
      533.6649475097656
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      137.43099975585938,
      522.8604431152344,
      183.24929809570312,
      533.6649475097656
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24400329589844,
      522.3224639892578,
      224.1768035888672,
      533.9877471923828
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      206.24400329589844,
      522.3224639892578,
      224.1768035888672,
      533.9877471923828
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.31700134277344,
      522.8604431152344,
      259.2498016357422,
      533.6649475097656
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      241.31700134277344,
      522.8604431152344,
      259.2498016357422,
      533.6649475097656
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.6419982910156,
      522.3224639892578,
      305.5747985839844,
      533.9877471923828
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      287.6419982910156,
      522.3224639892578,
      305.5747985839844,
      533.9877471923828
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.4459991455078,
      522.3224639892578,
      357.37879943847656,
      533.9877471923828
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      339.4459991455078,
      522.3224639892578,
      357.37879943847656,
      533.9877471923828
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7460021972656,
      522.3224639892578,
      410.6788024902344,
      533.9877471923828
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.7460021972656,
      522.3224639892578,
      410.6788024902344,
      533.9877471923828
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.5299987792969,
      522.3224639892578,
      469.4627990722656,
      533.9877471923828
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      451.5299987792969,
      522.3224639892578,
      469.4627990722656,
      533.9877471923828
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      126.84199523925781,
      575.8614349365234,
      279.23915100097656,
      693.4659576416016
    ],
    "caption": "Table 5: Evaluation (Chest CT).",
    "file_name": [
      "tables/fileoutpart29.xlsx",
      "tables/fileoutpart30.png"
    ],
    "output_file": "assets/images/paper/HLIP-Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging/table_299.png"
  },
  {
    "page": 15,
    "bbox": [
      143.1739959716797,
      682.6614379882812,
      167.83160400390625,
      693.4659576416016
    ],
    "caption": "projection embed dim",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      143.1739959716797,
      682.6614379882812,
      167.83160400390625,
      693.4659576416016
    ],
    "caption": "projection embed dim",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      225.0399932861328,
      682.6614379882812,
      246.47866821289062,
      693.4659576416016
    ],
    "caption": "linear",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      225.0399932861328,
      682.6614379882812,
      246.47866821289062,
      693.4659576416016
    ],
    "caption": "linear",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      134.33660888671875,
      660.3457641601562,
      176.6759490966797,
      681.1119537353516
    ],
    "caption": "Visual Encoder",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      134.33660888671875,
      660.3457641601562,
      176.6759490966797,
      681.1119537353516
    ],
    "caption": "Visual Encoder",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      224.43099975585938,
      660.3444366455078,
      247.08909606933594,
      681.1119537353516
    ],
    "caption": "ViT-B [(<>)11]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      224.43099975585938,
      670.3074340820312,
      247.08909606933594,
      681.1119537353516
    ],
    "caption": "ViT-B [(<>)11]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      227.91400146484375,
      660.3444366455078,
      243.60519409179688,
      671.1489562988281
    ],
    "caption": "ViT-B [(<>)11]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      126.84199523925781,
      598.1787719726562,
      183.9579620361328,
      658.7959594726562
    ],
    "caption": "Text Encoder",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      126.84199523925781,
      648.0900726318359,
      183.9579620361328,
      658.7959594726562
    ],
    "caption": "Text Encoder",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      127.30670166015625,
      598.1787719726562,
      183.7053680419922,
      648.8342895507812
    ],
    "caption": "Text Encoder",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      192.2830047607422,
      598.1774444580078,
      279.23915100097656,
      658.7959594726562
    ],
    "caption": "CXR-BERT [(<>)7]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      215.02499389648438,
      647.9914398193359,
      256.4945983886719,
      658.7959594726562
    ],
    "caption": "(2,5,8,11)",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      192.2830047607422,
      637.3290557861328,
      279.23915100097656,
      648.5998229980469
    ],
    "caption": "(112,336,336) (8,24,24)",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      211.10800170898438,
      627.3660583496094,
      260.4142303466797,
      638.6368255615234
    ],
    "caption": "0.0",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      204.0489959716797,
      607.4410552978516,
      267.47731018066406,
      628.6748199462891
    ],
    "caption": "CXR-BERT [(<>)7]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      229.03500366210938,
      598.1774444580078,
      242.48460388183594,
      608.9819488525391
    ],
    "caption": "CXR-BERT [(<>)7]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      128.9213104248047,
      575.8627777099609,
      182.0830841064453,
      596.6289520263672
    ],
    "caption": "slice attn index scan attn index input size patch size patch dropout",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      130.75100708007812,
      585.9230651855469,
      180.0482635498047,
      596.6289520263672
    ],
    "caption": "slice attn index scan attn index input size patch size patch dropout",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      128.9213104248047,
      575.8627777099609,
      182.0830841064453,
      586.6672821044922
    ],
    "caption": "slice attn index scan attn index input size patch size patch dropout",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      206.39100646972656,
      585.8244476318359,
      265.1298828125,
      596.6289520263672
    ],
    "caption": "Table 5: Evaluation (Chest CT).",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      206.39100646972656,
      585.8244476318359,
      265.1298828125,
      596.6289520263672
    ],
    "caption": "Table 5: Evaluation (Chest CT).",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      221.5659942626953,
      575.8614349365234,
      249.95362854003906,
      586.6659545898438
    ],
    "caption": "Table 5: Evaluation (Chest CT).",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      221.5659942626953,
      575.8614349365234,
      249.95362854003906,
      586.6659545898438
    ],
    "caption": "Table 5: Evaluation (Chest CT).",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      324.9219055175781,
      573.2714385986328,
      492.9970703125,
      698.4469604492188
    ],
    "caption": "Table 5: Evaluation (Chest CT).",
    "file_name": [
      "tables/fileoutpart31.xlsx",
      "tables/fileoutpart32.png"
    ],
    "output_file": "assets/images/paper/HLIP-Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging/table_325.png"
  },
  {
    "page": 15,
    "bbox": [
      353.0559997558594,
      687.6424407958984,
      377.7135925292969,
      698.4469604492188
    ],
    "caption": "image precision truncate (window) normalize (mean, std) report",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      353.0559997558594,
      687.6424407958984,
      377.7135925292969,
      698.4469604492188
    ],
    "caption": "image precision truncate (window) normalize (mean, std) report",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      442.7619934082031,
      687.6424407958984,
      464.20066833496094,
      698.4469604492188
    ],
    "caption": "float32",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      442.7619934082031,
      687.6424407958984,
      464.20066833496094,
      698.4469604492188
    ],
    "caption": "float32",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      325.04808044433594,
      645.4017639160156,
      405.7277526855469,
      686.0929565429688
    ],
    "caption": "optimizer base learning rate weight decay optimizer momentum learning rate schedule warmup (in steps) numerical precision",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      325.04808044433594,
      645.4017639160156,
      405.7277526855469,
      686.0929565429688
    ],
    "caption": "optimizer base learning rate weight decay optimizer momentum learning rate schedule warmup (in steps) numerical precision",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      421.77000427246094,
      645.4004364013672,
      485.1983184814453,
      686.0929565429688
    ],
    "caption": "AdamW [(<>)61] 1e-5 0.2 1, 2 = 0.9, 0.98 [(<>)2] cosine decay [(<>)62] 47 [(<>)6] amp",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      439.906005859375,
      675.2884368896484,
      467.05625915527344,
      686.0929565429688
    ],
    "caption": "original",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      421.77000427246094,
      654.6640625,
      485.1983184814453,
      675.8978271484375
    ],
    "caption": "AdamW [(<>)61] 1e-5 0.2 1, 2 = 0.9, 0.98 [(<>)2] cosine decay [(<>)62] 47 [(<>)6] amp",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      438.4129943847656,
      645.4004364013672,
      468.549072265625,
      656.2049560546875
    ],
    "caption": "AdamW [(<>)61] 1e-5 0.2 1, 2 = 0.9, 0.98 [(<>)2] cosine decay [(<>)62] 47 [(<>)6] amp",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      324.9219055175781,
      573.2717742919922,
      405.8436584472656,
      643.8519592285156
    ],
    "caption": "Table 5: Evaluation (Chest CT).",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      324.9219055175781,
      573.2717742919922,
      405.8436584472656,
      643.8519592285156
    ],
    "caption": "Table 5: Evaluation (Chest CT).",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      413.9649963378906,
      573.2714385986328,
      492.9970703125,
      643.8519592285156
    ],
    "caption": "original",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      413.9649963378906,
      573.2714385986328,
      492.9970703125,
      643.8519592285156
    ],
    "caption": "original",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      180.537109375,
      356.1600646972656,
      431.4607391357422,
      529.4569549560547
    ],
    "caption": "B.2 Brain MRI/Head CT",
    "file_name": [
      "tables/fileoutpart33.xlsx",
      "tables/fileoutpart34.png"
    ],
    "output_file": "assets/images/paper/HLIP-Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging/table_340.png"
  },
  {
    "page": 15,
    "bbox": [
      216.5050048828125,
      518.6524353027344,
      267.47898864746094,
      529.4569549560547
    ],
    "caption": "Emphysema",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      216.5050048828125,
      518.6524353027344,
      267.47898864746094,
      529.4569549560547
    ],
    "caption": "Emphysema",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      338.2720031738281,
      518.6524353027344,
      404.7668151855469,
      529.4569549560547
    ],
    "caption": "emphysema",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      338.2720031738281,
      518.6524353027344,
      404.7668151855469,
      529.4569549560547
    ],
    "caption": "emphysema",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      218.72900390625,
      506.2994384765625,
      265.2556457519531,
      517.1039581298828
    ],
    "caption": "Lung nodule",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      218.72900390625,
      506.2994384765625,
      265.2556457519531,
      517.1039581298828
    ],
    "caption": "Lung nodule",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      349.2200012207031,
      505.6000671386719,
      393.8188781738281,
      516.8708343505859
    ],
    "caption": "nodule",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      349.2200012207031,
      505.6000671386719,
      393.8188781738281,
      516.8708343505859
    ],
    "caption": "nodule",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      221.19773864746094,
      496.3377685546875,
      262.7839050292969,
      507.1422882080078
    ],
    "caption": "Lung opacity",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      221.19773864746094,
      496.3377685546875,
      262.7839050292969,
      507.1422882080078
    ],
    "caption": "Lung opacity",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      344.5140075683594,
      495.63706970214844,
      398.527587890625,
      506.90782165527344
    ],
    "caption": "opacity",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      344.5140075683594,
      495.63706970214844,
      398.527587890625,
      506.90782165527344
    ],
    "caption": "opacity",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      218.07879638671875,
      486.37477111816406,
      265.8966064453125,
      497.1792907714844
    ],
    "caption": "Pulmonary fibrotic sequela",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      218.07879638671875,
      486.37477111816406,
      265.8966064453125,
      497.1792907714844
    ],
    "caption": "Pulmonary fibrotic sequela",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      356.2799987792969,
      485.67405700683594,
      386.7567901611328,
      496.94482421875
    ],
    "caption": "fibrosis",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      356.2799987792969,
      485.67405700683594,
      386.7567901611328,
      496.94482421875
    ],
    "caption": "fibrosis",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      217.08560180664062,
      476.4117736816406,
      266.8939514160156,
      487.2162780761719
    ],
    "caption": "Pleural effusion",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      217.08560180664062,
      476.4117736816406,
      266.8939514160156,
      487.2162780761719
    ],
    "caption": "Pleural effusion",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      353.927001953125,
      475.7120666503906,
      389.1111602783203,
      486.9828338623047
    ],
    "caption": "pleural_effusion",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      353.927001953125,
      475.7120666503906,
      389.1111602783203,
      486.9828338623047
    ],
    "caption": "pleural_effusion",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      192.55870056152344,
      466.44976806640625,
      291.4311981201172,
      477.25428771972656
    ],
    "caption": "Mosaic attenuation pattern",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      192.55870056152344,
      466.44976806640625,
      291.4311981201172,
      477.25428771972656
    ],
    "caption": "Mosaic attenuation pattern",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      351.572998046875,
      465.7490692138672,
      391.4645080566406,
      477.0198211669922
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      351.572998046875,
      465.7490692138672,
      391.4645080566406,
      477.0198211669922
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      212.46827697753906,
      456.4867706298828,
      271.5120086669922,
      467.2912902832031
    ],
    "caption": "Peribronchial thickening",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      212.46827697753906,
      456.4867706298828,
      271.5120086669922,
      467.2912902832031
    ],
    "caption": "Peribronchial thickening",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      332.7489929199219,
      455.7860565185547,
      410.29042053222656,
      467.05682373046875
    ],
    "caption": "bronchial_wall_thickening",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      332.7489929199219,
      455.7860565185547,
      410.29042053222656,
      467.05682373046875
    ],
    "caption": "bronchial_wall_thickening",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      193.0614471435547,
      446.5237731933594,
      290.9297180175781,
      457.3282775878906
    ],
    "caption": "Consolidation",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      193.0614471435547,
      446.5237731933594,
      290.9297180175781,
      457.3282775878906
    ],
    "caption": "Consolidation",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      368.093994140625,
      440.45318603515625,
      374.9443359375,
      457.09483337402344
    ],
    "caption": "consolidation",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      368.093994140625,
      440.45318603515625,
      374.9443359375,
      457.09483337402344
    ],
    "caption": "consolidation",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      196.71023559570312,
      436.561767578125,
      287.2708740234375,
      447.3662872314453
    ],
    "caption": "Bronchiectasis",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      196.71023559570312,
      436.561767578125,
      287.2708740234375,
      447.3662872314453
    ],
    "caption": "Bronchiectasis",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      311.5709991455078,
      435.86106872558594,
      431.4607391357422,
      447.13182067871094
    ],
    "caption": "bronchiectasis",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      311.5709991455078,
      435.86106872558594,
      431.4607391357422,
      447.13182067871094
    ],
    "caption": "bronchiectasis",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      215.71121215820312,
      426.59877014160156,
      268.2722473144531,
      437.4032897949219
    ],
    "caption": "Interlobular septal thickening",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      215.71121215820312,
      426.59877014160156,
      268.2722473144531,
      437.4032897949219
    ],
    "caption": "Interlobular septal thickening",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      339.80799865722656,
      425.89805603027344,
      403.23631286621094,
      437.1688232421875
    ],
    "caption": "septal_thickening",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      339.80799865722656,
      425.89805603027344,
      403.23631286621094,
      437.1688232421875
    ],
    "caption": "septal_thickening",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      214.4756622314453,
      416.6357727050781,
      269.5114288330078,
      427.4402770996094
    ],
    "caption": "Cardiomegaly",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      214.4756622314453,
      416.6357727050781,
      269.5114288330078,
      427.4402770996094
    ],
    "caption": "Cardiomegaly",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      337.4550018310547,
      415.9360656738281,
      405.5906677246094,
      427.2068328857422
    ],
    "caption": "cardiomegaly",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      337.4550018310547,
      415.9360656738281,
      405.5906677246094,
      427.2068328857422
    ],
    "caption": "cardiomegaly",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      188.2092742919922,
      406.67376708984375,
      295.77020263671875,
      417.47828674316406
    ],
    "caption": "Pericardial effusion",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      188.2092742919922,
      406.67376708984375,
      295.77020263671875,
      417.47828674316406
    ],
    "caption": "Pericardial effusion",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      330.39599609375,
      405.9730682373047,
      412.6358184814453,
      417.2438201904297
    ],
    "caption": "pericardial_effusion",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      330.39599609375,
      405.9730682373047,
      412.6358184814453,
      417.2438201904297
    ],
    "caption": "pericardial_effusion",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      215.5633087158203,
      396.7107696533203,
      268.42921447753906,
      407.5152893066406
    ],
    "caption": "Coronary artery wall calcification",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      215.5633087158203,
      396.7107696533203,
      268.42921447753906,
      407.5152893066406
    ],
    "caption": "Coronary artery wall calcification",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      342.16099548339844,
      396.0110626220703,
      400.8819580078125,
      407.2818298339844
    ],
    "caption": "calcification",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      342.16099548339844,
      396.0110626220703,
      400.8819580078125,
      407.2818298339844
    ],
    "caption": "calcification",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      205.74618530273438,
      386.74876403808594,
      278.23057556152344,
      397.55328369140625
    ],
    "caption": "Hiatal hernia",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      205.74618530273438,
      386.74876403808594,
      278.23057556152344,
      397.55328369140625
    ],
    "caption": "Hiatal hernia",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      323.33599853515625,
      386.0480651855469,
      419.6979064941406,
      397.31883239746094
    ],
    "caption": "hernia",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      323.33599853515625,
      386.0480651855469,
      419.6979064941406,
      397.31883239746094
    ],
    "caption": "hernia",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      180.537109375,
      376.7857666015625,
      303.4574890136719,
      387.5902862548828
    ],
    "caption": "Arterial wall calcification",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      180.537109375,
      376.7857666015625,
      303.4574890136719,
      387.5902862548828
    ],
    "caption": "Arterial wall calcification",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      339.80799865722656,
      376.08506774902344,
      403.23631286621094,
      387.35581970214844
    ],
    "caption": "calcification",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      339.80799865722656,
      376.08506774902344,
      403.23631286621094,
      387.35581970214844
    ],
    "caption": "calcification",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      217.58700561523438,
      366.82276916503906,
      266.39111328125,
      377.6272888183594
    ],
    "caption": "B.2 Brain MRI/Head CT",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      217.58700561523438,
      366.82276916503906,
      266.39111328125,
      377.6272888183594
    ],
    "caption": "B.2 Brain MRI/Head CT",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      356.2799987792969,
      366.12306213378906,
      386.7567901611328,
      377.3938293457031
    ],
    "caption": "pericardial_effusion",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      356.2799987792969,
      366.12306213378906,
      386.7567901611328,
      377.3938293457031
    ],
    "caption": "pericardial_effusion",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      195.09999084472656,
      356.86077880859375,
      288.88853454589844,
      367.665283203125
    ],
    "caption": "B.2 Brain MRI/Head CT",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      195.09999084472656,
      356.86077880859375,
      288.88853454589844,
      367.665283203125
    ],
    "caption": "B.2 Brain MRI/Head CT",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      339.80799865722656,
      356.1600646972656,
      403.23631286621094,
      367.4308319091797
    ],
    "caption": "Table 7: Pre-training (Brain MRI/Head CT).",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      339.80799865722656,
      356.1600646972656,
      403.23631286621094,
      367.4308319091797
    ],
    "caption": "Table 7: Pre-training (Brain MRI/Head CT).",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      125.802001953125,
      141.37744140625,
      280.26617431640625,
      258.98194885253906
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": [
      "tables/fileoutpart35.xlsx",
      "tables/fileoutpart36.png"
    ],
    "output_file": "assets/images/paper/HLIP-Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging/table_409.png"
  },
  {
    "page": 15,
    "bbox": [
      143.1739959716797,
      248.1774444580078,
      167.83160400390625,
      258.98194885253906
    ],
    "caption": "projection embed dim",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      143.1739959716797,
      248.1774444580078,
      167.83160400390625,
      258.98194885253906
    ],
    "caption": "projection embed dim",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      226.0760040283203,
      248.1774444580078,
      247.51466369628906,
      258.98194885253906
    ],
    "caption": "linear",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      226.0760040283203,
      248.1774444580078,
      247.51466369628906,
      258.98194885253906
    ],
    "caption": "linear",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      134.33151245117188,
      225.8617706298828,
      176.6708526611328,
      246.62796020507812
    ],
    "caption": "Visual Encoder",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      134.33151245117188,
      225.8617706298828,
      176.6708526611328,
      246.62796020507812
    ],
    "caption": "Visual Encoder",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      225.46600341796875,
      225.86044311523438,
      248.1240997314453,
      246.62796020507812
    ],
    "caption": "ViT-B [(<>)11]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      225.46600341796875,
      235.8234405517578,
      248.1240997314453,
      246.62796020507812
    ],
    "caption": "ViT-B [(<>)11]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      228.9499969482422,
      225.86044311523438,
      244.64120483398438,
      236.66494750976562
    ],
    "caption": "ViT-B [(<>)11]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      125.802001953125,
      163.6947784423828,
      185.1954345703125,
      224.31195068359375
    ],
    "caption": "Text Encoder",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      126.84199523925781,
      213.6060791015625,
      183.9579620361328,
      224.31195068359375
    ],
    "caption": "Text Encoder",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      125.802001953125,
      163.6947784423828,
      185.1954345703125,
      214.35028076171875
    ],
    "caption": "Text Encoder",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      193.31900024414062,
      163.69444274902344,
      280.26617431640625,
      224.31195068359375
    ],
    "caption": "PubMedBERT [(<>)23, (<>)44] 256 [(<>)23]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      216.06100463867188,
      213.5074462890625,
      257.5305938720703,
      224.31195068359375
    ],
    "caption": "(2,5,8,11)",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      193.31900024414062,
      202.84506225585938,
      280.26617431640625,
      214.11582946777344
    ],
    "caption": "(48,224,224) (8,16,16)",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      212.1439971923828,
      192.88206481933594,
      261.4502410888672,
      204.15283203125
    ],
    "caption": "0.25",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      207.43699645996094,
      172.95706176757812,
      266.157958984375,
      194.19082641601562
    ],
    "caption": "PubMedBERT [(<>)23, (<>)44] 256 [(<>)23]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      227.82899475097656,
      163.69444274902344,
      245.7617950439453,
      174.4989471435547
    ],
    "caption": "PubMedBERT [(<>)23, (<>)44] 256 [(<>)23]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      128.92576599121094,
      141.37876892089844,
      182.08755493164062,
      162.14495849609375
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      130.75100708007812,
      151.43907165527344,
      180.0482635498047,
      162.14495849609375
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      128.92576599121094,
      141.37876892089844,
      182.08755493164062,
      152.18328857421875
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      193.9770050048828,
      141.37744140625,
      279.6150817871094,
      162.14495849609375
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      193.9770050048828,
      141.37744140625,
      279.6150817871094,
      162.14495849609375
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      322.680908203125,
      138.78744506835938,
      495.23927307128906,
      263.96295166015625
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": [
      "tables/fileoutpart37.xlsx",
      "tables/fileoutpart38.png"
    ],
    "output_file": "assets/images/paper/HLIP-Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging/table_433.png"
  },
  {
    "page": 15,
    "bbox": [
      350.81500244140625,
      253.158447265625,
      375.47259521484375,
      263.96295166015625
    ],
    "caption": "image precision",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      350.81500244140625,
      253.158447265625,
      375.47259521484375,
      263.96295166015625
    ],
    "caption": "image precision",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      442.7619934082031,
      253.158447265625,
      464.20066833496094,
      263.96295166015625
    ],
    "caption": "uint8",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      442.7619934082031,
      253.158447265625,
      464.20066833496094,
      263.96295166015625
    ],
    "caption": "uint8",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      333.50999450683594,
      240.804443359375,
      392.7779083251953,
      251.60894775390625
    ],
    "caption": "normalize (mean, std)",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      333.50999450683594,
      240.804443359375,
      392.7779083251953,
      251.60894775390625
    ],
    "caption": "normalize (mean, std)",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      443.14300537109375,
      240.804443359375,
      463.81951904296875,
      251.60894775390625
    ],
    "caption": "[0.449,0.226]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      443.14300537109375,
      240.804443359375,
      463.81951904296875,
      251.60894775390625
    ],
    "caption": "[0.449,0.226]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      325.7997283935547,
      230.8427734375,
      400.4898376464844,
      241.64727783203125
    ],
    "caption": "report",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      325.7997283935547,
      230.8427734375,
      400.4898376464844,
      241.64727783203125
    ],
    "caption": "report",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      426.4759979248047,
      230.14306640625,
      480.4895935058594,
      241.41383361816406
    ],
    "caption": "GPT-3.5",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      426.4759979248047,
      230.14306640625,
      480.4895935058594,
      241.41383361816406
    ],
    "caption": "GPT-3.5",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      322.8064880371094,
      220.88076782226562,
      403.48614501953125,
      231.68528747558594
    ],
    "caption": "optimizer",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      322.8064880371094,
      220.88076782226562,
      403.48614501953125,
      231.68528747558594
    ],
    "caption": "optimizer",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      421.77000427246094,
      220.18006896972656,
      485.1983184814453,
      231.45082092285156
    ],
    "caption": "AdamW [(<>)61]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      421.77000427246094,
      220.18006896972656,
      485.1983184814453,
      231.45082092285156
    ],
    "caption": "AdamW [(<>)61]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      351.3209991455078,
      210.9177703857422,
      374.974365234375,
      221.7222900390625
    ],
    "caption": "optimizer",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      351.3209991455078,
      210.9177703857422,
      374.974365234375,
      221.7222900390625
    ],
    "caption": "optimizer",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      437.2070007324219,
      210.91644287109375,
      469.7550354003906,
      221.720947265625
    ],
    "caption": "AdamW [(<>)61]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      437.2070007324219,
      210.91644287109375,
      469.7550354003906,
      221.720947265625
    ],
    "caption": "AdamW [(<>)61]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      344.83900451660156,
      198.56344604492188,
      381.4488067626953,
      209.36795043945312
    ],
    "caption": "weight decay",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      344.83900451660156,
      198.56344604492188,
      381.4488067626953,
      209.36795043945312
    ],
    "caption": "weight decay",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      428.58099365234375,
      198.56344604492188,
      478.38038635253906,
      209.36795043945312
    ],
    "caption": "0.2",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      428.58099365234375,
      198.56344604492188,
      478.38038635253906,
      209.36795043945312
    ],
    "caption": "0.2",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      330.39892578125,
      188.60177612304688,
      395.8805389404297,
      199.40628051757812
    ],
    "caption": "optimizer momentum",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      330.39892578125,
      188.60177612304688,
      395.8805389404297,
      199.40628051757812
    ],
    "caption": "optimizer momentum",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      444.3939971923828,
      188.60044860839844,
      462.5688934326172,
      199.4049530029297
    ],
    "caption": "1, 2 = 0.9, 0.95 [(<>)20]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      444.3939971923828,
      188.60044860839844,
      462.5688934326172,
      199.4049530029297
    ],
    "caption": "1, 2 = 0.9, 0.95 [(<>)20]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      338.24078369140625,
      178.63877868652344,
      388.04017639160156,
      189.4432830810547
    ],
    "caption": "learning rate schedule",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      338.24078369140625,
      178.63877868652344,
      388.04017639160156,
      189.4432830810547
    ],
    "caption": "learning rate schedule",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      446.75599670410156,
      178.63844299316406,
      460.2055969238281,
      189.4429473876953
    ],
    "caption": "cosine decay [(<>)62]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      446.75599670410156,
      178.63844299316406,
      460.2055969238281,
      189.4429473876953
    ],
    "caption": "cosine decay [(<>)62]",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      323.2886657714844,
      168.67677307128906,
      402.9909973144531,
      179.4812774658203
    ],
    "caption": "warmup (in steps)",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      323.2886657714844,
      168.67677307128906,
      402.9909973144531,
      179.4812774658203
    ],
    "caption": "warmup (in steps)",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      411.72300720214844,
      168.67544555664062,
      495.23927307128906,
      179.47994995117188
    ],
    "caption": "2000",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      411.72300720214844,
      168.67544555664062,
      495.23927307128906,
      179.47994995117188
    ],
    "caption": "2000",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      322.680908203125,
      158.71377563476562,
      403.6026611328125,
      169.51828002929688
    ],
    "caption": "numerical precision",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      322.680908203125,
      158.71377563476562,
      403.6026611328125,
      169.51828002929688
    ],
    "caption": "numerical precision",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      420.74000549316406,
      158.7124481201172,
      486.22161865234375,
      169.51695251464844
    ],
    "caption": "amp",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      420.74000549316406,
      158.7124481201172,
      486.22161865234375,
      169.51695251464844
    ],
    "caption": "amp",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      329.69517517089844,
      148.7507781982422,
      396.59349060058594,
      159.55528259277344
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      329.69517517089844,
      148.7507781982422,
      396.59349060058594,
      159.55528259277344
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      443.3939971923828,
      148.7504425048828,
      463.5684051513672,
      159.55494689941406
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      443.3939971923828,
      148.7504425048828,
      463.5684051513672,
      159.55494689941406
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      326.534912109375,
      138.7887725830078,
      399.7455596923828,
      149.59327697753906
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      326.534912109375,
      138.7887725830078,
      399.7455596923828,
      149.59327697753906
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      444.63999938964844,
      138.78744506835938,
      462.3217468261719,
      149.59194946289062
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      444.63999938964844,
      138.78744506835938,
      462.3217468261719,
      149.59194946289062
    ],
    "caption": "The model architecture and pre-training configuration for brain MRI/head CT are summarized in Table (<>)6 and Table (<>)7, respectively. These configurations also follows prior works [(<>)13, (<>)20, (<>)23]. In the pre-training configuration, we report the base learning rate corresponding to a batch size of 128. During training, we follow the linear learning rate scaling rule [(<>)20]: lr = base_lr  . batch_size 128",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      124.27861022949219,
      537.6151733398438,
      487.7209777832031,
      683.8302764892578
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": [
      "tables/fileoutpart39.xlsx",
      "tables/fileoutpart40.png"
    ],
    "output_file": "assets/images/paper/HLIP-Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging/table_482.png"
  },
  {
    "page": 16,
    "bbox": [
      157.61399841308594,
      668.0404357910156,
      187.7500762939453,
      678.8449554443359
    ],
    "caption": "CT-CLIP [(<>)3]",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      157.61399841308594,
      668.0404357910156,
      187.7500762939453,
      678.8449554443359
    ],
    "caption": "CT-CLIP [(<>)3]",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      233.0393524169922,
      673.0257568359375,
      346.5181121826172,
      683.8302764892578
    ],
    "caption": "AUC",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      233.0393524169922,
      673.0257568359375,
      346.5181121826172,
      683.8302764892578
    ],
    "caption": "AUC",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      358.47032165527344,
      673.0257568359375,
      487.7209777832031,
      683.8302764892578
    ],
    "caption": "AUC",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      358.47032165527344,
      673.0257568359375,
      487.7209777832031,
      683.8302764892578
    ],
    "caption": "AUC",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      237.7760009765625,
      658.0534362792969,
      258.4525146484375,
      668.8579559326172
    ],
    "caption": "73.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      237.7760009765625,
      658.0534362792969,
      258.4525146484375,
      668.8579559326172
    ],
    "caption": "73.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      284.1053924560547,
      658.0534362792969,
      304.4232482910156,
      668.8579559326172
    ],
    "caption": "66.9",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      284.1053924560547,
      658.0534362792969,
      304.4232482910156,
      668.8579559326172
    ],
    "caption": "66.9",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      330.0761260986328,
      658.0534362792969,
      341.78623962402344,
      668.8579559326172
    ],
    "caption": "70.8",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      330.0761260986328,
      658.0534362792969,
      341.78623962402344,
      668.8579559326172
    ],
    "caption": "70.8",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      367.4391174316406,
      658.0534362792969,
      388.1156311035156,
      668.8579559326172
    ],
    "caption": "63.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      367.4391174316406,
      658.0534362792969,
      388.1156311035156,
      668.8579559326172
    ],
    "caption": "63.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      413.76849365234375,
      658.0534362792969,
      434.08636474609375,
      668.8579559326172
    ],
    "caption": "59.9",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      413.76849365234375,
      658.0534362792969,
      434.08636474609375,
      668.8579559326172
    ],
    "caption": "59.9",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      463.39752197265625,
      658.0534362792969,
      475.1076354980469,
      668.8579559326172
    ],
    "caption": "64.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      463.39752197265625,
      658.0534362792969,
      475.1076354980469,
      668.8579559326172
    ],
    "caption": "64.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      148.68800354003906,
      645.6994476318359,
      196.67617797851562,
      656.5039520263672
    ],
    "caption": "+ IN.MAE",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      148.68800354003906,
      645.6994476318359,
      196.67617797851562,
      656.5039520263672
    ],
    "caption": "+ IN.MAE",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      239.15000915527344,
      645.6994476318359,
      257.0828094482422,
      656.5039520263672
    ],
    "caption": "+0.8",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      239.15000915527344,
      645.6994476318359,
      257.0828094482422,
      656.5039520263672
    ],
    "caption": "+0.8",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.30006408691406,
      645.6994476318359,
      303.2328643798828,
      656.5039520263672
    ],
    "caption": "+0.1",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.30006408691406,
      645.6994476318359,
      303.2328643798828,
      656.5039520263672
    ],
    "caption": "+0.1",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.96693420410156,
      645.6994476318359,
      344.8997344970703,
      656.5039520263672
    ],
    "caption": "+0.2",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.96693420410156,
      645.6994476318359,
      344.8997344970703,
      656.5039520263672
    ],
    "caption": "+0.2",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.81312561035156,
      645.6994476318359,
      386.7459259033203,
      656.5039520263672
    ],
    "caption": "+7.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.81312561035156,
      645.6994476318359,
      386.7459259033203,
      656.5039520263672
    ],
    "caption": "+7.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.9631805419922,
      645.6994476318359,
      432.89598083496094,
      656.5039520263672
    ],
    "caption": "+6.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.9631805419922,
      645.6994476318359,
      432.89598083496094,
      656.5039520263672
    ],
    "caption": "+6.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.288330078125,
      645.6994476318359,
      478.22113037109375,
      656.5039520263672
    ],
    "caption": "+5.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.288330078125,
      645.6994476318359,
      478.22113037109375,
      656.5039520263672
    ],
    "caption": "+5.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      131.07798767089844,
      635.7377777099609,
      214.29515075683594,
      646.5422821044922
    ],
    "caption": "+ lock text",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      131.07798767089844,
      635.7377777099609,
      214.29515075683594,
      646.5422821044922
    ],
    "caption": "+ lock text",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      238.7554931640625,
      635.0383911132812,
      257.4773254394531,
      646.3091583251953
    ],
    "caption": "+0.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      238.7554931640625,
      635.0383911132812,
      257.4773254394531,
      646.3091583251953
    ],
    "caption": "+0.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      284.9055480957031,
      635.0383911132812,
      303.6273956298828,
      646.3091583251953
    ],
    "caption": "+0.5",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      284.9055480957031,
      635.0383911132812,
      303.6273956298828,
      646.3091583251953
    ],
    "caption": "+0.5",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.57240295410156,
      635.0383911132812,
      345.29425048828125,
      646.3091583251953
    ],
    "caption": "+0.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.57240295410156,
      635.0383911132812,
      345.29425048828125,
      646.3091583251953
    ],
    "caption": "+0.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.41859436035156,
      635.0383911132812,
      387.14044189453125,
      646.3091583251953
    ],
    "caption": "+1.2",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.41859436035156,
      635.0383911132812,
      387.14044189453125,
      646.3091583251953
    ],
    "caption": "+1.2",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.56866455078125,
      635.0383911132812,
      433.2904968261719,
      646.3091583251953
    ],
    "caption": "+0.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.56866455078125,
      635.0383911132812,
      433.2904968261719,
      646.3091583251953
    ],
    "caption": "+0.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      459.8848419189453,
      635.0383911132812,
      478.606689453125,
      646.3091583251953
    ],
    "caption": "+0.2",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      459.8848419189453,
      635.0383911132812,
      478.606689453125,
      646.3091583251953
    ],
    "caption": "+0.2",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      153.1622314453125,
      625.0767211914062,
      191.9867401123047,
      636.5806121826172
    ],
    "caption": "+ hierarchical attn",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      153.1622314453125,
      625.0767211914062,
      191.9867401123047,
      636.5806121826172
    ],
    "caption": "+ hierarchical attn",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      238.74652099609375,
      625.0767211914062,
      257.46836853027344,
      636.3474884033203
    ],
    "caption": "+0.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      238.74652099609375,
      625.0767211914062,
      257.46836853027344,
      636.3474884033203
    ],
    "caption": "+0.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      284.8965759277344,
      625.0767211914062,
      303.61842346191406,
      636.3474884033203
    ],
    "caption": "+1.0",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      284.8965759277344,
      625.0767211914062,
      303.61842346191406,
      636.3474884033203
    ],
    "caption": "+1.0",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.5634460449219,
      625.0767211914062,
      345.28529357910156,
      636.3474884033203
    ],
    "caption": "+0.8",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.5634460449219,
      625.0767211914062,
      345.28529357910156,
      636.3474884033203
    ],
    "caption": "+0.8",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.4096374511719,
      625.0767211914062,
      387.1314697265625,
      636.3474884033203
    ],
    "caption": "+0.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.4096374511719,
      625.0767211914062,
      387.1314697265625,
      636.3474884033203
    ],
    "caption": "+0.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.5596923828125,
      625.0767211914062,
      433.2815399169922,
      636.3474884033203
    ],
    "caption": "+1.1",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.5596923828125,
      625.0767211914062,
      433.2815399169922,
      636.3474884033203
    ],
    "caption": "+1.1",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      459.8758850097656,
      625.0767211914062,
      478.59771728515625,
      636.3474884033203
    ],
    "caption": "+1.0",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      459.8758850097656,
      625.0767211914062,
      478.59771728515625,
      636.3474884033203
    ],
    "caption": "+1.0",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      153.10842895507812,
      615.1150512695312,
      192.06744384765625,
      626.6189422607422
    ],
    "caption": "+batch size 512",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      153.10842895507812,
      615.1150512695312,
      192.06744384765625,
      626.6189422607422
    ],
    "caption": "+batch size 512",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      238.737548828125,
      615.1150512695312,
      257.4593963623047,
      626.3858184814453
    ],
    "caption": "+0.9",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      238.737548828125,
      615.1150512695312,
      257.4593963623047,
      626.3858184814453
    ],
    "caption": "+0.9",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      284.8876190185547,
      615.1150512695312,
      303.6094665527344,
      626.3858184814453
    ],
    "caption": "+0.1",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      284.8876190185547,
      615.1150512695312,
      303.6094665527344,
      626.3858184814453
    ],
    "caption": "+0.1",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.5544738769531,
      615.1150512695312,
      345.2763214111328,
      626.3858184814453
    ],
    "caption": "+0.2",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.5544738769531,
      615.1150512695312,
      345.2763214111328,
      626.3858184814453
    ],
    "caption": "+0.2",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.4006652832031,
      615.1150512695312,
      387.1225128173828,
      626.3858184814453
    ],
    "caption": "+0.6",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.4006652832031,
      615.1150512695312,
      387.1225128173828,
      626.3858184814453
    ],
    "caption": "+0.6",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.55072021484375,
      615.1150512695312,
      433.27256774902344,
      626.3858184814453
    ],
    "caption": "+0.6",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.55072021484375,
      615.1150512695312,
      433.27256774902344,
      626.3858184814453
    ],
    "caption": "+0.6",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      459.8669128417969,
      615.1150512695312,
      478.58876037597656,
      626.3858184814453
    ],
    "caption": "+1.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      459.8669128417969,
      615.1150512695312,
      478.58876037597656,
      626.3858184814453
    ],
    "caption": "+1.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      138.10765075683594,
      605.1533813476562,
      207.2206573486328,
      616.6572723388672
    ],
    "caption": "= HLIP",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      138.10765075683594,
      605.1533813476562,
      207.2206573486328,
      616.6572723388672
    ],
    "caption": "= HLIP",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      238.7285919189453,
      605.1533813476562,
      257.450439453125,
      616.4241485595703
    ],
    "caption": "77.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      238.7285919189453,
      605.1533813476562,
      257.450439453125,
      616.4241485595703
    ],
    "caption": "77.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      284.87864685058594,
      605.1533813476562,
      303.6004943847656,
      616.4241485595703
    ],
    "caption": "71.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      284.87864685058594,
      605.1533813476562,
      303.6004943847656,
      616.4241485595703
    ],
    "caption": "71.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.54551696777344,
      605.1533813476562,
      345.26734924316406,
      616.4241485595703
    ],
    "caption": "74.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.54551696777344,
      605.1533813476562,
      345.26734924316406,
      616.4241485595703
    ],
    "caption": "74.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.3916931152344,
      605.1533813476562,
      387.11354064941406,
      616.4241485595703
    ],
    "caption": "72.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.3916931152344,
      605.1533813476562,
      387.11354064941406,
      616.4241485595703
    ],
    "caption": "72.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.54176330566406,
      605.1533813476562,
      433.2635955810547,
      616.4241485595703
    ],
    "caption": "68.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.54176330566406,
      605.1533813476562,
      433.2635955810547,
      616.4241485595703
    ],
    "caption": "68.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      459.8579406738281,
      605.1533813476562,
      478.5797882080078,
      616.4241485595703
    ],
    "caption": "72.1",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      459.8579406738281,
      605.1533813476562,
      478.5797882080078,
      616.4241485595703
    ],
    "caption": "72.1",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      143.58612060546875,
      595.1917114257812,
      201.70542907714844,
      606.6956024169922
    ],
    "caption": "reportQwen",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      143.58612060546875,
      595.1917114257812,
      201.70542907714844,
      606.6956024169922
    ],
    "caption": "reportQwen",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      238.99757385253906,
      595.1917114257812,
      257.154541015625,
      606.6956024169922
    ],
    "caption": "78.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      238.99757385253906,
      595.1917114257812,
      257.154541015625,
      606.6956024169922
    ],
    "caption": "78.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.14764404296875,
      595.1917114257812,
      303.3045959472656,
      606.6956024169922
    ],
    "caption": "72.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.14764404296875,
      595.1917114257812,
      303.3045959472656,
      606.6956024169922
    ],
    "caption": "72.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.8144989013672,
      595.1917114257812,
      344.9714660644531,
      606.6956024169922
    ],
    "caption": "75.5",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.8144989013672,
      595.1917114257812,
      344.9714660644531,
      606.6956024169922
    ],
    "caption": "75.5",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.6606903076172,
      595.1917114257812,
      386.8176574707031,
      606.6956024169922
    ],
    "caption": "71.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.6606903076172,
      595.1917114257812,
      386.8176574707031,
      606.6956024169922
    ],
    "caption": "71.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.8107452392578,
      595.1917114257812,
      432.96771240234375,
      606.6956024169922
    ],
    "caption": "67.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.8107452392578,
      595.1917114257812,
      432.96771240234375,
      606.6956024169922
    ],
    "caption": "67.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.12693786621094,
      595.1917114257812,
      478.2839050292969,
      606.6956024169922
    ],
    "caption": "71.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.12693786621094,
      595.1917114257812,
      478.2839050292969,
      606.6956024169922
    ],
    "caption": "71.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      157.08055114746094,
      585.2300415039062,
      188.19395446777344,
      597.0567169189453
    ],
    "caption": "reportQwen",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      157.08055114746094,
      585.2300415039062,
      188.19395446777344,
      597.0567169189453
    ],
    "caption": "reportQwen",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      239.1051788330078,
      585.9294128417969,
      257.03797912597656,
      596.7339324951172
    ],
    "caption": "78.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      239.1051788330078,
      585.9294128417969,
      257.03797912597656,
      596.7339324951172
    ],
    "caption": "78.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.25523376464844,
      585.9294128417969,
      303.1880340576172,
      596.7339324951172
    ],
    "caption": "72.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.25523376464844,
      585.9294128417969,
      303.1880340576172,
      596.7339324951172
    ],
    "caption": "72.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.92210388183594,
      585.9294128417969,
      344.8549041748047,
      596.7339324951172
    ],
    "caption": "75.5",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.92210388183594,
      585.9294128417969,
      344.8549041748047,
      596.7339324951172
    ],
    "caption": "75.5",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.76829528808594,
      585.9294128417969,
      386.7010955810547,
      596.7339324951172
    ],
    "caption": "71.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.76829528808594,
      585.9294128417969,
      386.7010955810547,
      596.7339324951172
    ],
    "caption": "71.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.91835021972656,
      585.9294128417969,
      432.8511505126953,
      596.7339324951172
    ],
    "caption": "67.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.91835021972656,
      585.9294128417969,
      432.8511505126953,
      596.7339324951172
    ],
    "caption": "67.7",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.2434997558594,
      585.9294128417969,
      478.1763000488281,
      596.7339324951172
    ],
    "caption": "71.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.2434997558594,
      585.9294128417969,
      478.1763000488281,
      596.7339324951172
    ],
    "caption": "71.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      145.78900146484375,
      567.5001831054688,
      199.57843017578125,
      584.3749542236328
    ],
    "caption": "construct batchresize",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      145.78900146484375,
      567.5001831054688,
      199.57843017578125,
      584.3749542236328
    ],
    "caption": "construct batchresize",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      239.14715576171875,
      573.5704345703125,
      257.0799560546875,
      584.3749542236328
    ],
    "caption": "77.9",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      239.14715576171875,
      573.5704345703125,
      257.0799560546875,
      584.3749542236328
    ],
    "caption": "77.9",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.2972106933594,
      573.5704345703125,
      303.2300109863281,
      584.3749542236328
    ],
    "caption": "71.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.2972106933594,
      573.5704345703125,
      303.2300109863281,
      584.3749542236328
    ],
    "caption": "71.3",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.9640808105469,
      573.5704345703125,
      344.8968811035156,
      584.3749542236328
    ],
    "caption": "74.6",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.9640808105469,
      573.5704345703125,
      344.8968811035156,
      584.3749542236328
    ],
    "caption": "74.6",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.8102722167969,
      573.5704345703125,
      386.7430725097656,
      584.3749542236328
    ],
    "caption": "71.9",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.8102722167969,
      573.5704345703125,
      386.7430725097656,
      584.3749542236328
    ],
    "caption": "71.9",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.9603271484375,
      573.5704345703125,
      432.89312744140625,
      584.3749542236328
    ],
    "caption": "67.6",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.9603271484375,
      573.5704345703125,
      432.89312744140625,
      584.3749542236328
    ],
    "caption": "67.6",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.2854766845703,
      573.5704345703125,
      478.21827697753906,
      584.3749542236328
    ],
    "caption": "71.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.2854766845703,
      573.5704345703125,
      478.21827697753906,
      584.3749542236328
    ],
    "caption": "71.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      129.22805786132812,
      557.5385131835938,
      216.14834594726562,
      574.4132843017578
    ],
    "caption": "truncate[-1000,1000]",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      129.22805786132812,
      557.5385131835938,
      216.14834594726562,
      574.4132843017578
    ],
    "caption": "truncate[-1000,1000]",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      239.14715576171875,
      563.6087646484375,
      257.0799560546875,
      574.4132843017578
    ],
    "caption": "77.6",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      239.14715576171875,
      563.6087646484375,
      257.0799560546875,
      574.4132843017578
    ],
    "caption": "77.6",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.2972106933594,
      563.6087646484375,
      303.2300109863281,
      574.4132843017578
    ],
    "caption": "71.1",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.2972106933594,
      563.6087646484375,
      303.2300109863281,
      574.4132843017578
    ],
    "caption": "71.1",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.9640808105469,
      563.6087646484375,
      344.8968811035156,
      574.4132843017578
    ],
    "caption": "74.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.9640808105469,
      563.6087646484375,
      344.8968811035156,
      574.4132843017578
    ],
    "caption": "74.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.8102722167969,
      563.6087646484375,
      386.7430725097656,
      574.4132843017578
    ],
    "caption": "71.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.8102722167969,
      563.6087646484375,
      386.7430725097656,
      574.4132843017578
    ],
    "caption": "71.4",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.9603271484375,
      563.6087646484375,
      432.89312744140625,
      574.4132843017578
    ],
    "caption": "66.9",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.9603271484375,
      563.6087646484375,
      432.89312744140625,
      574.4132843017578
    ],
    "caption": "66.9",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.2854766845703,
      563.6087646484375,
      478.21827697753906,
      574.4132843017578
    ],
    "caption": "70.8",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.2854766845703,
      563.6087646484375,
      478.21827697753906,
      574.4132843017578
    ],
    "caption": "70.8",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      129.39842224121094,
      547.5768432617188,
      215.97708129882812,
      564.4516143798828
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      129.39842224121094,
      547.5768432617188,
      215.97708129882812,
      564.4516143798828
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      239.14715576171875,
      553.6470947265625,
      257.0799560546875,
      564.4516143798828
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      239.14715576171875,
      553.6470947265625,
      257.0799560546875,
      564.4516143798828
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.2972106933594,
      553.6470947265625,
      303.2300109863281,
      564.4516143798828
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.2972106933594,
      553.6470947265625,
      303.2300109863281,
      564.4516143798828
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.9640808105469,
      553.6470947265625,
      344.8968811035156,
      564.4516143798828
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.9640808105469,
      553.6470947265625,
      344.8968811035156,
      564.4516143798828
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.8102722167969,
      553.6470947265625,
      386.7430725097656,
      564.4516143798828
    ],
    "caption": "In this section, we address the question: What factors could influence language-image pre-training in radiology? under the constraint of a fixed data scale. Specifically, we evaluate the impact of several components added to the CT-CLIP baseline [(<>)3] on chest CT benchmarks [(<>)3, (<>)22]. As shown in Table (<>)8:",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.8102722167969,
      553.6470947265625,
      386.7430725097656,
      564.4516143798828
    ],
    "caption": "In this section, we address the question: What factors could influence language-image pre-training in radiology? under the constraint of a fixed data scale. Specifically, we evaluate the impact of several components added to the CT-CLIP baseline [(<>)3] on chest CT benchmarks [(<>)3, (<>)22]. As shown in Table (<>)8:",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.9603271484375,
      553.6470947265625,
      432.89312744140625,
      564.4516143798828
    ],
    "caption": "In this section, we address the question: What factors could influence language-image pre-training in radiology? under the constraint of a fixed data scale. Specifically, we evaluate the impact of several components added to the CT-CLIP baseline [(<>)3] on chest CT benchmarks [(<>)3, (<>)22]. As shown in Table (<>)8:",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.9603271484375,
      553.6470947265625,
      432.89312744140625,
      564.4516143798828
    ],
    "caption": "In this section, we address the question: What factors could influence language-image pre-training in radiology? under the constraint of a fixed data scale. Specifically, we evaluate the impact of several components added to the CT-CLIP baseline [(<>)3] on chest CT benchmarks [(<>)3, (<>)22]. As shown in Table (<>)8:",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.2854766845703,
      553.6470947265625,
      478.21827697753906,
      564.4516143798828
    ],
    "caption": "In this section, we address the question: What factors could influence language-image pre-training in radiology? under the constraint of a fixed data scale. Specifically, we evaluate the impact of several components added to the CT-CLIP baseline [(<>)3] on chest CT benchmarks [(<>)3, (<>)22]. As shown in Table (<>)8:",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.2854766845703,
      553.6470947265625,
      478.21827697753906,
      564.4516143798828
    ],
    "caption": "In this section, we address the question: What factors could influence language-image pre-training in radiology? under the constraint of a fixed data scale. Specifically, we evaluate the impact of several components added to the CT-CLIP baseline [(<>)3] on chest CT benchmarks [(<>)3, (<>)22]. As shown in Table (<>)8:",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      124.27861022949219,
      537.6151733398438,
      221.09779357910156,
      554.4899444580078
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      124.27861022949219,
      537.6151733398438,
      221.09779357910156,
      554.4899444580078
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      239.14715576171875,
      543.6854248046875,
      257.0799560546875,
      554.4899444580078
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      239.14715576171875,
      543.6854248046875,
      257.0799560546875,
      554.4899444580078
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.2972106933594,
      543.6854248046875,
      303.2300109863281,
      554.4899444580078
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      285.2972106933594,
      543.6854248046875,
      303.2300109863281,
      554.4899444580078
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.9640808105469,
      543.6854248046875,
      344.8968811035156,
      554.4899444580078
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      326.9640808105469,
      543.6854248046875,
      344.8968811035156,
      554.4899444580078
    ],
    "caption": "C Additional Ablation Studies on Chest CT",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.8102722167969,
      543.6854248046875,
      386.7430725097656,
      554.4899444580078
    ],
    "caption": "In this section, we address the question: What factors could influence language-image pre-training in radiology? under the constraint of a fixed data scale. Specifically, we evaluate the impact of several components added to the CT-CLIP baseline [(<>)3] on chest CT benchmarks [(<>)3, (<>)22]. As shown in Table (<>)8:",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      368.8102722167969,
      543.6854248046875,
      386.7430725097656,
      554.4899444580078
    ],
    "caption": "In this section, we address the question: What factors could influence language-image pre-training in radiology? under the constraint of a fixed data scale. Specifically, we evaluate the impact of several components added to the CT-CLIP baseline [(<>)3] on chest CT benchmarks [(<>)3, (<>)22]. As shown in Table (<>)8:",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.9603271484375,
      543.6854248046875,
      432.89312744140625,
      554.4899444580078
    ],
    "caption": "In this section, we address the question: What factors could influence language-image pre-training in radiology? under the constraint of a fixed data scale. Specifically, we evaluate the impact of several components added to the CT-CLIP baseline [(<>)3] on chest CT benchmarks [(<>)3, (<>)22]. As shown in Table (<>)8:",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      414.9603271484375,
      543.6854248046875,
      432.89312744140625,
      554.4899444580078
    ],
    "caption": "In this section, we address the question: What factors could influence language-image pre-training in radiology? under the constraint of a fixed data scale. Specifically, we evaluate the impact of several components added to the CT-CLIP baseline [(<>)3] on chest CT benchmarks [(<>)3, (<>)22]. As shown in Table (<>)8:",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.2854766845703,
      543.6854248046875,
      478.21827697753906,
      554.4899444580078
    ],
    "caption": "In this section, we address the question: What factors could influence language-image pre-training in radiology? under the constraint of a fixed data scale. Specifically, we evaluate the impact of several components added to the CT-CLIP baseline [(<>)3] on chest CT benchmarks [(<>)3, (<>)22]. As shown in Table (<>)8:",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      460.2854766845703,
      543.6854248046875,
      478.21827697753906,
      554.4899444580078
    ],
    "caption": "In this section, we address the question: What factors could influence language-image pre-training in radiology? under the constraint of a fixed data scale. Specifically, we evaluate the impact of several components added to the CT-CLIP baseline [(<>)3] on chest CT benchmarks [(<>)3, (<>)22]. As shown in Table (<>)8:",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      129.84378051757812,
      208.92843627929688,
      482.1566162109375,
      286.28395080566406
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": [
      "tables/fileoutpart54.xlsx",
      "tables/fileoutpart55.png"
    ],
    "output_file": "assets/images/paper/HLIP-Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging/table_655.png"
  },
  {
    "page": -1,
    "bbox": null,
    "caption": "",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      231.94400024414062,
      275.5780792236328,
      282.5144958496094,
      286.28395080566406
    ],
    "caption": "throughput (img/s)",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      231.94400024414062,
      275.5780792236328,
      282.5144958496094,
      286.28395080566406
    ],
    "caption": "throughput (img/s)",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      379.0429992675781,
      275.4794464111328,
      442.30992126464844,
      286.28395080566406
    ],
    "caption": "throughput (img/s)",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      379.0429992675781,
      275.4794464111328,
      442.30992126464844,
      286.28395080566406
    ],
    "caption": "throughput (img/s)",
    "file_name": ""
  },
  {
    "page": -1,
    "bbox": null,
    "caption": "",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      183.3820037841797,
      248.18276977539062,
      224.2239532470703,
      268.94895935058594
    ],
    "caption": "88.2",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      183.3820037841797,
      258.1444396972656,
      224.2239532470703,
      268.94895935058594
    ],
    "caption": "88.2",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      195.71080017089844,
      248.18276977539062,
      211.89515686035156,
      258.98728942871094
    ],
    "caption": "88.2",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      236.43618774414062,
      237.13616943359375,
      278.03131103515625,
      273.93426513671875
    ],
    "caption": "9.9",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      236.43618774414062,
      253.1680908203125,
      278.03131103515625,
      273.93426513671875
    ],
    "caption": "9.0",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      241.2511444091797,
      237.13616943359375,
      273.22532653808594,
      254.01092529296875
    ],
    "caption": "9.9",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      290.2435607910156,
      237.13616943359375,
      322.3701629638672,
      273.93426513671875
    ],
    "caption": "8.0",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      290.2435607910156,
      263.1297607421875,
      322.3701629638672,
      273.93426513671875
    ],
    "caption": "=better",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      298.9678649902344,
      253.1680908203125,
      313.6548309326172,
      263.97259521484375
    ],
    "caption": "6.6",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      290.3242492675781,
      237.13616943359375,
      322.2984313964844,
      254.01092529296875
    ],
    "caption": "8.0",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      330.4810028076172,
      248.18276977539062,
      371.3229522705078,
      268.94895935058594
    ],
    "caption": "88.2",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      330.4810028076172,
      258.1444396972656,
      371.3229522705078,
      268.94895935058594
    ],
    "caption": "88.2",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      342.80979919433594,
      248.18276977539062,
      358.99415588378906,
      258.98728942871094
    ],
    "caption": "88.2",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      389.8744354248047,
      237.13616943359375,
      431.4695587158203,
      273.93426513671875
    ],
    "caption": "9.9",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      389.8744354248047,
      253.1680908203125,
      431.4695587158203,
      273.93426513671875
    ],
    "caption": "44.4",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      394.68939208984375,
      237.13616943359375,
      426.66357421875,
      254.01092529296875
    ],
    "caption": "9.9",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      450.03001403808594,
      237.13616943359375,
      482.1566162109375,
      273.93426513671875
    ],
    "caption": "9.0",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      450.03001403808594,
      263.1297607421875,
      482.1566162109375,
      273.93426513671875
    ],
    "caption": "=better",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      458.74534606933594,
      253.1680908203125,
      473.43231201171875,
      263.97259521484375
    ],
    "caption": "1.5",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      450.10174560546875,
      237.13616943359375,
      482.075927734375,
      254.01092529296875
    ],
    "caption": "9.0",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      144.67799377441406,
      228.8544464111328,
      160.8264923095703,
      239.65895080566406
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      144.67799377441406,
      228.8544464111328,
      160.8264923095703,
      239.65895080566406
    ],
    "caption": "HLIP (ours)",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      194.83604431152344,
      228.8544464111328,
      212.7688446044922,
      239.65895080566406
    ],
    "caption": "88.2",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      194.83604431152344,
      228.8544464111328,
      212.7688446044922,
      239.65895080566406
    ],
    "caption": "88.2",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      250.5084228515625,
      228.8544464111328,
      263.95802307128906,
      239.65895080566406
    ],
    "caption": "17.5",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      250.5084228515625,
      228.8544464111328,
      263.95802307128906,
      239.65895080566406
    ],
    "caption": "17.5",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      299.5725555419922,
      228.8544464111328,
      313.02215576171875,
      239.65895080566406
    ],
    "caption": "4.1",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      299.5725555419922,
      228.8544464111328,
      313.02215576171875,
      239.65895080566406
    ],
    "caption": "4.1",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      341.93499755859375,
      228.8544464111328,
      359.8677978515625,
      239.65895080566406
    ],
    "caption": "88.2",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      341.93499755859375,
      228.8544464111328,
      359.8677978515625,
      239.65895080566406
    ],
    "caption": "88.2",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      401.7139892578125,
      228.8544464111328,
      419.64678955078125,
      239.65895080566406
    ],
    "caption": "47.6",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      401.7139892578125,
      228.8544464111328,
      419.64678955078125,
      239.65895080566406
    ],
    "caption": "47.6",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      459.36793518066406,
      228.8544464111328,
      472.8175354003906,
      239.65895080566406
    ],
    "caption": "1.5",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      459.36793518066406,
      228.8544464111328,
      472.8175354003906,
      239.65895080566406
    ],
    "caption": "1.5",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      142.4146728515625,
      218.8927764892578,
      163.0911865234375,
      229.69728088378906
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      142.4146728515625,
      218.8927764892578,
      163.0911865234375,
      229.69728088378906
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      194.84120178222656,
      218.8927764892578,
      212.7740020751953,
      229.69728088378906
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      194.84120178222656,
      218.8927764892578,
      212.7740020751953,
      229.69728088378906
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      250.50462341308594,
      218.8927764892578,
      263.9542236328125,
      229.69728088378906
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      250.50462341308594,
      218.8927764892578,
      263.9542236328125,
      229.69728088378906
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      299.5777282714844,
      218.8927764892578,
      313.02732849121094,
      229.69728088378906
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      299.5777282714844,
      218.8927764892578,
      313.02732849121094,
      229.69728088378906
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      341.93499755859375,
      218.89144897460938,
      359.8677978515625,
      229.69595336914062
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      341.93499755859375,
      218.89144897460938,
      359.8677978515625,
      229.69595336914062
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      403.95558166503906,
      218.89144897460938,
      417.4051818847656,
      229.69595336914062
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      403.95558166503906,
      218.89144897460938,
      417.4051818847656,
      229.69595336914062
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      459.36793518066406,
      218.89144897460938,
      472.8175354003906,
      229.69595336914062
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      459.36793518066406,
      218.89144897460938,
      472.8175354003906,
      229.69595336914062
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      129.84378051757812,
      208.9297637939453,
      175.66207885742188,
      219.73428344726562
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      129.84378051757812,
      208.9297637939453,
      175.66207885742188,
      219.73428344726562
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      194.84120178222656,
      208.9297637939453,
      212.7740020751953,
      219.73428344726562
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      194.84120178222656,
      208.9297637939453,
      212.7740020751953,
      219.73428344726562
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      248.2630157470703,
      208.9297637939453,
      266.19581604003906,
      219.73428344726562
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      248.2630157470703,
      208.9297637939453,
      266.19581604003906,
      219.73428344726562
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      299.5777282714844,
      208.9297637939453,
      313.02732849121094,
      219.73428344726562
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      299.5777282714844,
      208.9297637939453,
      313.02732849121094,
      219.73428344726562
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      341.93499755859375,
      208.92843627929688,
      359.8677978515625,
      219.7329559326172
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      341.93499755859375,
      208.92843627929688,
      359.8677978515625,
      219.7329559326172
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      401.7139892578125,
      208.92843627929688,
      419.64678955078125,
      219.7329559326172
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      401.7139892578125,
      208.92843627929688,
      419.64678955078125,
      219.7329559326172
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      459.36793518066406,
      208.92843627929688,
      472.8175354003906,
      219.7329559326172
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      459.36793518066406,
      208.92843627929688,
      472.8175354003906,
      219.7329559326172
    ],
    "caption": "All experiments are conducted on a single A40 GPU. We report the model size, throughput and training memory for each model. To measure throughput, we include 100 warm-up iterations and compute the average over 1000 forward passes. As shown in Table (<>)9, when not using flash attention [(<>)19], Swin is slightly faster than the original ViT but still lags behind our HLIP visual encoder. Swin also consumes more memory during training due to the large codebook of relative position embeddings constructed for 3D inputs. Moreover, Swin is not compatible with flash attention, which has been integrated into PyTorch 2.0 and supports a wide range of GPUs. We acknowledge that when using flash attention, the memory efficiency advantage of our hierarchical attention is reduced, since we do not alter the actual feature map size. However, HLIP remains faster than the original ViT.",
    "file_name": ""
  }
]