{
  "architecture": {
    "page": 2,
    "bbox": [
      134.67494201660156,
      424.5639343261719,
      480.6753845214844,
      619.2679901123047
    ],
    "caption": "Fig. 1. Multi-modality evidence fusion framework. It is composed of four encoder-decoder feature extraction (FE) modules corresponding to T1Gd, T1, T2 and Flair modality inputs; four evidential segmentation (ES) modules corresponding to each of the inputs; and a multi-modality evidence fusion (MMEF) module.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/Evidence-fusion-with-contextual-discounting-for-multi-modality-medical-image-segmentation/fig_01.png"
  },
  "results": {
    "page": 6,
    "bbox": [
      149.03330993652344,
      536.9364013671875,
      468.5546569824219,
      646.2588195800781
    ],
    "caption": "The quantitative results are reported in Table (<>)1. Our methods outperform the two classical CNN-based models and two recent transformer-based methods according to the Dice score, the best result being obtained by MMEF-nnUNet according to this criterion. In contrast, MMEF-UNet achieves the lowest HD.",
    "file_name": [
      "tables/fileoutpart13.xlsx",
      "tables/fileoutpart14.png"
    ],
    "output_file": "assets/images/paper/Evidence-fusion-with-contextual-discounting-for-multi-modality-medical-image-segmentation/table_01.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/Evidence-fusion-with-contextual-discounting-for-multi-modality-medical-image-segmentation/fig_01.png)\n캡션: Fig. 1. Multi-modality evidence fusion framework. It is composed of four encoder-decoder feature extraction (FE) modules corresponding to T1Gd, T1, T2 and Flair modality inputs; four evidential segmentation (ES) modules corresponding to each of the inputs; and a multi-modality evidence fusion (MMEF) module.\n\n### Main Results Table\n![Results](/assets/images/paper/Evidence-fusion-with-contextual-discounting-for-multi-modality-medical-image-segmentation/table_01.png)\n캡션: The quantitative results are reported in Table (<>)1. Our methods outperform the two classical CNN-based models and two recent transformer-based methods according to the Dice score, the best result being obtained by MMEF-nnUNet according to this criterion. In contrast, MMEF-UNet achieves the lowest HD."
}