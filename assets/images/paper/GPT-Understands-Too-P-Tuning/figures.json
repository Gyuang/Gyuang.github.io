[
  {
    "page": -1,
    "bbox": [
      306.07203674316406,
      560.3830413818359,
      524.4800262451172,
      629.3613433837891
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/GPT-Understands-Too-P-Tuning/fig_01.png"
  },
  {
    "page": 2,
    "bbox": [
      70.75602722167969,
      666.2150268554688,
      524.5241394042969,
      771.1337890625
    ],
    "caption": "Figure 2: An example of prompt search for “The capital of Britain is [MASK]”. Given the context (blue zone, “Britain”) and target (red zone, “[MASK]”), the orange zone refer to the prompt. In (a), the prompt generator only receives discrete rewards; on the contrary, in (b) the continuous prompt embeddings and prompt encoder can be optimized in a differentiable way.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/GPT-Understands-Too-P-Tuning/fig_02.png"
  },
  {
    "page": 2,
    "bbox": [
      70.75602722167969,
      666.2150268554688,
      524.5241394042969,
      771.1337890625
    ],
    "caption": "Figure 2: An example of prompt search for “The capital of Britain is [MASK]”. Given the context (blue zone, “Britain”) and target (red zone, “[MASK]”), the orange zone refer to the prompt. In (a), the prompt generator only receives discrete rewards; on the contrary, in (b) the continuous prompt embeddings and prompt encoder can be optimized in a differentiable way.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      93.81854248046875,
      518.5301818847656,
      268.9124450683594,
      538.8032836914062
    ],
    "caption": "P-Tuning leverages an extra embedding function f : [Pi] → hi to map the template to",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/GPT-Understands-Too-P-Tuning/fig_04.png"
  },
  {
    "page": 2,
    "bbox": [
      93.81854248046875,
      518.5301818847656,
      268.9124450683594,
      538.8032836914062
    ],
    "caption": "P-Tuning leverages an extra embedding function f : [Pi] → hi to map the template to",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      73.35704040527344,
      461.87718200683594,
      289.3697509765625,
      482.15028381347656
    ],
    "caption": "Finally, we update the embeddings {Pi}ik=1 to optimize a task loss function.",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/GPT-Understands-Too-P-Tuning/fig_06.png"
  },
  {
    "page": 2,
    "bbox": [
      73.35704040527344,
      461.87718200683594,
      289.3697509765625,
      482.15028381347656
    ],
    "caption": "Finally, we update the embeddings {Pi}ik=1 to optimize a task loss function.",
    "file_name": ""
  }
]