[
  {
    "page": 1,
    "bbox": [
      296.88478088378906,
      683.9091796875,
      355.94482421875,
      753.5196533203125
    ],
    "caption": "EuroSAT",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/2109.01134_Learning to Prompt for Vision-Language Models/fig_01.png"
  },
  {
    "page": 1,
    "bbox": [
      296.88478088378906,
      683.9091796875,
      355.94482421875,
      753.5196533203125
    ],
    "caption": "EuroSAT",
    "file_name": ""
  },
  {
    "page": 1,
    "bbox": [
      296.88478088378906,
      597.5517730712891,
      355.94482421875,
      667.062255859375
    ],
    "caption": "Fig. 1 Prompt engineering vs Context Optimization (CoOp). The former needs to use a held-out validation set for words tuning, which is ineﬃcient; the latter automates the process and requires only a few labeled images for learning.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/2109.01134_Learning to Prompt for Vision-Language Models/fig_03.png"
  },
  {
    "page": 1,
    "bbox": [
      296.88478088378906,
      597.5517730712891,
      355.94482421875,
      667.062255859375
    ],
    "caption": "Fig. 1 Prompt engineering vs Context Optimization (CoOp). The former needs to use a held-out validation set for words tuning, which is ineﬃcient; the latter automates the process and requires only a few labeled images for learning.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      67.27024841308594,
      552.2982482910156,
      511.4011993408203,
      751.8544311523438
    ],
    "caption": "Fig. 2 Overview of Context Optimization (CoOp). The main idea is to model a prompt’s context using a set of learnable vectors, which can be optimized through minimizing the classification loss. Two designs are proposed: one is unified context, which shares the same context vectors with all classes; and the other is class-specific context, which learns for each class a specific set of context vectors.",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/2109.01134_Learning to Prompt for Vision-Language Models/fig_05.png"
  },
  {
    "page": 3,
    "bbox": [
      67.27024841308594,
      552.2982482910156,
      511.4011993408203,
      751.8544311523438
    ],
    "caption": "Fig. 2 Overview of Context Optimization (CoOp). The main idea is to model a prompt’s context using a set of learnable vectors, which can be optimized through minimizing the classification loss. Two designs are proposed: one is unified context, which shares the same context vectors with all classes; and the other is class-specific context, which learns for each class a specific set of context vectors.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      42.11199951171875,
      686.418701171875,
      212.12139892578125,
      732.8014221191406
    ],
    "caption": "Compared with the traditional classifier learning approach where closed-set visual concepts are learned from random vectors, vision-language pre-training allows open-set visual concepts to be explored through a high-capacity text encoder, leading to a broader semantic space and in turn making the learned representations more transferable to downstream tasks.",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/2109.01134_Learning to Prompt for Vision-Language Models/fig_07.png"
  },
  {
    "page": 4,
    "bbox": [
      42.11199951171875,
      686.418701171875,
      212.12139892578125,
      732.8014221191406
    ],
    "caption": "Compared with the traditional classifier learning approach where closed-set visual concepts are learned from random vectors, vision-language pre-training allows open-set visual concepts to be explored through a high-capacity text encoder, leading to a broader semantic space and in turn making the learned representations more transferable to downstream tasks.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      42.09898376464844,
      389.2255554199219,
      174.03848266601562,
      401.40940856933594
    ],
    "caption": "where each [V]m (m ∈ {1, . . . , M}) is a vector with the same dimension as word embeddings (i.e., 512 for CLIP), and M is a hyperparameter specifying the number of context tokens.",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/2109.01134_Learning to Prompt for Vision-Language Models/fig_09.png"
  },
  {
    "page": 4,
    "bbox": [
      42.09898376464844,
      389.2255554199219,
      174.03848266601562,
      401.40940856933594
    ],
    "caption": "where each [V]m (m ∈ {1, . . . , M}) is a vector with the same dimension as word embeddings (i.e., 512 for CLIP), and M is a hyperparameter specifying the number of context tokens.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      42.11199951171875,
      228.56069946289062,
      220.452880859375,
      274.9424133300781
    ],
    "caption": "Other than placing the class token at the end of a sequence as in Equation ((<>)2), we can also put it in the middle like",
    "file_name": [
      "figures/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/2109.01134_Learning to Prompt for Vision-Language Models/fig_11.png"
  },
  {
    "page": 4,
    "bbox": [
      42.11199951171875,
      228.56069946289062,
      220.452880859375,
      274.9424133300781
    ],
    "caption": "Other than placing the class token at the end of a sequence as in Equation ((<>)2), we can also put it in the middle like",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      42.11199951171875,
      140.86866760253906,
      227.2632598876953,
      156.11041259765625
    ],
    "caption": "which increases ﬂexibility for learning—the prompt is allowed to either ﬁll the latter cells with supplementary descriptions or cut oﬀ the sentence earlier by using a termination signal such as full stop.",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/2109.01134_Learning to Prompt for Vision-Language Models/fig_13.png"
  },
  {
    "page": 4,
    "bbox": [
      42.11199951171875,
      140.86866760253906,
      227.2632598876953,
      156.11041259765625
    ],
    "caption": "which increases ﬂexibility for learning—the prompt is allowed to either ﬁll the latter cells with supplementary descriptions or cut oﬀ the sentence earlier by using a termination signal such as full stop.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      -259.93946838378906,
      147.0784912109375,
      912.6046600341797,
      806.9683227539062
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart11.png"
    ],
    "output_file": "assets/images/paper/2109.01134_Learning to Prompt for Vision-Language Models/fig_15.png"
  },
  {
    "page": 5,
    "bbox": [
      -259.93946838378906,
      147.0784912109375,
      912.6046600341797,
      806.9683227539062
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      293.24571228027344,
      582.6510925292969,
      539.1604156494141,
      755.7997283935547
    ],
    "caption": "Fig. 4 Comparison with hand-crafted prompts.",
    "file_name": [
      "figures/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/2109.01134_Learning to Prompt for Vision-Language Models/fig_17.png"
  },
  {
    "page": 6,
    "bbox": [
      293.24571228027344,
      582.6510925292969,
      539.1604156494141,
      755.7997283935547
    ],
    "caption": "Fig. 4 Comparison with hand-crafted prompts.",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      89.25289916992188,
      587.1472015380859,
      286.28431701660156,
      754.2797088623047
    ],
    "caption": "Fig. 5 Investigations on CoOp’s context length and various vision backbones.",
    "file_name": [
      "figures/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/2109.01134_Learning to Prompt for Vision-Language Models/fig_19.png"
  },
  {
    "page": 8,
    "bbox": [
      89.25289916992188,
      587.1472015380859,
      286.28431701660156,
      754.2797088623047
    ],
    "caption": "Fig. 5 Investigations on CoOp’s context length and various vision backbones.",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      301.7061462402344,
      301.5115661621094,
      495.36956787109375,
      754.2797088623047
    ],
    "caption": "Comparison with Prompt Ensembling The authors of CLIP ((<>)Radford et al., (<>)2021) have suggested that additional improvements can be obtained by ensembling over multiple zero-shot classifiers generated using diﬀerent hand-crafted prompts, such as “a photo of the large [CLASS].”, “a bad photo of the [CLASS].” and “a origami [CLASS].”, which reﬂect a diﬀerent scale, view and abstraction respectively for an image. We are interested to know whether the prompts learned by CoOp can still maintain advantages when compared with prompt ensembling. For fair comparison, we use the select prompts from (<>)Radford et al. ((<>)2021), which have been extensively tuned on Ima-geNet, to construct the ensemble classiﬁer. Table (<>)2 shows the comparison and justiﬁes the superiority of",
    "file_name": [
      "figures/fileoutpart16.png"
    ],
    "output_file": "assets/images/paper/2109.01134_Learning to Prompt for Vision-Language Models/fig_21.png"
  },
  {
    "page": 8,
    "bbox": [
      301.7061462402344,
      301.5115661621094,
      495.36956787109375,
      754.2797088623047
    ],
    "caption": "Comparison with Prompt Ensembling The authors of CLIP ((<>)Radford et al., (<>)2021) have suggested that additional improvements can be obtained by ensembling over multiple zero-shot classifiers generated using diﬀerent hand-crafted prompts, such as “a photo of the large [CLASS].”, “a bad photo of the [CLASS].” and “a origami [CLASS].”, which reﬂect a diﬀerent scale, view and abstraction respectively for an image. We are interested to know whether the prompts learned by CoOp can still maintain advantages when compared with prompt ensembling. For fair comparison, we use the select prompts from (<>)Radford et al. ((<>)2021), which have been extensively tuned on Ima-geNet, to construct the ensemble classiﬁer. Table (<>)2 shows the comparison and justiﬁes the superiority of",
    "file_name": ""
  }
]