[
  {
    "page": -1,
    "bbox": [
      59.75,
      705.25,
      195.94090270996094,
      761.2490234375
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_01.png"
  },
  {
    "page": -1,
    "bbox": [
      523.1862945556641,
      708.4037017822266,
      552.2463073730469,
      737.4636993408203
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_02.png"
  },
  {
    "page": -1,
    "bbox": [
      59.75,
      56.9739990234375,
      125.76249694824219,
      84.1199951171875
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_03.png"
  },
  {
    "page": 2,
    "bbox": [
      126.0,
      610.8199768066406,
      485.99998474121094,
      736.0999755859375
    ],
    "caption": "Fig. 1. Examples of anatomy-aware SynthMorph affine 3D registration showing the moving brain transformed onto the fixed brain (red overlay). Trained with highly variable synthetic data, SynthMorph generalizes across a diverse array of real- world contrasts, resolutions, and subject populations without any preprocessing.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_04.png"
  },
  {
    "page": 4,
    "bbox": [
      250.4062957763672,
      529.5711059570312,
      301.6790008544922,
      544.9660949707031
    ],
    "caption": "T = A v 0 0 1 = t 0 0 1 (1)",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_05.png"
  },
  {
    "page": 4,
    "bbox": [
      255.3125,
      529.5910949707031,
      260.7095031738281,
      538.7120971679688
    ],
    "caption": "T = A v 0 0 1 = t 0 0 1 (1)",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      294.875,
      536.7411041259766,
      301.6790008544922,
      544.9660949707031
    ],
    "caption": "T = A v 0 0 1 = t 0 0 1 (1)",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      250.4062957763672,
      529.5711059570312,
      301.6790008544922,
      544.9660949707031
    ],
    "caption": "T = A v 0 0 1 = t 0 0 1 (1)",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      60.28129577636719,
      503.96441650390625,
      86.76600646972656,
      520.1802520751953
    ],
    "caption": "T = A v 0 0 1 = t 0 0 1 (1)",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_09.png"
  },
  {
    "page": 4,
    "bbox": [
      60.28129577636719,
      503.96441650390625,
      86.76600646972656,
      520.1802520751953
    ],
    "caption": "T = A v 0 0 1 = t 0 0 1 (1)",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      151.34849548339844,
      504.29100036621094,
      200.56253051757812,
      518.3119964599609
    ],
    "caption": "T = A v 0 0 1 = t 0 0 1 (1)",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_11.png"
  },
  {
    "page": 4,
    "bbox": [
      156.25469970703125,
      504.29100036621094,
      161.65170288085938,
      513.4120025634766
    ],
    "caption": "T = A v 0 0 1 = t 0 0 1 (1)",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      175.31719970703125,
      504.29100036621094,
      178.96417236328125,
      513.4120025634766
    ],
    "caption": "T = A v 0 0 1 = t 0 0 1 (1)",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      151.34849548339844,
      504.29100036621094,
      200.56253051757812,
      518.3119964599609
    ],
    "caption": "T = A v 0 0 1 = t 0 0 1 (1)",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      85.80270385742188,
      429.47509765625,
      302.64105224609375,
      481.7725067138672
    ],
    "caption": "N×N",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_15.png"
  },
  {
    "page": 4,
    "bbox": [
      85.80270385742188,
      429.47509765625,
      302.64105224609375,
      481.7725067138672
    ],
    "caption": "N×N",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      118.72099304199219,
      395.87109375,
      162.2147979736328,
      411.63710021972656
    ],
    "caption": "θ",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_17.png"
  },
  {
    "page": 4,
    "bbox": [
      124.22079467773438,
      395.89109802246094,
      129.6177978515625,
      405.01210021972656
    ],
    "caption": "N×",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      145.47079467773438,
      402.51609802246094,
      162.2147979736328,
      411.63710021972656
    ],
    "caption": "N×1",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      118.72099304199219,
      395.87109375,
      162.2147979736328,
      411.63710021972656
    ],
    "caption": "θ",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      60.001800537109375,
      369.6710968017578,
      304.54283142089844,
      398.7371063232422
    ],
    "caption": "θ! = arg min θ E m,f( ) ∈ D 2Ls m! hθ m,f( ),f( )⎡ ⎣ ⎤⎦,",
    "file_name": [
      "figures/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_21.png"
  },
  {
    "page": 4,
    "bbox": [
      115.23309326171875,
      382.99110412597656,
      120.63009643554688,
      392.1121063232422
    ],
    "caption": "θ! = arg min θ E m,f( ) ∈ D 2Ls m! hθ m,f( ),f( )⎡ ⎣ ⎤⎦,",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      136.48309326171875,
      389.61610412597656,
      151.50010681152344,
      398.7371063232422
    ],
    "caption": "N×",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      107.43670654296875,
      369.69110107421875,
      112.83369445800781,
      378.8121032714844
    ],
    "caption": "θ! = arg min θ E m,f( ) ∈ D 2Ls m! hθ m,f( ),f( )⎡ ⎣ ⎤⎦,",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      128.68670654296875,
      376.47230529785156,
      139.84469604492188,
      385.5933074951172
    ],
    "caption": "θ! = arg min θ E m,f( ) ∈ D 2Ls m! hθ m,f( ),f( )⎡ ⎣ ⎤⎦,",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      60.001800537109375,
      369.6710968017578,
      304.54283142089844,
      398.7371063232422
    ],
    "caption": "θ! = arg min θ E m,f( ) ∈ D 2Ls m! hθ m,f( ),f( )⎡ ⎣ ⎤⎦,",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      97.625,
      324.60980224609375,
      264.9822998046875,
      349.6914978027344
    ],
    "caption": "where the loss Ls measures the similarity of two input images, and  means m transformed by",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_27.png"
  },
  {
    "page": 4,
    "bbox": [
      97.625,
      324.60980224609375,
      264.9822998046875,
      349.6914978027344
    ],
    "caption": "where the loss Ls measures the similarity of two input images, and  means m transformed by",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      113.90199279785156,
      288.0910949707031,
      136.42396545410156,
      301.6085968017578
    ],
    "caption": "3.1.2. Synthesis-based training",
    "file_name": [
      "figures/fileoutpart11.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_29.png"
  },
  {
    "page": 4,
    "bbox": [
      132.77699279785156,
      288.0910949707031,
      136.42396545410156,
      297.21209716796875
    ],
    "caption": "3.1.2. Synthesis-based training",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      113.90199279785156,
      288.0910949707031,
      136.42396545410156,
      301.6085968017578
    ],
    "caption": "3.1.2. Synthesis-based training",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      246.060302734375,
      287.5910949707031,
      302.64109802246094,
      304.1802520751953
    ],
    "caption": "A recent strategy ((<>)Billot, Greve, et al., 2023; (<>)Billot et al., (<>)2020; (<>)Hoffmann et al., 2022, (<>)2023; (<>)Hoopes, Mora, et al., (<>)2022) achieves robustness to preprocessing and acquisition specifics by training networks exclusively with synthetic images generated from label maps. From a set of inputs. Instead of image similarity, the strategy optimizes spatial label overlap with a (soft) Dice-based loss Lo ((<>)Milletari et al., 2016), strictly independent of image appearance:",
    "file_name": [
      "figures/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_32.png"
  },
  {
    "page": 4,
    "bbox": [
      250.9665985107422,
      287.5910949707031,
      256.3636016845703,
      296.71209716796875
    ],
    "caption": "A recent strategy ((<>)Billot, Greve, et al., 2023; (<>)Billot et al., (<>)2020; (<>)Hoffmann et al., 2022, (<>)2023; (<>)Hoopes, Mora, et al., (<>)2022) achieves robustness to preprocessing and acquisition specifics by training networks exclusively with synthetic images generated from label maps. From a set of inputs. Instead of image similarity, the strategy optimizes spatial label overlap with a (soft) Dice-based loss Lo ((<>)Milletari et al., 2016), strictly independent of image appearance:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      270.0290985107422,
      287.5910949707031,
      275.4261016845703,
      296.71209716796875
    ],
    "caption": "A recent strategy ((<>)Billot, Greve, et al., 2023; (<>)Billot et al., (<>)2020; (<>)Hoffmann et al., 2022, (<>)2023; (<>)Hoopes, Mora, et al., (<>)2022) achieves robustness to preprocessing and acquisition specifics by training networks exclusively with synthetic images generated from label maps. From a set of inputs. Instead of image similarity, the strategy optimizes spatial label overlap with a (soft) Dice-based loss Lo ((<>)Milletari et al., 2016), strictly independent of image appearance:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      246.060302734375,
      287.5910949707031,
      302.64109802246094,
      304.1802520751953
    ],
    "caption": "A recent strategy ((<>)Billot, Greve, et al., 2023; (<>)Billot et al., (<>)2020; (<>)Hoffmann et al., 2022, (<>)2023; (<>)Hoopes, Mora, et al., (<>)2022) achieves robustness to preprocessing and acquisition specifics by training networks exclusively with synthetic images generated from label maps. From a set of inputs. Instead of image similarity, the strategy optimizes spatial label overlap with a (soft) Dice-based loss Lo ((<>)Milletari et al., 2016), strictly independent of image appearance:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      60.003997802734375,
      158.56442260742188,
      305.68531799316406,
      188.64450073242188
    ],
    "caption": "Lo θ,s m,sf( ) = − 2 J j∈J x∈Ω ∑ sm |j !Tθ() x( ) × sf |j x( ) sm |j !Tθ( ) x( ) + sf |j x( ) ,",
    "file_name": [
      "figures/fileoutpart13.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_36.png"
  },
  {
    "page": 4,
    "bbox": [
      118.80389404296875,
      171.54409790039062,
      124.76792907714844,
      179.74110412597656
    ],
    "caption": "Lo θ,s m,sf( ) = − 2 J j∈J x∈Ω ∑ sm |j !Tθ() x( ) × sf |j x( ) sm |j !Tθ( ) x( ) + sf |j x( ) ,",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      132.92889404296875,
      171.54409790039062,
      135.0009307861328,
      179.74110412597656
    ],
    "caption": "Lo θ,s m,sf( ) = − 2 J j∈J x∈Ω ∑ sm |j !Tθ() x( ) × sf |j x( ) sm |j !Tθ( ) x( ) + sf |j x( ) ,",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      60.003997802734375,
      158.56442260742188,
      305.68531799316406,
      188.64450073242188
    ],
    "caption": "Lo θ,s m,sf( ) = − 2 J j∈J x∈Ω ∑ sm |j !Tθ() x( ) × sf |j x( ) sm |j !Tθ( ) x( ) + sf |j x( ) ,",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      81.62510681152344,
      51.1820068359375,
      280.10980224609375,
      100.10490417480469
    ],
    "caption": "A recent strategy ((<>)Billot, Greve, et al., 2023; (<>)Billot et al., (<>)2020; (<>)Hoffmann et al., 2022, (<>)2023; (<>)Hoopes, Mora, et al., (<>)2022) achieves robustness to preprocessing and acquisition specifics by training networks exclusively with synthetic images generated from label maps. From a set of inputs. Instead of image similarity, the strategy optimizes spatial label overlap with a (soft) Dice-based loss Lo ((<>)Milletari et al., 2016), strictly independent of image appearance:",
    "file_name": [
      "figures/fileoutpart14.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_40.png"
  },
  {
    "page": 4,
    "bbox": [
      81.62510681152344,
      51.1820068359375,
      280.10980224609375,
      100.10490417480469
    ],
    "caption": "A recent strategy ((<>)Billot, Greve, et al., 2023; (<>)Billot et al., (<>)2020; (<>)Hoffmann et al., 2022, (<>)2023; (<>)Hoopes, Mora, et al., (<>)2022) achieves robustness to preprocessing and acquisition specifics by training networks exclusively with synthetic images generated from label maps. From a set of inputs. Instead of image similarity, the strategy optimizes spatial label overlap with a (soft) Dice-based loss Lo ((<>)Milletari et al., 2016), strictly independent of image appearance:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      450.7908020019531,
      319.7522888183594,
      487.65699768066406,
      337.8249053955078
    ],
    "caption": "Let K be the complete set of labels in  To encourage networks to register specific anatomy while ignoring irrelevant image content, we propose to recode  such that the label maps include only a subset of labels J ⊂ K. For brain-specific registration, J consists of individual brain structures in the deformable case or larger tissue classes in the affine case. At training, the loss L optimizes only the overlap of J, whereas we synthesize images from the complete set of labels K, providing rich image content outside the brain as illustrated in (<>)Figure 2.",
    "file_name": [
      "figures/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_42.png"
  },
  {
    "page": 4,
    "bbox": [
      460.0720977783203,
      320.7243957519531,
      466.0361328125,
      328.92140197753906
    ],
    "caption": "Let K be the complete set of labels in  To encourage networks to register specific anatomy while ignoring irrelevant image content, we propose to recode  such that the label maps include only a subset of labels J ⊂ K. For brain-specific registration, J consists of individual brain structures in the deformable case or larger tissue classes in the affine case. At training, the loss L optimizes only the overlap of J, whereas we synthesize images from the complete set of labels K, providing rich image content outside the brain as illustrated in (<>)Figure 2.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      474.1970977783203,
      320.7243957519531,
      476.2691345214844,
      328.92140197753906
    ],
    "caption": "Let K be the complete set of labels in  To encourage networks to register specific anatomy while ignoring irrelevant image content, we propose to recode  such that the label maps include only a subset of labels J ⊂ K. For brain-specific registration, J consists of individual brain structures in the deformable case or larger tissue classes in the affine case. At training, the loss L optimizes only the overlap of J, whereas we synthesize images from the complete set of labels K, providing rich image content outside the brain as illustrated in (<>)Figure 2.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      450.7908020019531,
      319.7522888183594,
      487.65699768066406,
      337.8249053955078
    ],
    "caption": "Let K be the complete set of labels in  To encourage networks to register specific anatomy while ignoring irrelevant image content, we propose to recode  such that the label maps include only a subset of labels J ⊂ K. For brain-specific registration, J consists of individual brain structures in the deformable case or larger tissue classes in the affine case. At training, the loss L optimizes only the overlap of J, whereas we synthesize images from the complete set of labels K, providing rich image content outside the brain as illustrated in (<>)Figure 2.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      502.1011047363281,
      268.2522888183594,
      535.5486145019531,
      286.3249053955078
    ],
    "caption": "We compose the affine transform with a randomly sampled and randomly smoothed deformation field ((<>)Hoffmann et al., 2022) and apply the composite transform in a single interpolation step. Finally, we simulate acquisitions with a partial field of view (FOV) by randomly cropping the label map, yielding",
    "file_name": [
      "figures/fileoutpart16.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_46.png"
  },
  {
    "page": 4,
    "bbox": [
      511.3824005126953,
      269.2243957519531,
      517.346435546875,
      277.42140197753906
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      525.5074005126953,
      269.2243957519531,
      527.5794372558594,
      277.42140197753906
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      502.1011047363281,
      268.2522888183594,
      535.5486145019531,
      286.3249053955078
    ],
    "caption": "We compose the affine transform with a randomly sampled and randomly smoothed deformation field ((<>)Hoffmann et al., 2022) and apply the composite transform in a single interpolation step. Finally, we simulate acquisitions with a partial field of view (FOV) by randomly cropping the label map, yielding",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      312.0063018798828,
      229.35218811035156,
      343.6887664794922,
      247.4248046875
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart17.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_50.png"
  },
  {
    "page": 4,
    "bbox": [
      321.2875061035156,
      230.32440185546875,
      327.2515411376953,
      238.52139282226562
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      335.4125061035156,
      230.32440185546875,
      337.4845428466797,
      238.52139282226562
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      312.0063018798828,
      229.35218811035156,
      343.6887664794922,
      247.4248046875
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      74.94000244140625,
      581.6599884033203,
      537.0599975585938,
      736.0999755859375
    ],
    "caption": "Fig. 2. Training strategy for affine registration. At each iteration, we augment a pair of moving and fixed label maps only, such as WM, GM, and CSF.",
    "file_name": [
      "figures/fileoutpart18.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_54.png"
  },
  {
    "page": 5,
    "bbox": [
      60.00160217285156,
      538.1398162841797,
      552.2286682128906,
      565.7785949707031
    ],
    "caption": "ai = pi −1 x∈Ω ∑xFi | m x( ) and pi |m = x∈Ω ∑Fi | m x( ),",
    "file_name": [
      "figures/fileoutpart19.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_55.png"
  },
  {
    "page": 5,
    "bbox": [
      69.21879577636719,
      549.5119934082031,
      75.18283081054688,
      557.7089996337891
    ],
    "caption": "all voxels associated with label j. Second, we corrupt m by randomly applying additive Gaussian noise, anisotropic Gaussian blurring, a multiplicative spatial intensity bias field, intensity exponentiation with a global parameter, and downsampling along randomized axes. In aggregate, these steps produce widely varying intensity distributions within each anatomical label ((<>)Fig. 3).",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      82.875,
      549.5119934082031,
      84.94703674316406,
      557.7089996337891
    ],
    "caption": "all voxels associated with label j. Second, we corrupt m by randomly applying additive Gaussian noise, anisotropic Gaussian blurring, a multiplicative spatial intensity bias field, intensity exponentiation with a global parameter, and downsampling along randomized axes. In aggregate, these steps produce widely varying intensity distributions within each anatomical label ((<>)Fig. 3).",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      320.5218048095703,
      548.7590026855469,
      324.1687774658203,
      557.8800048828125
    ],
    "caption": "m",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      149.4575958251953,
      539.1840057373047,
      157.17160034179688,
      547.4089965820312
    ],
    "caption": "all voxels associated with label j. Second, we corrupt m by randomly applying additive Gaussian noise, anisotropic Gaussian blurring, a multiplicative spatial intensity bias field, intensity exponentiation with a global parameter, and downsampling along randomized axes. In aggregate, these steps produce widely varying intensity distributions within each anatomical label ((<>)Fig. 3).",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      220.84320068359375,
      538.8119964599609,
      224.86123657226562,
      547.0090026855469
    ],
    "caption": "all voxels associated with label j. Second, we corrupt m by randomly applying additive Gaussian noise, anisotropic Gaussian blurring, a multiplicative spatial intensity bias field, intensity exponentiation with a global parameter, and downsampling along randomized axes. In aggregate, these steps produce widely varying intensity distributions within each anatomical label ((<>)Fig. 3).",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      319.29949951171875,
      539.1119995117188,
      325.26353454589844,
      547.3090057373047
    ],
    "caption": "and separately center of mass bi with channel power pi | f for each Fi | f of the fixed image. We interpret the sets  and  as corresponding moving and fixed point clouds. Detector refers to a network hθ that predicts the affine transform  aligning these point clouds subject to",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      332.95570373535156,
      539.1119995117188,
      335.0277404785156,
      547.3090057373047
    ],
    "caption": "and separately center of mass bi with channel power pi | f for each Fi | f of the fixed image. We interpret the sets  and  as corresponding moving and fixed point clouds. Detector refers to a network hθ that predicts the affine transform  aligning these point clouds subject to",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      60.00160217285156,
      538.1398162841797,
      552.2286682128906,
      565.7785949707031
    ],
    "caption": "ai = pi −1 x∈Ω ∑xFi | m x( ) and pi |m = x∈Ω ∑Fi | m x( ),",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      231.94410705566406,
      178.5819549560547,
      266.6050262451172,
      194.30264282226562
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart20.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_64.png"
  },
  {
    "page": 5,
    "bbox": [
      231.94410705566406,
      178.5819549560547,
      266.6050262451172,
      194.30264282226562
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      342.5,
      488.16900634765625,
      524.4221954345703,
      515.3825073242188
    ],
    "caption": "and separately center of mass bi with channel power pi | f for each Fi | f of the fixed image. We interpret the sets  and  as corresponding moving and fixed point clouds. Detector refers to a network hθ that predicts the affine transform  aligning these point clouds subject to",
    "file_name": [
      "figures/fileoutpart21.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_66.png"
  },
  {
    "page": 5,
    "bbox": [
      342.5,
      488.16900634765625,
      524.4221954345703,
      515.3825073242188
    ],
    "caption": "and separately center of mass bi with channel power pi | f for each Fi | f of the fixed image. We interpret the sets  and  as corresponding moving and fixed point clouds. Detector refers to a network hθ that predicts the affine transform  aligning these point clouds subject to",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      312.1562042236328,
      432.3148956298828,
      328.9996643066406,
      450.3874969482422
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart22.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_68.png"
  },
  {
    "page": 5,
    "bbox": [
      321.75,
      433.28700256347656,
      323.3040008544922,
      441.48399353027344
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      312.1562042236328,
      432.3148956298828,
      328.9996643066406,
      450.3874969482422
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      352.4465026855469,
      432.01478576660156,
      369.8789520263672,
      450.08740234375
    ],
    "caption": "t!θ = arg min t εi || ai ⊤− (bi ⊤ 1)t ⊤ ||2 , i=1 k ∑",
    "file_name": [
      "figures/fileoutpart23.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_71.png"
  },
  {
    "page": 5,
    "bbox": [
      362.6340026855469,
      432.98699951171875,
      364.18800354003906,
      441.1840057373047
    ],
    "caption": "t!θ = arg min t εi || ai ⊤− (bi ⊤ 1)t ⊤ ||2 , i=1 k ∑",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      352.4465026855469,
      432.01478576660156,
      369.8789520263672,
      450.08740234375
    ],
    "caption": "t!θ = arg min t εi || ai ⊤− (bi ⊤ 1)t ⊤ ||2 , i=1 k ∑",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      383.97410583496094,
      406.33399963378906,
      431.06312561035156,
      420.35499572753906
    ],
    "caption": "t!θ = arg min t εi || ai ⊤− (bi ⊤ 1)t ⊤ ||2 , i=1 k ∑",
    "file_name": [
      "figures/fileoutpart24.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_74.png"
  },
  {
    "page": 5,
    "bbox": [
      386.7554016113281,
      406.33399963378906,
      392.15240478515625,
      415.4550018310547
    ],
    "caption": "t!θ = arg min t εi || ai ⊤− (bi ⊤ 1)t ⊤ ||2 , i=1 k ∑",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      405.8179016113281,
      406.33399963378906,
      409.4648742675781,
      415.4550018310547
    ],
    "caption": "t!θ = arg min t εi || ai ⊤− (bi ⊤ 1)t ⊤ ||2 , i=1 k ∑",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      383.97410583496094,
      406.33399963378906,
      431.06312561035156,
      420.35499572753906
    ],
    "caption": "t!θ = arg min t εi || ai ⊤− (bi ⊤ 1)t ⊤ ||2 , i=1 k ∑",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      354.3321990966797,
      357.67149353027344,
      504.50439453125,
      389.17149353027344
    ],
    "caption": "where we use the definition of t from (<>)Equation (1) as the submatrix of T that excludes the last row, and we define the normalized scalar weight εi as",
    "file_name": [
      "figures/fileoutpart25.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_78.png"
  },
  {
    "page": 5,
    "bbox": [
      354.3321990966797,
      357.67149353027344,
      504.50439453125,
      389.17149353027344
    ],
    "caption": "where we use the definition of t from (<>)Equation (1) as the submatrix of T that excludes the last row, and we define the normalized scalar weight εi as",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      356.6178283691406,
      268.52769470214844,
      508.95350646972656,
      300.02769470214844
    ],
    "caption": "Let X and y be matrices whose i th rows are ( ai ⊤1) and bi⊤ , respectively. Denoting W = diag εi{ }( ), the closed-form WLS solution t!θ of is such that",
    "file_name": [
      "figures/fileoutpart26.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_80.png"
  },
  {
    "page": 5,
    "bbox": [
      356.6178283691406,
      268.52769470214844,
      508.95350646972656,
      300.02769470214844
    ],
    "caption": "Let X and y be matrices whose i th rows are ( ai ⊤1) and bi⊤ , respectively. Denoting W = diag εi{ }( ), the closed-form WLS solution t!θ of is such that",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      312.0050048828125,
      212.34979248046875,
      554.0018615722656,
      251.9506072998047
    ],
    "caption": "t!θ ⊤= (X⊤WX )−1X⊤Wy.",
    "file_name": [
      "figures/fileoutpart27.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_82.png"
  },
  {
    "page": 5,
    "bbox": [
      393.78399658203125,
      214.1269989013672,
      445.6927185058594,
      225.20399475097656
    ],
    "caption": "t!θ ⊤= (X⊤WX )−1X⊤Wy.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      312.0050048828125,
      212.34979248046875,
      554.0018615722656,
      251.9506072998047
    ],
    "caption": "t!θ ⊤= (X⊤WX )−1X⊤Wy.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      387.8085021972656,
      182.31109619140625,
      478.9447021484375,
      197.55709838867188
    ],
    "caption": "3.3.2. Symmetric joint registration",
    "file_name": [
      "figures/fileoutpart28.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_85.png"
  },
  {
    "page": 5,
    "bbox": [
      387.8085021972656,
      182.31109619140625,
      478.9447021484375,
      197.55709838867188
    ],
    "caption": "3.3.2. Symmetric joint registration",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      452.09869384765625,
      75.30731201171875,
      478.9055938720703,
      91.52314758300781
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart29.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_87.png"
  },
  {
    "page": 5,
    "bbox": [
      452.09869384765625,
      75.30731201171875,
      478.9055938720703,
      91.52314758300781
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      126.0,
      376.09999084472656,
      485.99998474121094,
      736.0999755859375
    ],
    "caption": "Fig. 3. Synthetic 3D training data with arbitrary contrasts, resolutions, and artifact levels, generated from brain label maps. The image characteristics exceed the realistic range to promote network generalization across acquisition protocols. All examples are based on the same label map. In practice, we use several different subjects.",
    "file_name": [
      "figures/fileoutpart30.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_89.png"
  },
  {
    "page": 6,
    "bbox": [
      120.0,
      277.8852996826172,
      242.89100646972656,
      296.8632507324219
    ],
    "caption": "from which we obtain the diffeomorphic warp field φη via vector-field integration ((<>)Ashburner, 2007; (<>)Dalca et al., (<>)2018), and integrating  yields the inverse warp  up to the numerical precision of the algorithm used. Usually, approaches to learning deformable registration directly fit weights η by optimizing a loss of the form",
    "file_name": [
      "figures/fileoutpart31.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_90.png"
  },
  {
    "page": 6,
    "bbox": [
      120.0,
      277.8852996826172,
      242.89100646972656,
      296.8632507324219
    ],
    "caption": "from which we obtain the diffeomorphic warp field φη via vector-field integration ((<>)Ashburner, 2007; (<>)Dalca et al., (<>)2018), and integrating  yields the inverse warp  up to the numerical precision of the algorithm used. Usually, approaches to learning deformable registration directly fit weights η by optimizing a loss of the form",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      158.75689697265625,
      222.4676971435547,
      196.54037475585938,
      238.2136993408203
    ],
    "caption": "L(η,sm,sf ) = (1− λ)Lo(φη,sm,sf ) + λLr (φη),",
    "file_name": [
      "figures/fileoutpart32.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_92.png"
  },
  {
    "page": 6,
    "bbox": [
      164.03810119628906,
      222.4676971435547,
      168.2590789794922,
      231.5886993408203
    ],
    "caption": "L(η,sm,sf ) = (1− λ)Lo(φη,sm,sf ) + λLr (φη),",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      163.91310119628906,
      229.0926971435547,
      173.33639526367188,
      238.2136993408203
    ],
    "caption": "L(η,sm,sf ) = (1− λ)Lo(φη,sm,sf ) + λLr (φη),",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      192.31939697265625,
      222.4676971435547,
      196.54037475585938,
      231.5886993408203
    ],
    "caption": "L(η,sm,sf ) = (1− λ)Lo(φη,sm,sf ) + λLr (φη),",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      158.75689697265625,
      222.4676971435547,
      196.54037475585938,
      238.2136993408203
    ],
    "caption": "L(η,sm,sf ) = (1− λ)Lo(φη,sm,sf ) + λLr (φη),",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      60.125,
      209.5677032470703,
      74.70329284667969,
      225.31370544433594
    ],
    "caption": "where Lo quantifies label overlap as before, Lr is a regularization term that encourages smooth warps, and the parameter  controls the weighting of both terms.",
    "file_name": [
      "figures/fileoutpart33.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_97.png"
  },
  {
    "page": 6,
    "bbox": [
      65.40629577636719,
      209.5677032470703,
      69.62727355957031,
      218.68870544433594
    ],
    "caption": "where Lo quantifies label overlap as before, Lr is a regularization term that encourages smooth warps, and the parameter  controls the weighting of both terms.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      65.28129577636719,
      216.1927032470703,
      74.70329284667969,
      225.31370544433594
    ],
    "caption": "where Lo quantifies label overlap as before, Lr is a regularization term that encourages smooth warps, and the parameter  controls the weighting of both terms.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      60.125,
      209.5677032470703,
      74.70329284667969,
      225.31370544433594
    ],
    "caption": "where Lo quantifies label overlap as before, Lr is a regularization term that encourages smooth warps, and the parameter  controls the weighting of both terms.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      95.125,
      162.0677032470703,
      266.8596954345703,
      176.0886993408203
    ],
    "caption": "where Lo quantifies label overlap as before, Lr is a regularization term that encourages smooth warps, and the parameter  controls the weighting of both terms.",
    "file_name": [
      "figures/fileoutpart34.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_101.png"
  },
  {
    "page": 6,
    "bbox": [
      95.125,
      162.0677032470703,
      266.8596954345703,
      176.0886993408203
    ],
    "caption": "where Lo quantifies label overlap as before, Lr is a regularization term that encourages smooth warps, and the parameter  controls the weighting of both terms.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      500.9438018798828,
      267.6802978515625,
      540.3585357666016,
      284.7827453613281
    ],
    "caption": "As shown in (<>)Figure 4, for symmetric joint registration, we move images {m,f} into an affine mid-space using the matrix square roots of  and have gη predict νη between images  and  using kernels  specific to input λ.",
    "file_name": [
      "figures/fileoutpart35.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_103.png"
  },
  {
    "page": 6,
    "bbox": [
      522.47509765625,
      267.6802978515625,
      527.6761016845703,
      276.8013000488281
    ],
    "caption": "As shown in (<>)Figure 4, for symmetric joint registration, we move images {m,f} into an affine mid-space using the matrix square roots of  and have gη predict νη between images  and  using kernels  specific to input λ.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      500.9438018798828,
      267.6802978515625,
      540.3585357666016,
      284.7827453613281
    ],
    "caption": "As shown in (<>)Figure 4, for symmetric joint registration, we move images {m,f} into an affine mid-space using the matrix square roots of  and have gη predict νη between images  and  using kernels  specific to input λ.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      106.1842041015625,
      113.91044616699219,
      141.18893432617188,
      130.3738555908203
    ],
    "caption": "L(η,sm,sf ) = (1− λ)Lo(φη,sm,sf ) + λLr (φη),",
    "file_name": [
      "figures/fileoutpart36.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_106.png"
  },
  {
    "page": 6,
    "bbox": [
      106.1842041015625,
      113.91044616699219,
      141.18893432617188,
      130.3738555908203
    ],
    "caption": "L(η,sm,sf ) = (1− λ)Lo(φη,sm,sf ) + λLr (φη),",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      411.4188995361328,
      176.3553009033203,
      460.63291931152344,
      190.3762969970703
    ],
    "caption": "θ",
    "file_name": [
      "figures/fileoutpart37.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_108.png"
  },
  {
    "page": 6,
    "bbox": [
      416.3251037597656,
      176.3553009033203,
      421.72210693359375,
      185.47630310058594
    ],
    "caption": "While users of SynthMorph can choose between running the deformable step in the affine mid-space or after applying the full transform Tθ to m, only the former yields symmetric joint transforms. At training, the total forward transform is  and the loss of (<>)Equation (<>)(9) becomes",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      435.3876037597656,
      176.3553009033203,
      439.0345764160156,
      185.47630310058594
    ],
    "caption": "While users of SynthMorph can choose between running the deformable step in the affine mid-space or after applying the full transform Tθ to m, only the former yields symmetric joint transforms. At training, the total forward transform is  and the loss of (<>)Equation (<>)(9) becomes",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      411.4188995361328,
      176.3553009033203,
      460.63291931152344,
      190.3762969970703
    ],
    "caption": "θ",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      403.0189971923828,
      163.65530395507812,
      435.25579833984375,
      178.99530029296875
    ],
    "caption": "While users of SynthMorph can choose between running the deformable step in the affine mid-space or after applying the full transform Tθ to m, only the former yields symmetric joint transforms. At training, the total forward transform is  and the loss of (<>)Equation (<>)(9) becomes",
    "file_name": [
      "figures/fileoutpart38.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_112.png"
  },
  {
    "page": 6,
    "bbox": [
      421.89500427246094,
      163.65530395507812,
      425.54197692871094,
      172.77630615234375
    ],
    "caption": "While users of SynthMorph can choose between running the deformable step in the affine mid-space or after applying the full transform Tθ to m, only the former yields symmetric joint transforms. At training, the total forward transform is  and the loss of (<>)Equation (<>)(9) becomes",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      423.4888000488281,
      170.8052978515625,
      435.25579833984375,
      178.99530029296875
    ],
    "caption": "While users of SynthMorph can choose between running the deformable step in the affine mid-space or after applying the full transform Tθ to m, only the former yields symmetric joint transforms. At training, the total forward transform is  and the loss of (<>)Equation (<>)(9) becomes",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      403.0189971923828,
      163.65530395507812,
      435.25579833984375,
      178.99530029296875
    ],
    "caption": "While users of SynthMorph can choose between running the deformable step in the affine mid-space or after applying the full transform Tθ to m, only the former yields symmetric joint transforms. At training, the total forward transform is  and the loss of (<>)Equation (<>)(9) becomes",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      458.79139709472656,
      163.5552978515625,
      491.3083953857422,
      179.30130004882812
    ],
    "caption": "While users of SynthMorph can choose between running the deformable step in the affine mid-space or after applying the full transform Tθ to m, only the former yields symmetric joint transforms. At training, the total forward transform is  and the loss of (<>)Equation (<>)(9) becomes",
    "file_name": [
      "figures/fileoutpart39.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_116.png"
  },
  {
    "page": 6,
    "bbox": [
      473.72889709472656,
      163.5552978515625,
      477.37586975097656,
      172.67630004882812
    ],
    "caption": "While users of SynthMorph can choose between running the deformable step in the affine mid-space or after applying the full transform Tθ to m, only the former yields symmetric joint transforms. At training, the total forward transform is  and the loss of (<>)Equation (<>)(9) becomes",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      475.76019287109375,
      170.1802978515625,
      491.3083953857422,
      179.30130004882812
    ],
    "caption": "While users of SynthMorph can choose between running the deformable step in the affine mid-space or after applying the full transform Tθ to m, only the former yields symmetric joint transforms. At training, the total forward transform is  and the loss of (<>)Equation (<>)(9) becomes",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      458.79139709472656,
      163.5552978515625,
      491.3083953857422,
      179.30130004882812
    ],
    "caption": "While users of SynthMorph can choose between running the deformable step in the affine mid-space or after applying the full transform Tθ to m, only the former yields symmetric joint transforms. At training, the total forward transform is  and the loss of (<>)Equation (<>)(9) becomes",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      313.10400390625,
      150.4803009033203,
      352.5147247314453,
      167.58274841308594
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart40.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_120.png"
  },
  {
    "page": 6,
    "bbox": [
      334.6313018798828,
      150.4803009033203,
      339.8323059082031,
      159.60130310058594
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      313.10400390625,
      150.4803009033203,
      352.5147247314453,
      167.58274841308594
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      364.8851013183594,
      85.8489990234375,
      448.5148010253906,
      101.31399536132812
    ],
    "caption": "L(λ,θ,ξ, …) = (1− λ)Lo(ψθξ,sm,sf ) + λLr (φξ ), (10)",
    "file_name": [
      "figures/fileoutpart41.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_123.png"
  },
  {
    "page": 6,
    "bbox": [
      372.0726013183594,
      85.8489990234375,
      380.89959716796875,
      94.97000122070312
    ],
    "caption": "L(λ,θ,ξ, …) = (1− λ)Lo(ψθξ,sm,sf ) + λLr (φξ ), (10)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      394.54139709472656,
      85.9739990234375,
      399.9384002685547,
      95.09500122070312
    ],
    "caption": "L(λ,θ,ξ, …) = (1− λ)Lo(ψθξ,sm,sf ) + λLr (φξ ), (10)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      396.1374053955078,
      93.12100219726562,
      407.90440368652344,
      101.31100463867188
    ],
    "caption": "L(λ,θ,ξ, …) = (1− λ)Lo(ψθξ,sm,sf ) + λLr (φξ ), (10)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      418.35389709472656,
      85.8489990234375,
      423.5549011230469,
      94.97000122070312
    ],
    "caption": "L(λ,θ,ξ, …) = (1− λ)Lo(ψθξ,sm,sf ) + λLr (φξ ), (10)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      433.8226013183594,
      85.9739990234375,
      439.2196044921875,
      95.09500122070312
    ],
    "caption": "L(λ,θ,ξ, …) = (1− λ)Lo(ψθξ,sm,sf ) + λLr (φξ ), (10)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      435.41859436035156,
      93.12100219726562,
      447.1833953857422,
      101.31399536132812
    ],
    "caption": "L(λ,θ,ξ, …) = (1− λ)Lo(ψθξ,sm,sf ) + λLr (φξ ), (10)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      364.8851013183594,
      85.8489990234375,
      448.5148010253906,
      101.31399536132812
    ],
    "caption": "L(λ,θ,ξ, …) = (1− λ)Lo(ψθξ,sm,sf ) + λLr (φξ ), (10)",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      343.3688049316406,
      48.8489990234375,
      554.8915405273438,
      63.00129699707031
    ],
    "caption": "While users of SynthMorph can choose between running the deformable step in the affine mid-space or after applying the full transform Tθ to m, only the former yields symmetric joint transforms. At training, the total forward transform is  and the loss of (<>)Equation (<>)(9) becomes",
    "file_name": [
      "figures/fileoutpart42.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_131.png"
  },
  {
    "page": 6,
    "bbox": [
      343.3688049316406,
      48.8489990234375,
      554.8915405273438,
      63.00129699707031
    ],
    "caption": "While users of SynthMorph can choose between running the deformable step in the affine mid-space or after applying the full transform Tθ to m, only the former yields symmetric joint transforms. At training, the total forward transform is  and the loss of (<>)Equation (<>)(9) becomes",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      75.0,
      584.2000579833984,
      536.9999847412109,
      736.0000610351562
    ],
    "caption": "Fig. 4. Training strategy for joint registration. As in , network hθ predicts an affine transform T between moving and fixed images m,f{ } synthesized from label maps sm,sf{ }. Hypernetwork Γξ takes the regularization weight λ as input and outputs the parameters η = Γξ λ( ) of network gη. The moved images m!T1/2 and f !T −1/2 are inputs to gη , which predicts a diffeomorphic warp field φ. We form the symmetric joint transform ψ = T1/2 ! φ !T1/ 2 by composition and compute the moved label map sm ! ψ. Loss Lo recodes the labels of sm,sf{ } to optimize the overlap of select anatomy of interest—in this case brain labels only.",
    "file_name": [
      "figures/fileoutpart43.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_133.png"
  },
  {
    "page": 7,
    "bbox": [
      59.99609375,
      507.7760009765625,
      548.8053588867188,
      575.4750061035156
    ],
    "caption": "L(φ) = !∇u !,",
    "file_name": [
      "figures/fileoutpart44.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_134.png"
  },
  {
    "page": 7,
    "bbox": [
      268.6811981201172,
      564.7740020751953,
      301.1883239746094,
      575.2680053710938
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      59.99609375,
      507.7760009765625,
      548.8053588867188,
      575.4750061035156
    ],
    "caption": "L(φ) = !∇u !,",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      126.15660095214844,
      465.8489990234375,
      186.72329711914062,
      479.3450012207031
    ],
    "caption": "3.3.3. Overlap loss",
    "file_name": [
      "figures/fileoutpart45.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_137.png"
  },
  {
    "page": 7,
    "bbox": [
      132.06129455566406,
      465.8489990234375,
      136.14230346679688,
      474.0740051269531
    ],
    "caption": "3.3.3. Overlap loss",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      179.34249877929688,
      465.8769989013672,
      183.2344970703125,
      474.03900146484375
    ],
    "caption": "In this work, we replace the Dice-based overlap loss term of (<>)Equation (3) with a simpler term ((<>)Heinrich, 2019; (<>)Y. (<>)Wang et al., 2021) that measures MSE between one-hot encoded labels s | j ,",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      126.15660095214844,
      465.8489990234375,
      186.72329711914062,
      479.3450012207031
    ],
    "caption": "3.3.3. Overlap loss",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      76.625,
      300.1148986816406,
      302.6425476074219,
      340.5377960205078
    ],
    "caption": "θ,ξ{ }",
    "file_name": [
      "figures/fileoutpart46.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_141.png"
  },
  {
    "page": 7,
    "bbox": [
      76.625,
      300.1148986816406,
      302.6425476074219,
      340.5377960205078
    ],
    "caption": "θ,ξ{ }",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      203.29989624023438,
      269.1377716064453,
      225.14036560058594,
      286.59144592285156
    ],
    "caption": "As a result, the MSE loss term discourages the optimization to disproportionately focus on aligning smaller structures, which we find favorable for warp regularity at structure boundaries. In (<>)Appendix E, we analyze how optimizing Lo on label maps compares to an image- similarity loss term.",
    "file_name": [
      "figures/fileoutpart47.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_143.png"
  },
  {
    "page": 7,
    "bbox": [
      203.29989624023438,
      269.1377716064453,
      225.14036560058594,
      286.59144592285156
    ],
    "caption": "As a result, the MSE loss term discourages the optimization to disproportionately focus on aligning smaller structures, which we find favorable for warp regularity at structure boundaries. In (<>)Appendix E, we analyze how optimizing Lo on label maps compares to an image- similarity loss term.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      338.9907989501953,
      413.8665008544922,
      374.98109436035156,
      426.2449951171875
    ],
    "caption": "All kernels are of size 3N . For computational efficiency, our 3D models linearly downsample the network inputs  and loss inputs  by a factor of 2. We min- max normalize input images such that their intensities fall in the interval [0, 1]. Affine coordinate transforms operate in a zero-centered index space. (<>)Appendix B includes further details.",
    "file_name": [
      "figures/fileoutpart48.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_145.png"
  },
  {
    "page": 7,
    "bbox": [
      338.9907989501953,
      413.8665008544922,
      374.98109436035156,
      426.2449951171875
    ],
    "caption": "All kernels are of size 3N . For computational efficiency, our 3D models linearly downsample the network inputs  and loss inputs  by a factor of 2. We min- max normalize input images such that their intensities fall in the interval [0, 1]. Affine coordinate transforms operate in a zero-centered index space. (<>)Appendix B includes further details.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      311.9062957763672,
      334.497314453125,
      335.7512664794922,
      350.71315002441406
    ],
    "caption": "3.3.5. Optimization",
    "file_name": [
      "figures/fileoutpart49.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_147.png"
  },
  {
    "page": 7,
    "bbox": [
      311.9062957763672,
      334.497314453125,
      335.7512664794922,
      350.71315002441406
    ],
    "caption": "3.3.5. Optimization",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      408.15220642089844,
      334.0047912597656,
      439.83465576171875,
      352.077392578125
    ],
    "caption": "As in our prior work, the deformable model gη implements a U-Net ((<>)Ronneberger et al., 2015) architecture of width  and we integrate the SVF νη via scaling and squaring ((<>)Ashburner, 2007; (<>)Dalca et al., 2018). Hyper-model Γξ is a simple feed-forward network with 4 ReLU- activated hidden FC layers of 32 output units each.",
    "file_name": [
      "figures/fileoutpart50.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_149.png"
  },
  {
    "page": 7,
    "bbox": [
      417.4333953857422,
      334.9770050048828,
      423.3974304199219,
      343.1739959716797
    ],
    "caption": "As in our prior work, the deformable model gη implements a U-Net ((<>)Ronneberger et al., 2015) architecture of width  and we integrate the SVF νη via scaling and squaring ((<>)Ashburner, 2007; (<>)Dalca et al., 2018). Hyper-model Γξ is a simple feed-forward network with 4 ReLU- activated hidden FC layers of 32 output units each.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      431.5583953857422,
      334.9770050048828,
      433.63043212890625,
      343.1739959716797
    ],
    "caption": "As in our prior work, the deformable model gη implements a U-Net ((<>)Ronneberger et al., 2015) architecture of width  and we integrate the SVF νη via scaling and squaring ((<>)Ashburner, 2007; (<>)Dalca et al., 2018). Hyper-model Γξ is a simple feed-forward network with 4 ReLU- activated hidden FC layers of 32 output units each.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      408.15220642089844,
      334.0047912597656,
      439.83465576171875,
      352.077392578125
    ],
    "caption": "As in our prior work, the deformable model gη implements a U-Net ((<>)Ronneberger et al., 2015) architecture of width  and we integrate the SVF νη via scaling and squaring ((<>)Ashburner, 2007; (<>)Dalca et al., 2018). Hyper-model Γξ is a simple feed-forward network with 4 ReLU- activated hidden FC layers of 32 output units each.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      358.2769012451172,
      204.3769989013672,
      425.55963134765625,
      220.22935485839844
    ],
    "caption": "Because SynthMorph training is generally not prone to overfitting, it uses a simple stopping criterion measuring progress Pi over batches  in terms of validation Dice overlap D ((<>)Section 4.3). The 3D models train with a batch size of 1 until the mean overlap across Si exceeds  of the maximum, that is,",
    "file_name": [
      "figures/fileoutpart51.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_153.png"
  },
  {
    "page": 7,
    "bbox": [
      364.18310546875,
      204.3769989013672,
      365.7371063232422,
      212.57400512695312
    ],
    "caption": "Because SynthMorph training is generally not prone to overfitting, it uses a simple stopping criterion measuring progress Pi over batches  in terms of validation Dice overlap D ((<>)Section 4.3). The 3D models train with a batch size of 1 until the mean overlap across Si exceeds  of the maximum, that is,",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      358.2769012451172,
      204.3769989013672,
      425.55963134765625,
      220.22935485839844
    ],
    "caption": "Because SynthMorph training is generally not prone to overfitting, it uses a simple stopping criterion measuring progress Pi over batches  in terms of validation Dice overlap D ((<>)Section 4.3). The 3D models train with a batch size of 1 until the mean overlap across Si exceeds  of the maximum, that is,",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      517.6876068115234,
      180.3665008544922,
      554.0473022460938,
      194.47000122070312
    ],
    "caption": "Because SynthMorph training is generally not prone to overfitting, it uses a simple stopping criterion measuring progress Pi over batches  in terms of validation Dice overlap D ((<>)Section 4.3). The 3D models train with a batch size of 1 until the mean overlap across Si exceeds  of the maximum, that is,",
    "file_name": [
      "figures/fileoutpart52.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_156.png"
  },
  {
    "page": 7,
    "bbox": [
      542.9687957763672,
      185.3489990234375,
      550.6407928466797,
      194.47000122070312
    ],
    "caption": "Because SynthMorph training is generally not prone to overfitting, it uses a simple stopping criterion measuring progress Pi over batches  in terms of validation Dice overlap D ((<>)Section 4.3). The 3D models train with a batch size of 1 until the mean overlap across Si exceeds  of the maximum, that is,",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      517.6876068115234,
      180.3665008544922,
      554.0473022460938,
      194.47000122070312
    ],
    "caption": "Because SynthMorph training is generally not prone to overfitting, it uses a simple stopping criterion measuring progress Pi over batches  in terms of validation Dice overlap D ((<>)Section 4.3). The 3D models train with a batch size of 1 until the mean overlap across Si exceeds  of the maximum, that is,",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      311.75010681152344,
      167.51699829101562,
      357.39109802246094,
      180.8699951171875
    ],
    "caption": "Because SynthMorph training is generally not prone to overfitting, it uses a simple stopping criterion measuring progress Pi over batches  in terms of validation Dice overlap D ((<>)Section 4.3). The 3D models train with a batch size of 1 until the mean overlap across Si exceeds  of the maximum, that is,",
    "file_name": [
      "figures/fileoutpart53.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_159.png"
  },
  {
    "page": 7,
    "bbox": [
      322.43760681152344,
      171.74899291992188,
      330.10960388183594,
      180.8699951171875
    ],
    "caption": "Because SynthMorph training is generally not prone to overfitting, it uses a simple stopping criterion measuring progress Pi over batches  in terms of validation Dice overlap D ((<>)Section 4.3). The 3D models train with a batch size of 1 until the mean overlap across Si exceeds  of the maximum, that is,",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      342.31260681152344,
      171.74899291992188,
      351.73460388183594,
      180.8699951171875
    ],
    "caption": "Because SynthMorph training is generally not prone to overfitting, it uses a simple stopping criterion measuring progress Pi over batches  in terms of validation Dice overlap D ((<>)Section 4.3). The 3D models train with a batch size of 1 until the mean overlap across Si exceeds  of the maximum, that is,",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      311.75010681152344,
      167.51699829101562,
      357.39109802246094,
      180.8699951171875
    ],
    "caption": "Because SynthMorph training is generally not prone to overfitting, it uses a simple stopping criterion measuring progress Pi over batches  in terms of validation Dice overlap D ((<>)Section 4.3). The 3D models train with a batch size of 1 until the mean overlap across Si exceeds  of the maximum, that is,",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      419.9382019042969,
      87.57699584960938,
      440.05470275878906,
      101.04499816894531
    ],
    "caption": "We fit model parameters with stochastic gradient descent using Adam ((<>)Kingma & Ba, 2014) over consecutive training strips  of 106 batches each. At the beginning of each strip or in the event of divergence, we choose successively smaller learning rates from  For fast convergence, the first strip of affine training optimizes the overlap of larger label groups than indicated in (<>)Section 4.1.3: left hemisphere, right hemisphere, and cerebellum.",
    "file_name": [
      "figures/fileoutpart54.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_163.png"
  },
  {
    "page": 7,
    "bbox": [
      438.5007019042969,
      87.57699584960938,
      440.05470275878906,
      95.77400207519531
    ],
    "caption": "We fit model parameters with stochastic gradient descent using Adam ((<>)Kingma & Ba, 2014) over consecutive training strips  of 106 batches each. At the beginning of each strip or in the event of divergence, we choose successively smaller learning rates from  For fast convergence, the first strip of affine training optimizes the overlap of larger label groups than indicated in (<>)Section 4.1.3: left hemisphere, right hemisphere, and cerebellum.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      419.9382019042969,
      87.57699584960938,
      440.05470275878906,
      101.04499816894531
    ],
    "caption": "We fit model parameters with stochastic gradient descent using Adam ((<>)Kingma & Ba, 2014) over consecutive training strips  of 106 batches each. At the beginning of each strip or in the event of divergence, we choose successively smaller learning rates from  For fast convergence, the first strip of affine training optimizes the overlap of larger label groups than indicated in (<>)Section 4.1.3: left hemisphere, right hemisphere, and cerebellum.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      312.56260681152344,
      49.04899597167969,
      357.5157928466797,
      62.54499816894531
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart55.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_166.png"
  },
  {
    "page": 7,
    "bbox": [
      317.68760681152344,
      49.04899597167969,
      320.99159240722656,
      57.27400207519531
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      312.56260681152344,
      49.04899597167969,
      357.5157928466797,
      62.54499816894531
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      120.3125,
      711.7225036621094,
      242.4846954345703,
      739.7225036621094
    ],
    "caption": "For joint registration, we uniformly sample hyperpa-rameter values  For efficiency, we freeze parameters θ of the trained affine submodel hθ to fit only the weights ξ of hypernetwork Γξ, optimizing the loss of (<>)Equation (10).",
    "file_name": [
      "figures/fileoutpart56.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_169.png"
  },
  {
    "page": 8,
    "bbox": [
      120.3125,
      711.7225036621094,
      242.4846954345703,
      739.7225036621094
    ],
    "caption": "For joint registration, we uniformly sample hyperpa-rameter values  For efficiency, we freeze parameters θ of the trained affine submodel hθ to fit only the weights ξ of hypernetwork Γξ, optimizing the loss of (<>)Equation (10).",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      128.81460571289062,
      672.7695465087891,
      166.44200134277344,
      689.2329559326172
    ],
    "caption": "However, unfreezing the affine weights within the setup of (<>)Figure 4 has no substantial impact on accuracy. Specifically, after one additional strip of joint training, deformable large-21 Dice scores change by  depending on the dataset, while affine accuracy decreases by only ΔD < 0.1 points relative to affine-only training.",
    "file_name": [
      "figures/fileoutpart57.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_171.png"
  },
  {
    "page": 8,
    "bbox": [
      128.81460571289062,
      672.7695465087891,
      166.44200134277344,
      689.2329559326172
    ],
    "caption": "However, unfreezing the affine weights within the setup of (<>)Figure 4 has no substantial impact on accuracy. Specifically, after one additional strip of joint training, deformable large-21 Dice scores change by  depending on the dataset, while affine accuracy decreases by only ΔD < 0.1 points relative to affine-only training.",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      197.30450439453125,
      582.0513458251953,
      261.5309295654297,
      598.5147552490234
    ],
    "caption": "In a first experiment, we train the Detector architecture with synthetic data only. This experiment focuses on building a readily usable tool, and we assess its accuracy in various affine registration tasks. In contrast, (<>)Appendix (<>)A analyzes the performance of the different architectures across a broad range of variants and transformations, to understand how networks learn and best represent the affine component. In a second experiment, we complete the affine model with a deformable hypernetwork to produce a joint registration solution and compare its performance to readily usable baseline tools.",
    "file_name": [
      "figures/fileoutpart58.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_173.png"
  },
  {
    "page": 8,
    "bbox": [
      197.30450439453125,
      582.0513458251953,
      261.5309295654297,
      598.5147552490234
    ],
    "caption": "In a first experiment, we train the Detector architecture with synthetic data only. This experiment focuses on building a readily usable tool, and we assess its accuracy in various affine registration tasks. In contrast, (<>)Appendix (<>)A analyzes the performance of the different architectures across a broad range of variants and transformations, to understand how networks learn and best represent the affine component. In a second experiment, we complete the affine model with a deformable hypernetwork to produce a joint registration solution and compare its performance to readily usable baseline tools.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      124.625,
      173.5753936767578,
      237.69700622558594,
      203.2093963623047
    ],
    "caption": "Ω= {x ∈Ω| Jx( ) ≠ 0}.",
    "file_name": [
      "figures/fileoutpart63.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_175.png"
  },
  {
    "page": 10,
    "bbox": [
      124.625,
      173.5753936767578,
      237.69700622558594,
      203.2093963623047
    ],
    "caption": "Ω= {x ∈Ω| Jx( ) ≠ 0}.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      90.54719543457031,
      149.79110717773438,
      194.95970153808594,
      166.12155151367188
    ],
    "caption": "Jx( ) < 0.",
    "file_name": [
      "figures/fileoutpart64.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_177.png"
  },
  {
    "page": 10,
    "bbox": [
      97.82850646972656,
      150.31610107421875,
      104.37350463867188,
      158.54110717773438
    ],
    "caption": "Jx( ) < 0.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      97.70249938964844,
      154.90809631347656,
      100.16653442382812,
      163.07009887695312
    ],
    "caption": "Jx( ) < 0.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      138.9846954345703,
      150.31610107421875,
      145.52969360351562,
      158.54110717773438
    ],
    "caption": "T,T{},",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      155.2346954345703,
      149.79110717773438,
      160.63169860839844,
      158.91209411621094
    ],
    "caption": "T,T{},",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      90.54719543457031,
      149.79110717773438,
      194.95970153808594,
      166.12155151367188
    ],
    "caption": "Jx( ) < 0.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      273.0937042236328,
      137.71609497070312,
      301.1699981689453,
      151.21209716796875
    ],
    "caption": "(14)",
    "file_name": [
      "figures/fileoutpart65.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_183.png"
  },
  {
    "page": 10,
    "bbox": [
      294.625,
      137.71609497070312,
      301.1699981689453,
      145.94110107421875
    ],
    "caption": "(14)",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      273.0937042236328,
      137.71609497070312,
      301.1699981689453,
      151.21209716796875
    ],
    "caption": "(14)",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      89.31849670410156,
      123.69110107421875,
      129.27420043945312,
      140.02154541015625
    ],
    "caption": "E(T1,T2 ) = 1 |ΩB | x∈ΩB ∑ || (T2 !T1)( x) − x ||2.",
    "file_name": [
      "figures/fileoutpart66.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_186.png"
  },
  {
    "page": 10,
    "bbox": [
      94.03460693359375,
      123.69110107421875,
      99.43159484863281,
      132.81210327148438
    ],
    "caption": "Ω= {x ∈Ω| Jx( ) ≠ 0}.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      89.31849670410156,
      123.69110107421875,
      129.27420043945312,
      140.02154541015625
    ],
    "caption": "E(T1,T2 ) = 1 |ΩB | x∈ΩB ∑ || (T2 !T1)( x) − x ||2.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      137.595703125,
      84.17189025878906,
      172.68199157714844,
      102.2445068359375
    ],
    "caption": "E(T1,T2 ) = 1 |ΩB | x∈ΩB ∑ || (T2 !T1)( x) − x ||2.",
    "file_name": [
      "figures/fileoutpart67.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_189.png"
  },
  {
    "page": 10,
    "bbox": [
      146.5019989013672,
      85.14410400390625,
      150.3939971923828,
      93.30610656738281
    ],
    "caption": "E(T1,T2 ) = 1 |ΩB | x∈ΩB ∑ || (T2 !T1)( x) − x ||2.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      157.7519989013672,
      85.14410400390625,
      161.6439971923828,
      93.30610656738281
    ],
    "caption": "E(T1,T2 ) = 1 |ΩB | x∈ΩB ∑ || (T2 !T1)( x) − x ||2.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      137.595703125,
      84.17189025878906,
      172.68199157714844,
      102.2445068359375
    ],
    "caption": "E(T1,T2 ) = 1 |ΩB | x∈ΩB ∑ || (T2 !T1)( x) − x ||2.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      102.0,
      49.07539367675781,
      260.32850646972656,
      78.52189636230469
    ],
    "caption": "T,T{},",
    "file_name": [
      "figures/fileoutpart68.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_193.png"
  },
  {
    "page": 10,
    "bbox": [
      102.0,
      49.07539367675781,
      260.32850646972656,
      78.52189636230469
    ],
    "caption": "T,T{},",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      442.10890197753906,
      712.2160949707031,
      486.2877197265625,
      725.7120971679688
    ],
    "caption": "I h,m,f( ) = 0.5 E T1,T2( ) + E T2,T1( )⎡ ⎣ ⎤⎦.",
    "file_name": [
      "figures/fileoutpart69.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_195.png"
  },
  {
    "page": 10,
    "bbox": [
      446.73390197753906,
      712.2160949707031,
      452.3759002685547,
      720.4060974121094
    ],
    "caption": "I h,m,f( ) = 0.5 E T1,T2( ) + E T2,T1( )⎡ ⎣ ⎤⎦.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      442.10890197753906,
      712.2160949707031,
      486.2877197265625,
      725.7120971679688
    ],
    "caption": "I h,m,f( ) = 0.5 E T1,T2( ) + E T2,T1( )⎡ ⎣ ⎤⎦.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      505.9062042236328,
      712.2160949707031,
      553.9192047119141,
      725.7120971679688
    ],
    "caption": "(15)",
    "file_name": [
      "figures/fileoutpart70.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_198.png"
  },
  {
    "page": 10,
    "bbox": [
      510.9687042236328,
      712.2160949707031,
      516.6107025146484,
      720.4060974121094
    ],
    "caption": "In this experiment, we focus on “affine SynthMorph,” an anatomy-aware affine registration tool that generalizes across acquisition protocols while enabling brain registration without preprocessing. In contrast, (<>)Appendix A compares competing network architectures and analyzes how they learn and best represent affine transforms.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      505.9062042236328,
      712.2160949707031,
      553.9192047119141,
      725.7120971679688
    ],
    "caption": "(15)",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      431.0980987548828,
      698.9644165039062,
      460.28199768066406,
      715.1802520751953
    ],
    "caption": "I h,m,f( ) = 0.5 E T1,T2( ) + E T2,T1( )⎡ ⎣ ⎤⎦.",
    "file_name": [
      "figures/fileoutpart71.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_201.png"
  },
  {
    "page": 10,
    "bbox": [
      431.0980987548828,
      698.9644165039062,
      460.28199768066406,
      715.1802520751953
    ],
    "caption": "I h,m,f( ) = 0.5 E T1,T2( ) + E T2,T1( )⎡ ⎣ ⎤⎦.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      351.0,
      670.3379058837891,
      500.1750030517578,
      689.9445953369141
    ],
    "caption": "4.4. Experiment 1: affine registration",
    "file_name": [
      "figures/fileoutpart72.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_203.png"
  },
  {
    "page": 10,
    "bbox": [
      351.0,
      670.3379058837891,
      500.1750030517578,
      689.9445953369141
    ],
    "caption": "4.4. Experiment 1: affine registration",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      312.00450134277344,
      308.33380126953125,
      360.4572296142578,
      322.8121032714844
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart73.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_205.png"
  },
  {
    "page": 10,
    "bbox": [
      354.0162048339844,
      308.33380126953125,
      360.4572296142578,
      314.97999572753906
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      312.00450134277344,
      308.33380126953125,
      360.4572296142578,
      322.8121032714844
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      335.37669372558594,
      295.6757507324219,
      427.8449401855469,
      311.3964538574219
    ],
    "caption": "Third, we evaluate the importance of skull-stripping the input images for accurate registration. With the exception of skull-stripping, we preprocess full-head  pairs as expected by each method and assess brain-specific registration accuracy by evaluating image-based metrics within the brain only.",
    "file_name": [
      "figures/fileoutpart74.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_208.png"
  },
  {
    "page": 10,
    "bbox": [
      335.37669372558594,
      295.6757507324219,
      427.8449401855469,
      311.3964538574219
    ],
    "caption": "Third, we evaluate the importance of skull-stripping the input images for accurate registration. With the exception of skull-stripping, we preprocess full-head  pairs as expected by each method and assess brain-specific registration accuracy by evaluating image-based metrics within the brain only.",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      311.99839782714844,
      165.33380126953125,
      361.73143005371094,
      179.81210327148438
    ],
    "caption": "4.4.2. Results",
    "file_name": [
      "figures/fileoutpart75.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_210.png"
  },
  {
    "page": 10,
    "bbox": [
      355.2904052734375,
      165.33380126953125,
      361.73143005371094,
      171.97999572753906
    ],
    "caption": "4.4.2. Results",
    "file_name": ""
  },
  {
    "page": 10,
    "bbox": [
      311.99839782714844,
      165.33380126953125,
      361.73143005371094,
      179.81210327148438
    ],
    "caption": "4.4.2. Results",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      75.0,
      440.12001037597656,
      536.9999847412109,
      737.0
    ],
    "caption": "Fig. 5. Affine 3D registration accuracy as mean Dice scores and in terms of image similarity. Each violin shows the distribution across the skull-stripped cross-subject pairs from (<>)Table 1. For comparison, the asterisk indicates SynthMorph performance without skull-stripping. Downward arrows indicate median scores outside the plotted range. Higher Dice and lower MSE-MIND are better.",
    "file_name": [
      "figures/fileoutpart76.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_213.png"
  },
  {
    "page": 11,
    "bbox": [
      104.735595703125,
      321.83380126953125,
      159.72433471679688,
      336.3121032714844
    ],
    "caption": "not seen any real MRI data at training, it achieves the highest Dice score for every dataset tested.",
    "file_name": [
      "figures/fileoutpart77.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_214.png"
  },
  {
    "page": 11,
    "bbox": [
      153.28329467773438,
      321.83380126953125,
      159.72433471679688,
      328.47999572753906
    ],
    "caption": "not seen any real MRI data at training, it achieves the highest Dice score for every dataset tested.",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      104.735595703125,
      321.83380126953125,
      159.72433471679688,
      336.3121032714844
    ],
    "caption": "not seen any real MRI data at training, it achieves the highest Dice score for every dataset tested.",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      59.99879455566406,
      284.63710021972656,
      96.74899291992188,
      298.7371063232422
    ],
    "caption": "not seen any real MRI data at training, it achieves the highest Dice score for every dataset tested.",
    "file_name": [
      "figures/fileoutpart78.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_217.png"
  },
  {
    "page": 11,
    "bbox": [
      89.07699584960938,
      289.61610412597656,
      96.74899291992188,
      298.7371063232422
    ],
    "caption": "not seen any real MRI data at training, it achieves the highest Dice score for every dataset tested.",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      59.99879455566406,
      284.63710021972656,
      96.74899291992188,
      298.7371063232422
    ],
    "caption": "not seen any real MRI data at training, it achieves the highest Dice score for every dataset tested.",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      151.95359802246094,
      167.33360290527344,
      227.84112548828125,
      181.63710021972656
    ],
    "caption": "(p = 0.8).",
    "file_name": [
      "figures/fileoutpart79.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_220.png"
  },
  {
    "page": 11,
    "bbox": [
      213.4481964111328,
      172.51609802246094,
      224.9001922607422,
      181.63710021972656
    ],
    "caption": "In contrast, the DL baselines do not reach the same accuracy. Even for the T1w pairs they were trained with, SynthMorph leads by ΔD = 3.7 or more, likely due to domain shift between the test and baseline training data. As expected, DL-baseline performance continues to decrease as the test-image characteristics deviate further from those at training. Interestingly, VTN consistently",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      151.95359802246094,
      167.33360290527344,
      227.84112548828125,
      181.63710021972656
    ],
    "caption": "(p = 0.8).",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      88.15310668945312,
      152.83380126953125,
      141.14422607421875,
      167.31210327148438
    ],
    "caption": "In contrast, the DL baselines do not reach the same accuracy. Even for the T1w pairs they were trained with, SynthMorph leads by ΔD = 3.7 or more, likely due to domain shift between the test and baseline training data. As expected, DL-baseline performance continues to decrease as the test-image characteristics deviate further from those at training. Interestingly, VTN consistently",
    "file_name": [
      "figures/fileoutpart80.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_223.png"
  },
  {
    "page": 11,
    "bbox": [
      134.7032012939453,
      152.83380126953125,
      141.14422607421875,
      159.47999572753906
    ],
    "caption": "In contrast, the DL baselines do not reach the same accuracy. Even for the T1w pairs they were trained with, SynthMorph leads by ΔD = 3.7 or more, likely due to domain shift between the test and baseline training data. As expected, DL-baseline performance continues to decrease as the test-image characteristics deviate further from those at training. Interestingly, VTN consistently",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      88.15310668945312,
      152.83380126953125,
      141.14422607421875,
      167.31210327148438
    ],
    "caption": "In contrast, the DL baselines do not reach the same accuracy. Even for the T1w pairs they were trained with, SynthMorph leads by ΔD = 3.7 or more, likely due to domain shift between the test and baseline training data. As expected, DL-baseline performance continues to decrease as the test-image characteristics deviate further from those at training. Interestingly, VTN consistently",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      165.20770263671875,
      154.53359985351562,
      205.9656219482422,
      166.91209411621094
    ],
    "caption": "In contrast, the DL baselines do not reach the same accuracy. Even for the T1w pairs they were trained with, SynthMorph leads by ΔD = 3.7 or more, likely due to domain shift between the test and baseline training data. As expected, DL-baseline performance continues to decrease as the test-image characteristics deviate further from those at training. Interestingly, VTN consistently",
    "file_name": [
      "figures/fileoutpart81.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_226.png"
  },
  {
    "page": 11,
    "bbox": [
      165.20770263671875,
      154.53359985351562,
      205.9656219482422,
      166.91209411621094
    ],
    "caption": "In contrast, the DL baselines do not reach the same accuracy. Even for the T1w pairs they were trained with, SynthMorph leads by ΔD = 3.7 or more, likely due to domain shift between the test and baseline training data. As expected, DL-baseline performance continues to decrease as the test-image characteristics deviate further from those at training. Interestingly, VTN consistently",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      311.99440002441406,
      269.83380126953125,
      420.7560729980469,
      285.53709411621094
    ],
    "caption": "(<>)Figure 9 shows that SynthMorph’s affine transforms across are more symmetric than all baselines tested. When we reverse the order of the input images, the mean inconsistency between forward and backward transforms is I = 5 × 10−5 mm per brain voxel, closely followed by NiftyReg. Robust also uses an inverse- consistent algorithm, leading to  The remaining baselines are substantially less symmetric, with inconsistencies of I = 2 mm for KeyMorph or more.",
    "file_name": [
      "figures/fileoutpart82.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_228.png"
  },
  {
    "page": 11,
    "bbox": [
      353.8699035644531,
      269.83380126953125,
      360.1969909667969,
      276.47999572753906
    ],
    "caption": "(<>)Figure 9 shows that SynthMorph’s affine transforms across are more symmetric than all baselines tested. When we reverse the order of the input images, the mean inconsistency between forward and backward transforms is I = 5 × 10−5 mm per brain voxel, closely followed by NiftyReg. Robust also uses an inverse- consistent algorithm, leading to  The remaining baselines are substantially less symmetric, with inconsistencies of I = 2 mm for KeyMorph or more.",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      407.6109924316406,
      276.4161071777344,
      419.06549072265625,
      285.53709411621094
    ],
    "caption": "(<>)Figure 9 shows that SynthMorph’s affine transforms across are more symmetric than all baselines tested. When we reverse the order of the input images, the mean inconsistency between forward and backward transforms is I = 5 × 10−5 mm per brain voxel, closely followed by NiftyReg. Robust also uses an inverse- consistent algorithm, leading to  The remaining baselines are substantially less symmetric, with inconsistencies of I = 2 mm for KeyMorph or more.",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      311.99440002441406,
      269.83380126953125,
      420.7560729980469,
      285.53709411621094
    ],
    "caption": "(<>)Figure 9 shows that SynthMorph’s affine transforms across are more symmetric than all baselines tested. When we reverse the order of the input images, the mean inconsistency between forward and backward transforms is I = 5 × 10−5 mm per brain voxel, closely followed by NiftyReg. Robust also uses an inverse- consistent algorithm, leading to  The remaining baselines are substantially less symmetric, with inconsistencies of I = 2 mm for KeyMorph or more.",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      468.99249267578125,
      271.43359375,
      554.6411285400391,
      284.3121032714844
    ],
    "caption": "(<>)Figure 9 shows that SynthMorph’s affine transforms across are more symmetric than all baselines tested. When we reverse the order of the input images, the mean inconsistency between forward and backward transforms is I = 5 × 10−5 mm per brain voxel, closely followed by NiftyReg. Robust also uses an inverse- consistent algorithm, leading to  The remaining baselines are substantially less symmetric, with inconsistencies of I = 2 mm for KeyMorph or more.",
    "file_name": [
      "figures/fileoutpart83.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_232.png"
  },
  {
    "page": 11,
    "bbox": [
      468.99249267578125,
      271.43359375,
      554.6411285400391,
      284.3121032714844
    ],
    "caption": "(<>)Figure 9 shows that SynthMorph’s affine transforms across are more symmetric than all baselines tested. When we reverse the order of the input images, the mean inconsistency between forward and backward transforms is I = 5 × 10−5 mm per brain voxel, closely followed by NiftyReg. Robust also uses an inverse- consistent algorithm, leading to  The remaining baselines are substantially less symmetric, with inconsistencies of I = 2 mm for KeyMorph or more.",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      340.1501007080078,
      191.83380126953125,
      391.6051330566406,
      206.31210327148438
    ],
    "caption": "Even though affine SynthMorph does not directly optimize image similarity at training, it surpasses NiftyReg for  and MASi pairs in terms of the image-based MSE-MIND metric. Generally, MSE-MIND ranks the methods similarly to Dice overlap, as does NCC across the T1w registration pairs ((<>)Fig. 8a).",
    "file_name": [
      "figures/fileoutpart84.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_234.png"
  },
  {
    "page": 11,
    "bbox": [
      385.1640930175781,
      191.83380126953125,
      391.6051330566406,
      198.47999572753906
    ],
    "caption": "Even though affine SynthMorph does not directly optimize image similarity at training, it surpasses NiftyReg for  and MASi pairs in terms of the image-based MSE-MIND metric. Generally, MSE-MIND ranks the methods similarly to Dice overlap, as does NCC across the T1w registration pairs ((<>)Fig. 8a).",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      340.1501007080078,
      191.83380126953125,
      391.6051330566406,
      206.31210327148438
    ],
    "caption": "Even though affine SynthMorph does not directly optimize image similarity at training, it surpasses NiftyReg for  and MASi pairs in terms of the image-based MSE-MIND metric. Generally, MSE-MIND ranks the methods similarly to Dice overlap, as does NCC across the T1w registration pairs ((<>)Fig. 8a).",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      461.1925048828125,
      128.83360290527344,
      530.3008422851562,
      142.93710327148438
    ],
    "caption": "(<>)Figure 7a shows how registration accuracy evolves with increasing moving-image slice thickness Δz. Synth-Morph and ANTs remain the most robust for Δz ≤ 6 mm, reducing only to 99% at  For",
    "file_name": [
      "figures/fileoutpart85.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_237.png"
  },
  {
    "page": 11,
    "bbox": [
      497.91119384765625,
      133.81610107421875,
      507.3345031738281,
      142.93710327148438
    ],
    "caption": "(<>)Figure 7a shows how registration accuracy evolves with increasing moving-image slice thickness Δz. Synth-Morph and ANTs remain the most robust for Δz ≤ 6 mm, reducing only to 99% at  For",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      461.1925048828125,
      128.83360290527344,
      530.3008422851562,
      142.93710327148438
    ],
    "caption": "(<>)Figure 7a shows how registration accuracy evolves with increasing moving-image slice thickness Δz. Synth-Morph and ANTs remain the most robust for Δz ≤ 6 mm, reducing only to 99% at  For",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      416.9678955078125,
      50.43359375,
      468.88233947753906,
      62.812103271484375
    ],
    "caption": "(<>)Figure 9 shows that SynthMorph’s affine transforms across are more symmetric than all baselines tested. When we reverse the order of the input images, the mean inconsistency between forward and backward transforms is I = 5 × 10−5 mm per brain voxel, closely followed by NiftyReg. Robust also uses an inverse- consistent algorithm, leading to  The remaining baselines are substantially less symmetric, with inconsistencies of I = 2 mm for KeyMorph or more.",
    "file_name": [
      "figures/fileoutpart86.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_240.png"
  },
  {
    "page": 11,
    "bbox": [
      416.9678955078125,
      50.43359375,
      468.88233947753906,
      62.812103271484375
    ],
    "caption": "(<>)Figure 9 shows that SynthMorph’s affine transforms across are more symmetric than all baselines tested. When we reverse the order of the input images, the mean inconsistency between forward and backward transforms is I = 5 × 10−5 mm per brain voxel, closely followed by NiftyReg. Robust also uses an inverse- consistent algorithm, leading to  The remaining baselines are substantially less symmetric, with inconsistencies of I = 2 mm for KeyMorph or more.",
    "file_name": ""
  },
  {
    "page": 11,
    "bbox": [
      488.82029724121094,
      49.02754211425781,
      554.6381378173828,
      65.49095153808594
    ],
    "caption": "(<>)Figure 9 shows that SynthMorph’s affine transforms across are more symmetric than all baselines tested. When we reverse the order of the input images, the mean inconsistency between forward and backward transforms is I = 5 × 10−5 mm per brain voxel, closely followed by NiftyReg. Robust also uses an inverse- consistent algorithm, leading to  The remaining baselines are substantially less symmetric, with inconsistencies of I = 2 mm for KeyMorph or more.",
    "file_name": [
      "figures/fileoutpart87.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_242.png"
  },
  {
    "page": 11,
    "bbox": [
      488.82029724121094,
      49.02754211425781,
      554.6381378173828,
      65.49095153808594
    ],
    "caption": "(<>)Figure 9 shows that SynthMorph’s affine transforms across are more symmetric than all baselines tested. When we reverse the order of the input images, the mean inconsistency between forward and backward transforms is I = 5 × 10−5 mm per brain voxel, closely followed by NiftyReg. Robust also uses an inverse- consistent algorithm, leading to  The remaining baselines are substantially less symmetric, with inconsistencies of I = 2 mm for KeyMorph or more.",
    "file_name": ""
  },
  {
    "page": 12,
    "bbox": [
      61.94999694824219,
      222.3900146484375,
      298.34999084472656,
      735.75
    ],
    "caption": "Fig. 6. Representative affine 3D registration examples showing the image moved by each method overlaid with the fixed brain mask (red). Each row is an example from a different dataset. Subscripts indicate MRI contrast.",
    "file_name": [
      "figures/fileoutpart88.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_244.png"
  },
  {
    "page": 12,
    "bbox": [
      462.5854949951172,
      698.9338073730469,
      514.8358306884766,
      713.4120941162109
    ],
    "caption": "(<>)Table 2 lists the registration time required by each affine method on a 2.2-GHz Intel Xeon Silver 4114 CPU using a single computational thread. The values shown reflect averages over n = 10 uni-modal runs. Classical runtimes range between 2 and 27 minutes, with Deeds being the fastest and Robust being the slowest, although we highlight that we substantially increased the number of Robust iterations. Complete single-threaded DL run-times are about 1 minute, including model setup. However, inference only takes a few seconds and reduces to well under a second on an NVIDIA V100 GPU.",
    "file_name": [
      "figures/fileoutpart89.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_245.png"
  },
  {
    "page": 12,
    "bbox": [
      508.3948059082031,
      698.9338073730469,
      514.8358306884766,
      705.5800018310547
    ],
    "caption": "(<>)Table 2 lists the registration time required by each affine method on a 2.2-GHz Intel Xeon Silver 4114 CPU using a single computational thread. The values shown reflect averages over n = 10 uni-modal runs. Classical runtimes range between 2 and 27 minutes, with Deeds being the fastest and Robust being the slowest, although we highlight that we substantially increased the number of Robust iterations. Complete single-threaded DL run-times are about 1 minute, including model setup. However, inference only takes a few seconds and reduces to well under a second on an NVIDIA V100 GPU.",
    "file_name": ""
  },
  {
    "page": 12,
    "bbox": [
      462.5854949951172,
      698.9338073730469,
      514.8358306884766,
      713.4120941162109
    ],
    "caption": "(<>)Table 2 lists the registration time required by each affine method on a 2.2-GHz Intel Xeon Silver 4114 CPU using a single computational thread. The values shown reflect averages over n = 10 uni-modal runs. Classical runtimes range between 2 and 27 minutes, with Deeds being the fastest and Robust being the slowest, although we highlight that we substantially increased the number of Robust iterations. Complete single-threaded DL run-times are about 1 minute, including model setup. However, inference only takes a few seconds and reduces to well under a second on an NVIDIA V100 GPU.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      126.0,
      605.3200073242188,
      485.99998474121094,
      736.0
    ],
    "caption": "Fig. 7. Dependency of 3D (a) affine and (b) deformable registration accuracy on slice thickness. For comparability, we initialize all deformable tools with affine transforms estimated by NiftyReg. Each value indicates the mean over 100 skull- stripped pairs. Higher is better. Shaded areas indicate the standard error of the mean.",
    "file_name": [
      "figures/fileoutpart90.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_248.png"
  },
  {
    "page": 13,
    "bbox": [
      126.0,
      411.33995056152344,
      485.99998474121094,
      543.8199462890625
    ],
    "caption": "Fig. 8. Within-contrast 3D (a) affine and (b) deformable registration accuracy across skull-stripped cross-subject pairs in terms of brain-only NCC. We initialize all deformable tools with affine transforms estimated by NiftyReg. The asterisk indicates SynthMorph without skull-stripping. Higher is better.",
    "file_name": [
      "figures/fileoutpart91.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_249.png"
  },
  {
    "page": 13,
    "bbox": [
      61.80000305175781,
      125.43998718261719,
      298.1999969482422,
      346.8399963378906
    ],
    "caption": "Fig. 9. Forward-backward inconsistency between transforms when reversing the order of input images. We compare the mean displacement per brain voxel upon subsequent application of both transforms. Lower values are better.",
    "file_name": [
      "figures/fileoutpart92.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_250.png"
  },
  {
    "page": 13,
    "bbox": [
      313.79998779296875,
      125.67999267578125,
      550.1999816894531,
      346.8399963378906
    ],
    "caption": "Fig. 10. Relative reduction in brain-specific accuracy when registering full-head as opposed to skull-stripped images. Lower values are better. Although affine Deeds is the only method whose Dice overlap increases, it ranks as the least accurate on the testset. Error bars show the standard error of the mean.",
    "file_name": [
      "figures/fileoutpart93.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_251.png"
  },
  {
    "page": 13,
    "bbox": [
      422.0957946777344,
      60.02740478515625,
      469.7715759277344,
      73.77000427246094
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart94.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_252.png"
  },
  {
    "page": 13,
    "bbox": [
      463.7236022949219,
      60.02740478515625,
      469.7715759277344,
      66.32380676269531
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      422.0957946777344,
      60.02740478515625,
      469.7715759277344,
      73.77000427246094
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      179.67599487304688,
      620.9338073730469,
      229.0612335205078,
      635.4120941162109
    ],
    "caption": "(<>)Figure 11 shows typical deformable registration examples for each method, and (<>)Figure 12 compares registration accuracy across testsets in terms of mean Dice overlap D over the 21 largest anatomical structures (large-21), 10 fine-grained structures (small-10) not optimized at training, and image similarity measured with MSE-MIND. Supplementary Figures S1–S5 show deformable registration accuracy across individual brain structures.",
    "file_name": [
      "figures/fileoutpart95.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_255.png"
  },
  {
    "page": 14,
    "bbox": [
      221.0355987548828,
      620.9338073730469,
      229.0612335205078,
      627.5800018310547
    ],
    "caption": "(<>)Figure 11 shows typical deformable registration examples for each method, and (<>)Figure 12 compares registration accuracy across testsets in terms of mean Dice overlap D over the 21 largest anatomical structures (large-21), 10 fine-grained structures (small-10) not optimized at training, and image similarity measured with MSE-MIND. Supplementary Figures S1–S5 show deformable registration accuracy across individual brain structures.",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      179.67599487304688,
      620.9338073730469,
      229.0612335205078,
      635.4120941162109
    ],
    "caption": "(<>)Figure 11 shows typical deformable registration examples for each method, and (<>)Figure 12 compares registration accuracy across testsets in terms of mean Dice overlap D over the 21 largest anatomical structures (large-21), 10 fine-grained structures (small-10) not optimized at training, and image similarity measured with MSE-MIND. Supplementary Figures S1–S5 show deformable registration accuracy across individual brain structures.",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      59.98719787597656,
      349.43359375,
      172.5626983642578,
      363.53709411621094
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart96.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_258.png"
  },
  {
    "page": 14,
    "bbox": [
      161.1082000732422,
      354.4161071777344,
      172.5626983642578,
      363.53709411621094
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      59.98719787597656,
      349.43359375,
      172.5626983642578,
      363.53709411621094
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      59.90629577636719,
      230.93380737304688,
      90.41558837890625,
      245.81210327148438
    ],
    "caption": "Across the T1w testsets, FNIRT outperforms NiftyReg by several Dice points and also ANTs for MASi → HCP-D pairs. Surprisingly, FNIRT beats NiftyReg’s NMI implementation for even though FNIRT’s cost function targets within-contrast registration. The most robust baseline is Deeds, which ranks third at adult T1w registration. Its performance reduces the least for the cross-contrast and clinical testsets, where it achieves the highest Dice overlap after SynthMorph.",
    "file_name": [
      "figures/fileoutpart97.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_261.png"
  },
  {
    "page": 14,
    "bbox": [
      81.02020263671875,
      230.93380737304688,
      87.63221740722656,
      237.5800018310547
    ],
    "caption": "Across the T1w testsets, FNIRT outperforms NiftyReg by several Dice points and also ANTs for MASi → HCP-D pairs. Surprisingly, FNIRT beats NiftyReg’s NMI implementation for even though FNIRT’s cost function targets within-contrast registration. The most robust baseline is Deeds, which ranks third at adult T1w registration. Its performance reduces the least for the cross-contrast and clinical testsets, where it achieves the highest Dice overlap after SynthMorph.",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      59.90629577636719,
      230.93380737304688,
      90.41558837890625,
      245.81210327148438
    ],
    "caption": "Across the T1w testsets, FNIRT outperforms NiftyReg by several Dice points and also ANTs for MASi → HCP-D pairs. Surprisingly, FNIRT beats NiftyReg’s NMI implementation for even though FNIRT’s cost function targets within-contrast registration. The most robust baseline is Deeds, which ranks third at adult T1w registration. Its performance reduces the least for the cross-contrast and clinical testsets, where it achieves the highest Dice overlap after SynthMorph.",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      60.002105712890625,
      206.63259887695312,
      300.00975036621094,
      232.41209411621094
    ],
    "caption": "Across the T1w testsets, FNIRT outperforms NiftyReg by several Dice points and also ANTs for MASi → HCP-D pairs. Surprisingly, FNIRT beats NiftyReg’s NMI implementation for even though FNIRT’s cost function targets within-contrast registration. The most robust baseline is Deeds, which ranks third at adult T1w registration. Its performance reduces the least for the cross-contrast and clinical testsets, where it achieves the highest Dice overlap after SynthMorph.",
    "file_name": [
      "figures/fileoutpart98.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_264.png"
  },
  {
    "page": 14,
    "bbox": [
      154.09010314941406,
      217.93380737304688,
      160.70211791992188,
      224.5800018310547
    ],
    "caption": "Across the T1w testsets, FNIRT outperforms NiftyReg by several Dice points and also ANTs for MASi → HCP-D pairs. Surprisingly, FNIRT beats NiftyReg’s NMI implementation for even though FNIRT’s cost function targets within-contrast registration. The most robust baseline is Deeds, which ranks third at adult T1w registration. Its performance reduces the least for the cross-contrast and clinical testsets, where it achieves the highest Dice overlap after SynthMorph.",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      238.6239013671875,
      211.61610412597656,
      250.07839965820312,
      220.7371063232422
    ],
    "caption": "Across the T1w testsets, FNIRT outperforms NiftyReg by several Dice points and also ANTs for MASi → HCP-D pairs. Surprisingly, FNIRT beats NiftyReg’s NMI implementation for even though FNIRT’s cost function targets within-contrast registration. The most robust baseline is Deeds, which ranks third at adult T1w registration. Its performance reduces the least for the cross-contrast and clinical testsets, where it achieves the highest Dice overlap after SynthMorph.",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      60.002105712890625,
      206.63259887695312,
      300.00975036621094,
      232.41209411621094
    ],
    "caption": "Across the T1w testsets, FNIRT outperforms NiftyReg by several Dice points and also ANTs for MASi → HCP-D pairs. Surprisingly, FNIRT beats NiftyReg’s NMI implementation for even though FNIRT’s cost function targets within-contrast registration. The most robust baseline is Deeds, which ranks third at adult T1w registration. Its performance reduces the least for the cross-contrast and clinical testsets, where it achieves the highest Dice overlap after SynthMorph.",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      119.5574951171875,
      152.93380737304688,
      175.6291961669922,
      167.41209411621094
    ],
    "caption": "The joint DL baseline VTN yields relatively low accuracy across all testsets. This was expected for the cross- contrast pairings, since the model was trained with T1w",
    "file_name": [
      "figures/fileoutpart99.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_268.png"
  },
  {
    "page": 14,
    "bbox": [
      166.54730224609375,
      152.93380737304688,
      172.9883270263672,
      159.5800018310547
    ],
    "caption": "The joint DL baseline VTN yields relatively low accuracy across all testsets. This was expected for the cross- contrast pairings, since the model was trained with T1w",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      119.5574951171875,
      152.93380737304688,
      175.6291961669922,
      167.41209411621094
    ],
    "caption": "The joint DL baseline VTN yields relatively low accuracy across all testsets. This was expected for the cross- contrast pairings, since the model was trained with T1w",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      318.9119873046875,
      114.52175903320312,
      545.0879821777344,
      737.219970703125
    ],
    "caption": "Fig. 11. Deformable 3D registration examples comparing the moved image m! φ and the deformation field φ across methods. Each row is an example from a different dataset. For comparability, we initialize all methods with NiftyReg’s affine registration.",
    "file_name": [
      "figures/fileoutpart100.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_271.png"
  },
  {
    "page": 15,
    "bbox": [
      74.75,
      291.0400085449219,
      536.7499847412109,
      736.0
    ],
    "caption": "Fig. 12. Deformable 3D registration accuracy as mean Dice scores over the 21 largest brain regions (large-21), 10 fine- grained structures not optimized at SynthMorph training (small-10), and image similarity. Each violin shows the distribution across the skull-stripped cross-subject pairs from (<>)Table 1. For comparability, we initialize all deformable tools with affine transforms estimated by NiftyReg. The asterisk indicates SynthMorph performance without skull-stripping. Downward arrows show median scores outside the plotted range. Higher Dice and lower MSE-MIND are better.",
    "file_name": [
      "figures/fileoutpart101.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_272.png"
  },
  {
    "page": 15,
    "bbox": [
      202.5189971923828,
      166.08380126953125,
      254.11892700195312,
      180.56210327148438
    ],
    "caption": "Considering the fine-grained small-10 brain structures held out at training, SynthMorph consistently matches or exceeds the best performing method, except for MASi →HCP-D, where Deeds leads by On the clinical testset, SynthMorph leads by at least ΔD > 4.5 Interestingly, SynthMorph outperforms all baselines across testsets in terms of MSE- and",
    "file_name": [
      "figures/fileoutpart102.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_273.png"
  },
  {
    "page": 15,
    "bbox": [
      247.6779022216797,
      166.08380126953125,
      254.11892700195312,
      172.72999572753906
    ],
    "caption": "Considering the fine-grained small-10 brain structures held out at training, SynthMorph consistently matches or exceeds the best performing method, except for MASi →HCP-D, where Deeds leads by On the clinical testset, SynthMorph leads by at least ΔD > 4.5 Interestingly, SynthMorph outperforms all baselines across testsets in terms of MSE- and",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      202.5189971923828,
      166.08380126953125,
      254.11892700195312,
      180.56210327148438
    ],
    "caption": "Considering the fine-grained small-10 brain structures held out at training, SynthMorph consistently matches or exceeds the best performing method, except for MASi →HCP-D, where Deeds leads by On the clinical testset, SynthMorph leads by at least ΔD > 4.5 Interestingly, SynthMorph outperforms all baselines across testsets in terms of MSE- and",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      105.22529602050781,
      154.68359375,
      207.20481872558594,
      168.88710021972656
    ],
    "caption": "Considering the fine-grained small-10 brain structures held out at training, SynthMorph consistently matches or exceeds the best performing method, except for MASi →HCP-D, where Deeds leads by On the clinical testset, SynthMorph leads by at least ΔD > 4.5 Interestingly, SynthMorph outperforms all baselines across testsets in terms of MSE- and",
    "file_name": [
      "figures/fileoutpart103.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_276.png"
  },
  {
    "page": 15,
    "bbox": [
      193.4705047607422,
      159.76609802246094,
      202.8925018310547,
      168.88710021972656
    ],
    "caption": "Considering the fine-grained small-10 brain structures held out at training, SynthMorph consistently matches or exceeds the best performing method, except for MASi →HCP-D, where Deeds leads by On the clinical testset, SynthMorph leads by at least ΔD > 4.5 Interestingly, SynthMorph outperforms all baselines across testsets in terms of MSE- and",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      105.22529602050781,
      154.68359375,
      207.20481872558594,
      168.88710021972656
    ],
    "caption": "Considering the fine-grained small-10 brain structures held out at training, SynthMorph consistently matches or exceeds the best performing method, except for MASi →HCP-D, where Deeds leads by On the clinical testset, SynthMorph leads by at least ΔD > 4.5 Interestingly, SynthMorph outperforms all baselines across testsets in terms of MSE- and",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      203.66140747070312,
      89.28460693359375,
      287.59844970703125,
      103.38710021972656
    ],
    "caption": "−4",
    "file_name": [
      "figures/fileoutpart104.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_279.png"
  },
  {
    "page": 15,
    "bbox": [
      271.1730041503906,
      94.26609802246094,
      278.8450012207031,
      103.38710021972656
    ],
    "caption": "−4",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      203.66140747070312,
      89.28460693359375,
      287.59844970703125,
      103.38710021972656
    ],
    "caption": "−4",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      60.001495361328125,
      63.583099365234375,
      104.5616455078125,
      77.68710327148438
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart105.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_282.png"
  },
  {
    "page": 15,
    "bbox": [
      88.98199462890625,
      68.56610107421875,
      98.40400695800781,
      77.68710327148438
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      60.001495361328125,
      63.583099365234375,
      104.5616455078125,
      77.68710327148438
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      216.24349975585938,
      50.583099365234375,
      281.72967529296875,
      64.68710327148438
    ],
    "caption": "ΔD = 0.6 (p = 10).",
    "file_name": [
      "figures/fileoutpart106.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_285.png"
  },
  {
    "page": 15,
    "bbox": [
      271.32569885253906,
      55.56610107421875,
      278.99769592285156,
      64.68710327148438
    ],
    "caption": "ΔD = 0.6 (p = 10).",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      216.24349975585938,
      50.583099365234375,
      281.72967529296875,
      64.68710327148438
    ],
    "caption": "ΔD = 0.6 (p = 10).",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      494.4855041503906,
      193.4665069580078,
      554.4513854980469,
      207.57000732421875
    ],
    "caption": "(<>)Figure 13 shows the relative change in large-21 Dice for each tool when run end-to-end compared to affine initialization with NiftyReg. SynthMorph’s drop in performance is 0.05% or less across all datasets. For GSP  classical-baseline accuracy decreases by no more than 0.3%. Across the other datasets, the classical methods generally cannot make up for the discrepancy between their own and NiftyReg’s affine transform: accuracy drops by up to 5.2%, whereas SynthMorph remains",
    "file_name": [
      "figures/fileoutpart107.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_288.png"
  },
  {
    "page": 15,
    "bbox": [
      539.2449035644531,
      198.44900512695312,
      546.9169006347656,
      207.57000732421875
    ],
    "caption": "(<>)Figure 13 shows the relative change in large-21 Dice for each tool when run end-to-end compared to affine initialization with NiftyReg. SynthMorph’s drop in performance is 0.05% or less across all datasets. For GSP  classical-baseline accuracy decreases by no more than 0.3%. Across the other datasets, the classical methods generally cannot make up for the discrepancy between their own and NiftyReg’s affine transform: accuracy drops by up to 5.2%, whereas SynthMorph remains",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      494.4855041503906,
      193.4665069580078,
      554.4513854980469,
      207.57000732421875
    ],
    "caption": "(<>)Figure 13 shows the relative change in large-21 Dice for each tool when run end-to-end compared to affine initialization with NiftyReg. SynthMorph’s drop in performance is 0.05% or less across all datasets. For GSP  classical-baseline accuracy decreases by no more than 0.3%. Across the other datasets, the classical methods generally cannot make up for the discrepancy between their own and NiftyReg’s affine transform: accuracy drops by up to 5.2%, whereas SynthMorph remains",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      311.7812042236328,
      100.66670227050781,
      341.9320983886719,
      115.34500122070312
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart108.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_291.png"
  },
  {
    "page": 15,
    "bbox": [
      332.85020446777344,
      100.66670227050781,
      339.2912292480469,
      107.31289672851562
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 15,
    "bbox": [
      311.7812042236328,
      100.66670227050781,
      341.9320983886719,
      115.34500122070312
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      75.14999389648438,
      587.3599395751953,
      537.1499786376953,
      735.7999267578125
    ],
    "caption": "Fig. 13. Mean decrease in Dice scores for end-to-end joint registration relative to affine initialization with NiftyReg. Except for adult T1w registration pairs, the classical tools in blue generally cannot compensate for the discrepancy between their own and NiftyReg’s affine transform, indicating that inaccurate affine initialization can have a detrimental effect on subsequent deformable registration.",
    "file_name": [
      "figures/fileoutpart109.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_294.png"
  },
  {
    "page": 16,
    "bbox": [
      75.0,
      374.2799987792969,
      536.9999847412109,
      516.0
    ],
    "caption": "Fig. 14. Regularization analysis of SynthMorph registration accuracy, the proportion of folding voxels with a negative Jacobian determinant, and the spread of the distribution of absolute log-Jacobian determinants as a function of the regularization weight λ. The dots indicate maximum accuracy. For the other metrics, lower is better.",
    "file_name": [
      "figures/fileoutpart110.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_295.png"
  },
  {
    "page": 16,
    "bbox": [
      411.78900146484375,
      258.01649475097656,
      452.94520568847656,
      272.1199951171875
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart111.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_296.png"
  },
  {
    "page": 16,
    "bbox": [
      436.07020568847656,
      262.9989929199219,
      443.7434997558594,
      272.1199951171875
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 16,
    "bbox": [
      411.78900146484375,
      258.01649475097656,
      452.94520568847656,
      272.1199951171875
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 19,
    "bbox": [
      231.6977996826172,
      634.2160949707031,
      269.841552734375,
      648.6531066894531
    ],
    "caption": "(<>)et al., 2016; (<>)Singh et al., 2024; (<>)Tisdall et al., 2012; (<>)White (<>)et al., 2010). Although the SynthMorph utility includes a model for rigid alignment trained with scaling and shear ((<>)Appendix B) removed from matrix t! of (<>)Equation (7), the evaluation focuses on affine registration.",
    "file_name": [
      "figures/fileoutpart112.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_299.png"
  },
  {
    "page": 19,
    "bbox": [
      237.72909545898438,
      634.2160949707031,
      241.03309631347656,
      642.4411010742188
    ],
    "caption": "(<>)et al., 2016; (<>)Singh et al., 2024; (<>)Tisdall et al., 2012; (<>)White (<>)et al., 2010). Although the SynthMorph utility includes a model for rigid alignment trained with scaling and shear ((<>)Appendix B) removed from matrix t! of (<>)Equation (7), the evaluation focuses on affine registration.",
    "file_name": ""
  },
  {
    "page": 19,
    "bbox": [
      267.0415954589844,
      640.1761016845703,
      269.841552734375,
      648.6531066894531
    ],
    "caption": "(<>)et al., 2016; (<>)Singh et al., 2024; (<>)Tisdall et al., 2012; (<>)White (<>)et al., 2010). Although the SynthMorph utility includes a model for rigid alignment trained with scaling and shear ((<>)Appendix B) removed from matrix t! of (<>)Equation (7), the evaluation focuses on affine registration.",
    "file_name": ""
  },
  {
    "page": 19,
    "bbox": [
      231.6977996826172,
      634.2160949707031,
      269.841552734375,
      648.6531066894531
    ],
    "caption": "(<>)et al., 2016; (<>)Singh et al., 2024; (<>)Tisdall et al., 2012; (<>)White (<>)et al., 2010). Although the SynthMorph utility includes a model for rigid alignment trained with scaling and shear ((<>)Appendix B) removed from matrix t! of (<>)Equation (7), the evaluation focuses on affine registration.",
    "file_name": ""
  },
  {
    "page": 19,
    "bbox": [
      108.97149658203125,
      569.0162048339844,
      179.6149444580078,
      583.4532012939453
    ],
    "caption": "Fourth, we train SynthMorph as a general tool for cross-subject registration, and the evaluation on clinical data is limited to 50 glioblastoma patients.",
    "file_name": [
      "figures/fileoutpart113.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_303.png"
  },
  {
    "page": 19,
    "bbox": [
      111.72149658203125,
      569.0162048339844,
      115.02549743652344,
      577.2411956787109
    ],
    "caption": "Fourth, we train SynthMorph as a general tool for cross-subject registration, and the evaluation on clinical data is limited to 50 glioblastoma patients.",
    "file_name": ""
  },
  {
    "page": 19,
    "bbox": [
      146.90899658203125,
      574.9761962890625,
      151.45899963378906,
      583.4532012939453
    ],
    "caption": "Fourth, we train SynthMorph as a general tool for cross-subject registration, and the evaluation on clinical data is limited to 50 glioblastoma patients.",
    "file_name": ""
  },
  {
    "page": 19,
    "bbox": [
      170.53399658203125,
      574.9761962890625,
      175.08399963378906,
      583.4532012939453
    ],
    "caption": "Fourth, we train SynthMorph as a general tool for cross-subject registration, and the evaluation on clinical data is limited to 50 glioblastoma patients.",
    "file_name": ""
  },
  {
    "page": 19,
    "bbox": [
      108.97149658203125,
      569.0162048339844,
      179.6149444580078,
      583.4532012939453
    ],
    "caption": "Fourth, we train SynthMorph as a general tool for cross-subject registration, and the evaluation on clinical data is limited to 50 glioblastoma patients.",
    "file_name": ""
  },
  {
    "page": 19,
    "bbox": [
      260.48779296875,
      517.6161041259766,
      301.7565002441406,
      532.0531005859375
    ],
    "caption": "Fourth, we train SynthMorph as a general tool for cross-subject registration, and the evaluation on clinical data is limited to 50 glioblastoma patients.",
    "file_name": [
      "figures/fileoutpart114.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_308.png"
  },
  {
    "page": 19,
    "bbox": [
      266.5189971923828,
      517.6161041259766,
      269.822998046875,
      525.8410949707031
    ],
    "caption": "Fourth, we train SynthMorph as a general tool for cross-subject registration, and the evaluation on clinical data is limited to 50 glioblastoma patients.",
    "file_name": ""
  },
  {
    "page": 19,
    "bbox": [
      297.2064971923828,
      523.5760955810547,
      301.7565002441406,
      532.0531005859375
    ],
    "caption": "Fourth, we train SynthMorph as a general tool for cross-subject registration, and the evaluation on clinical data is limited to 50 glioblastoma patients.",
    "file_name": ""
  },
  {
    "page": 19,
    "bbox": [
      260.48779296875,
      517.6161041259766,
      301.7565002441406,
      532.0531005859375
    ],
    "caption": "Fourth, we train SynthMorph as a general tool for cross-subject registration, and the evaluation on clinical data is limited to 50 glioblastoma patients.",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      91.38630676269531,
      541.5008544921875,
      126.04722595214844,
      557.2215576171875
    ],
    "caption": "A.1.1. Parameter encoder",
    "file_name": [
      "figures/fileoutpart115.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_312.png"
  },
  {
    "page": 27,
    "bbox": [
      91.38630676269531,
      541.5008544921875,
      126.04722595214844,
      557.2215576171875
    ],
    "caption": "A.1.1. Parameter encoder",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      240.60150146484375,
      516.0644226074219,
      303.04100036621094,
      532.2802429199219
    ],
    "caption": "Ti = h1(m0,f ) h2(m1,f ) ! hi (mi−1,f )",
    "file_name": [
      "figures/fileoutpart116.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_314.png"
  },
  {
    "page": 27,
    "bbox": [
      240.60150146484375,
      516.0644226074219,
      303.04100036621094,
      532.2802429199219
    ],
    "caption": "Ti = h1(m0,f ) h2(m1,f ) ! hi (mi−1,f )",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      341.0332946777344,
      725.0989990234375,
      382.52479553222656,
      738.5950012207031
    ],
    "caption": "T= I",
    "file_name": [
      "figures/fileoutpart117.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_316.png"
  },
  {
    "page": 27,
    "bbox": [
      348.9082946777344,
      725.0989990234375,
      352.21229553222656,
      733.3240051269531
    ],
    "caption": "T= I",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      380.9707946777344,
      725.1269989013672,
      382.52479553222656,
      733.3240051269531
    ],
    "caption": "For balanced gradient steps, we complete each subnetwork with a layer applying a learnable rescaling weight to each affine parameter before matrix construction.",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      341.0332946777344,
      725.0989990234375,
      382.52479553222656,
      738.5950012207031
    ],
    "caption": "T= I",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      331.2541046142578,
      711.5989990234375,
      358.46429443359375,
      725.0950012207031
    ],
    "caption": "For balanced gradient steps, we complete each subnetwork with a layer applying a learnable rescaling weight to each affine parameter before matrix construction.",
    "file_name": [
      "figures/fileoutpart118.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_320.png"
  },
  {
    "page": 27,
    "bbox": [
      336.16029357910156,
      711.5989990234375,
      341.80230712890625,
      719.7890014648438
    ],
    "caption": "For balanced gradient steps, we complete each subnetwork with a layer applying a learnable rescaling weight to each affine parameter before matrix construction.",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      353.41029357910156,
      711.6269989013672,
      358.46429443359375,
      719.8240051269531
    ],
    "caption": "For balanced gradient steps, we complete each subnetwork with a layer applying a learnable rescaling weight to each affine parameter before matrix construction.",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      331.2541046142578,
      711.5989990234375,
      358.46429443359375,
      725.0950012207031
    ],
    "caption": "For balanced gradient steps, we complete each subnetwork with a layer applying a learnable rescaling weight to each affine parameter before matrix construction.",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      355.3361053466797,
      516.5740051269531,
      404.5501251220703,
      530.5950012207031
    ],
    "caption": "θ",
    "file_name": [
      "figures/fileoutpart119.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_324.png"
  },
  {
    "page": 27,
    "bbox": [
      360.24229431152344,
      516.5740051269531,
      365.63929748535156,
      525.6950073242188
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      379.30479431152344,
      516.5740051269531,
      382.95176696777344,
      525.6950073242188
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      355.3361053466797,
      516.5740051269531,
      404.5501251220703,
      530.5950012207031
    ],
    "caption": "θ",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      365.07350158691406,
      503.4739990234375,
      427.5583953857422,
      517.4949951171875
    ],
    "caption": "(φ,κ)",
    "file_name": [
      "figures/fileoutpart120.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_328.png"
  },
  {
    "page": 27,
    "bbox": [
      385.8022003173828,
      503.4739990234375,
      391.19920349121094,
      512.5950012207031
    ],
    "caption": "T= h(m,f )",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      404.95849609375,
      503.4739990234375,
      410.3554992675781,
      512.5950012207031
    ],
    "caption": "(φ,κ)",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      420.4272003173828,
      503.4739990234375,
      424.0741729736328,
      512.5950012207031
    ],
    "caption": "(φ,κ)",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      365.07350158691406,
      503.4739990234375,
      427.5583953857422,
      517.4949951171875
    ],
    "caption": "(φ,κ)",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      405.3273010253906,
      477.8739929199219,
      433.47532653808594,
      491.89500427246094
    ],
    "caption": "θ",
    "file_name": [
      "figures/fileoutpart121.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_333.png"
  },
  {
    "page": 27,
    "bbox": [
      413.0773010253906,
      477.8739929199219,
      416.7242736816406,
      486.9949951171875
    ],
    "caption": "is, φ= δ!T.",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      426.17100524902344,
      477.8739929199219,
      431.5679931640625,
      486.9949951171875
    ],
    "caption": "is, φ= δ!T.",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      405.3273010253906,
      477.8739929199219,
      433.47532653808594,
      491.89500427246094
    ],
    "caption": "θ",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      75.0,
      136.44000244140625,
      536.9999847412109,
      408.0
    ],
    "caption": "Appendix Fig. A1. Affine architectures. Detector outputs ReLU-activated feature maps for a single image. We compute their centers of mass (COM) and weights separately for m and f , to fit a transform T that aligns these point sets. A recurrent Encoder estimates refinements to the current transform Ti from moved image  and fixed image f .  Decomposer predicts a one-shot displacement field (no activation) with corresponding voxel weights (ReLU), that we decompose in a weighted least-squares (WLS) sense to estimate affine transform T . Parentheses specify filter numbers. We LeakyReLU-activate the output of unnamed convolutional blocks (param. α = 0.2). Stacked convolutional blocks of decreasing size indicate subsampling by a factor of 2 via max pooling following each activation.",
    "file_name": [
      "figures/fileoutpart122.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_337.png"
  },
  {
    "page": 27,
    "bbox": [
      411.5379943847656,
      93.26400756835938,
      451.1231994628906,
      106.1300048828125
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart123.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_338.png"
  },
  {
    "page": 27,
    "bbox": [
      419.00669860839844,
      93.26400756835938,
      422.3106994628906,
      101.48899841308594
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      449.56919860839844,
      93.29200744628906,
      451.1231994628906,
      101.48899841308594
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 27,
    "bbox": [
      411.5379943847656,
      93.26400756835938,
      451.1231994628906,
      106.1300048828125
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      93.3125,
      712.8114929199219,
      302.64300537109375,
      740.0225067138672
    ],
    "caption": "where t⊤ is the matrix transpose of t. Denoting W =  and by X and y the matrices whose corresponding rows are ( x⊤ 1) and  for each respectively, (<>)Equation (5) yields the closed-form WLS solution as in (<>)Section 3.3.1.",
    "file_name": [
      "figures/fileoutpart124.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_342.png"
  },
  {
    "page": 28,
    "bbox": [
      93.3125,
      712.8114929199219,
      302.64300537109375,
      740.0225067138672
    ],
    "caption": "where t⊤ is the matrix transpose of t. Denoting W =  and by X and y the matrices whose corresponding rows are ( x⊤ 1) and  for each respectively, (<>)Equation (5) yields the closed-form WLS solution as in (<>)Section 3.3.1.",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      60.225006103515625,
      678.6739959716797,
      96.54100036621094,
      692.6950073242188
    ],
    "caption": "A.1.3. Implementation and training",
    "file_name": [
      "figures/fileoutpart125.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_344.png"
  },
  {
    "page": 28,
    "bbox": [
      86.19369506835938,
      678.6739959716797,
      91.5906982421875,
      687.7949981689453
    ],
    "caption": "A.1.3. Implementation and training",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      60.225006103515625,
      678.6739959716797,
      96.54100036621094,
      692.6950073242188
    ],
    "caption": "A.1.3. Implementation and training",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      202.9884033203125,
      665.7740936279297,
      229.40310668945312,
      680.4911041259766
    ],
    "caption": "A.1.3. Implementation and training",
    "file_name": [
      "figures/fileoutpart126.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_347.png"
  },
  {
    "page": 28,
    "bbox": [
      207.9884033203125,
      665.7740936279297,
      211.6353759765625,
      674.8950958251953
    ],
    "caption": "A.1.3. Implementation and training",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      223.9571075439453,
      672.6790924072266,
      229.40310668945312,
      680.4911041259766
    ],
    "caption": "Encoder predicts rotation parameters in degrees. This parameterization ensures varying rotation angles has an effect of similar magnitude as translations in millimeters, at the scale of the brain, which helps networks converge faster in our experiments. We initialize the rescaling weights of Encoder to 1 for translations and rotations, and to 0.05 for scaling and shear, which we find favorable to faster convergence. (<>)Appendix B includes details.",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      202.9884033203125,
      665.7740936279297,
      229.40310668945312,
      680.4911041259766
    ],
    "caption": "A.1.3. Implementation and training",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      275.5617980957031,
      667.41650390625,
      302.6409912109375,
      679.7949981689453
    ],
    "caption": "Encoder predicts rotation parameters in degrees. This parameterization ensures varying rotation angles has an effect of similar magnitude as translations in millimeters, at the scale of the brain, which helps networks converge faster in our experiments. We initialize the rescaling weights of Encoder to 1 for translations and rotations, and to 0.05 for scaling and shear, which we find favorable to faster convergence. (<>)Appendix B includes details.",
    "file_name": [
      "figures/fileoutpart127.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_351.png"
  },
  {
    "page": 28,
    "bbox": [
      275.5617980957031,
      667.41650390625,
      302.6409912109375,
      679.7949981689453
    ],
    "caption": "Encoder predicts rotation parameters in degrees. This parameterization ensures varying rotation angles has an effect of similar magnitude as translations in millimeters, at the scale of the brain, which helps networks converge faster in our experiments. We initialize the rescaling weights of Encoder to 1 for translations and rotations, and to 0.05 for scaling and shear, which we find favorable to faster convergence. (<>)Appendix B includes details.",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      138.59080505371094,
      468.7740020751953,
      187.4429931640625,
      485.36314392089844
    ],
    "caption": "T= φ⊙ κ,",
    "file_name": [
      "figures/fileoutpart128.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_353.png"
  },
  {
    "page": 28,
    "bbox": [
      157.6844940185547,
      468.7740020751953,
      163.0814971923828,
      477.89500427246094
    ],
    "caption": "Encoder predicts rotation parameters in degrees. This parameterization ensures varying rotation angles has an effect of similar magnitude as translations in millimeters, at the scale of the brain, which helps networks converge faster in our experiments. We initialize the rescaling weights of Encoder to 1 for translations and rotations, and to 0.05 for scaling and shear, which we find favorable to faster convergence. (<>)Appendix B includes details.",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      138.59080505371094,
      468.7740020751953,
      187.4429931640625,
      485.36314392089844
    ],
    "caption": "T= φ⊙ κ,",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      96.06120300292969,
      404.1739959716797,
      146.97369384765625,
      418.19500732421875
    ],
    "caption": "A.2. Data",
    "file_name": [
      "figures/fileoutpart129.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_356.png"
  },
  {
    "page": 28,
    "bbox": [
      100.96749877929688,
      404.1739959716797,
      106.364501953125,
      413.2949981689453
    ],
    "caption": "A.2. Data",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      120.12370300292969,
      404.1739959716797,
      125.52070617675781,
      413.2949981689453
    ],
    "caption": "For architecture analysis, we use T1w images with isotropic 1-mm resolution from adult participants aged 40– 75 years from the UK Biobank (UKBB) study ((<>)Alfaro-Almagro (<>)et al., 2018; (<>)Miller et al., 2016; (<>)Sudlow et al., 2015). We conform images and derive label maps as in (<>)Section 4.1, extracting mid-sagittal slices from corresponding 3D images and label maps.",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      140.6862030029297,
      404.1739959716797,
      144.3331756591797,
      413.2949981689453
    ],
    "caption": "For architecture analysis, we use T1w images with isotropic 1-mm resolution from adult participants aged 40– 75 years from the UK Biobank (UKBB) study ((<>)Alfaro-Almagro (<>)et al., 2018; (<>)Miller et al., 2016; (<>)Sudlow et al., 2015). We conform images and derive label maps as in (<>)Section 4.1, extracting mid-sagittal slices from corresponding 3D images and label maps.",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      96.06120300292969,
      404.1739959716797,
      146.97369384765625,
      418.19500732421875
    ],
    "caption": "A.2. Data",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      184.63150024414062,
      88.747314453125,
      208.47645568847656,
      104.96315002441406
    ],
    "caption": "Assuming a network capacity of ~250 k learnable parameters, we explore the relative strengths and weaknesses of each affine architecture. We conduct the experiment in a 2D-registration context, which reduces the computational burden to consider numerous model configurations.",
    "file_name": [
      "figures/fileoutpart130.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_361.png"
  },
  {
    "page": 28,
    "bbox": [
      184.63150024414062,
      88.747314453125,
      208.47645568847656,
      104.96315002441406
    ],
    "caption": "Assuming a network capacity of ~250 k learnable parameters, we explore the relative strengths and weaknesses of each affine architecture. We conduct the experiment in a 2D-registration context, which reduces the computational burden to consider numerous model configurations.",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      442.15440368652344,
      387.0587615966797,
      516.4678649902344,
      402.7794494628906
    ],
    "caption": "Second, we compare Decomposer variants that fit T in an OLS sense, using weights  or in a WLS sense. For both, we assess the impact of the resolution ! of the field φθ relative to f on performance, by upsampling by a factor of 2 after each of the first resolution of ! = 1/ 4 corresponds to 25% of the resolution of f, that is, 25% of the original image dimensions.",
    "file_name": [
      "figures/fileoutpart133.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_363.png"
  },
  {
    "page": 28,
    "bbox": [
      442.15440368652344,
      387.0587615966797,
      516.4678649902344,
      402.7794494628906
    ],
    "caption": "Second, we compare Decomposer variants that fit T in an OLS sense, using weights  or in a WLS sense. For both, we assess the impact of the resolution ! of the field φθ relative to f on performance, by upsampling by a factor of 2 after each of the first resolution of ! = 1/ 4 corresponds to 25% of the resolution of f, that is, 25% of the original image dimensions.",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      445.2073974609375,
      347.6739959716797,
      518.6098937988281,
      361.69500732421875
    ],
    "caption": "n ∈ 0, 1, 2, 3{ } convolutional decoder blocks, using skip connections where possible, such that ! n( ) = 1/ 2. A",
    "file_name": [
      "figures/fileoutpart134.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_365.png"
  },
  {
    "page": 28,
    "bbox": [
      486.0697937011719,
      347.6739959716797,
      489.7167663574219,
      356.7949981689453
    ],
    "caption": "n ∈ 0, 1, 2, 3{ } convolutional decoder blocks, using skip connections where possible, such that ! n( ) = 1/ 2. A",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      445.2073974609375,
      347.6739959716797,
      518.6098937988281,
      361.69500732421875
    ],
    "caption": "n ∈ 0, 1, 2, 3{ } convolutional decoder blocks, using skip connections where possible, such that ! n( ) = 1/ 2. A",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      311.99949645996094,
      283.3837585449219,
      555.9310760498047,
      311.7794494628906
    ],
    "caption": "Third, we analyze OLS and WLS variants of Detector predicting k ∈{8, 16, 32, 64} feature maps to compute the corresponding (ai, pi ) and (bi,qi ) for",
    "file_name": [
      "figures/fileoutpart135.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_368.png"
  },
  {
    "page": 28,
    "bbox": [
      527.27099609375,
      289.3990020751953,
      539.6316986083984,
      298.52000427246094
    ],
    "caption": "Third, we analyze OLS and WLS variants of Detector predicting k ∈{8, 16, 32, 64} feature maps to compute the corresponding (ai, pi ) and (bi,qi ) for",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      311.99949645996094,
      283.3837585449219,
      555.9310760498047,
      311.7794494628906
    ],
    "caption": "Third, we analyze OLS and WLS variants of Detector predicting k ∈{8, 16, 32, 64} feature maps to compute the corresponding (ai, pi ) and (bi,qi ) for",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      482.60789489746094,
      218.45875549316406,
      544.2819976806641,
      234.179443359375
    ],
    "caption": "Finally, we select one configuration per architecture and analyze its performance across a range of transformation magnitudes. We investigate how models adapt to larger transforms, by fine-tuning trained weights to twice the affine augmentation amplitudes of (<>)Appendix Table A2 until convergence, and we repeat the experiment with doubled capacity. The test considers copies of the test set, applying random affine transforms of maximum strength  relative to the augmentation range of (<>)Appendix (<>)Table A2. For example, at a given γ , we uniformly sample a rotation angle r ∼ U (−γα,γα) for each of the 200 moving and fixed images, where  and similarly for the other 6 degrees of freedom (DOF, (<>)Appendix B).",
    "file_name": [
      "figures/fileoutpart136.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_371.png"
  },
  {
    "page": 28,
    "bbox": [
      482.60789489746094,
      218.45875549316406,
      544.2819976806641,
      234.179443359375
    ],
    "caption": "Finally, we select one configuration per architecture and analyze its performance across a range of transformation magnitudes. We investigate how models adapt to larger transforms, by fine-tuning trained weights to twice the affine augmentation amplitudes of (<>)Appendix Table A2 until convergence, and we repeat the experiment with doubled capacity. The test considers copies of the test set, applying random affine transforms of maximum strength  relative to the augmentation range of (<>)Appendix (<>)Table A2. For example, at a given γ , we uniformly sample a rotation angle r ∼ U (−γα,γα) for each of the 200 moving and fixed images, where  and similarly for the other 6 degrees of freedom (DOF, (<>)Appendix B).",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      312.5538024902344,
      100.61054992675781,
      348.96922302246094,
      117.07394409179688
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart137.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_373.png"
  },
  {
    "page": 28,
    "bbox": [
      312.5538024902344,
      100.61054992675781,
      348.96922302246094,
      117.07394409179688
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      414.38299560546875,
      63.21659851074219,
      446.35455322265625,
      76.53610229492188
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart138.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_375.png"
  },
  {
    "page": 28,
    "bbox": [
      440.75799560546875,
      68.05909729003906,
      443.5579528808594,
      76.53610229492188
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 28,
    "bbox": [
      414.38299560546875,
      63.21659851074219,
      446.35455322265625,
      76.53610229492188
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 29,
    "bbox": [
      243.1282958984375,
      575.9441070556641,
      276.8033447265625,
      589.2120971679688
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart139.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_378.png"
  },
  {
    "page": 29,
    "bbox": [
      248.0657958984375,
      575.9441070556641,
      249.6197967529297,
      584.1410980224609
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 29,
    "bbox": [
      243.1282958984375,
      575.9441070556641,
      276.8033447265625,
      589.2120971679688
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 29,
    "bbox": [
      126.0,
      293.010009765625,
      485.99998474121094,
      389.25
    ],
    "caption": "Appendix Fig. A2. Network analysis. We assess Encoder with different numbers of subnetworks C. We also analyze Decomposer and Detector variants using ordinary (OLS) or weighted least squares (WLS), varying the warp resolution ! and number of output feature maps k, respectively. A value of  for example, means the warp φθ has resolution and dimensions of only 25% compared to the input images. Dice scores are averages over 100 UKBB cross-subject 2D pairs. Shaded areas indicate the standard error of the mean.",
    "file_name": [
      "figures/fileoutpart140.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_381.png"
  },
  {
    "page": 29,
    "bbox": [
      310.0913543701172,
      250.7729949951172,
      341.3634948730469,
      264.5113067626953
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart141.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_382.png"
  },
  {
    "page": 29,
    "bbox": [
      316.5043029785156,
      255.3903045654297,
      325.9263000488281,
      264.5113067626953
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 29,
    "bbox": [
      310.0913543701172,
      250.7729949951172,
      341.3634948730469,
      264.5113067626953
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 29,
    "bbox": [
      126.0,
      114.4000244140625,
      485.99998474121094,
      209.80001831054688
    ],
    "caption": "Appendix Fig. A3. Network robustness across affine transform strengths γ relative to the range of (<>)Appendix Table A2. At a given γ, we resample each image, drawing affine parameters from uniform distributions modulated by γ, such as angle  with  We test models trained with doubled augmentation (aug) and capacity (cap), comparing C = 4 Encoders sharing weights, the WLS Decomposer without upsampling, and a k = 32 Detector. Dice scores are averages over 100 UKBB cross-subject 2D pairs. Shaded areas indicate the standard error of the mean.",
    "file_name": [
      "figures/fileoutpart142.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_385.png"
  },
  {
    "page": 29,
    "bbox": [
      60.98750305175781,
      70.47288513183594,
      115.98023986816406,
      87.24249267578125
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart143.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_386.png"
  },
  {
    "page": 29,
    "bbox": [
      60.98750305175781,
      70.47288513183594,
      115.98023986816406,
      87.24249267578125
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 29,
    "bbox": [
      138.61489868164062,
      72.46299743652344,
      168.70199584960938,
      85.41729736328125
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart144.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_388.png"
  },
  {
    "page": 29,
    "bbox": [
      163.64610290527344,
      76.9403076171875,
      168.19610595703125,
      85.41729736328125
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 29,
    "bbox": [
      138.61489868164062,
      72.46299743652344,
      168.70199584960938,
      85.41729736328125
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      60.887603759765625,
      179.37574768066406,
      108.60556030273438,
      195.09645080566406
    ],
    "caption": "Ω={−Δdi, 1− Δdi, …,di −1− Δdi } i=1 N ∏",
    "file_name": [
      "figures/fileoutpart145.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_391.png"
  },
  {
    "page": 30,
    "bbox": [
      60.887603759765625,
      179.37574768066406,
      108.60556030273438,
      195.09645080566406
    ],
    "caption": "Ω={−Δdi, 1− Δdi, …,di −1− Δdi } i=1 N ∏",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      103.72410583496094,
      112.41610717773438,
      258.7010955810547,
      143.57310485839844
    ],
    "caption": "Δd= ( d− 1) / 2 ,",
    "file_name": [
      "figures/fileoutpart146.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_393.png"
  },
  {
    "page": 30,
    "bbox": [
      103.72410583496094,
      112.41610717773438,
      258.7010955810547,
      143.57310485839844
    ],
    "caption": "Δd= ( d− 1) / 2 ,",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      81.01350402832031,
      90.81610107421875,
      149.19772338867188,
      104.31210327148438
    ],
    "caption": "Ω={−Δdi, 1− Δdi, …,di −1− Δdi } i=1 N ∏",
    "file_name": [
      "figures/fileoutpart147.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_395.png"
  },
  {
    "page": 30,
    "bbox": [
      92.63850402832031,
      90.81610107421875,
      95.9425048828125,
      99.04110717773438
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      113.88850402832031,
      90.81610107421875,
      117.1925048828125,
      99.04110717773438
    ],
    "caption": "Ω={−Δdi, 1− Δdi, …,di −1− Δdi } i=1 N ∏",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      81.01350402832031,
      90.81610107421875,
      149.19772338867188,
      104.31210327148438
    ],
    "caption": "Ω={−Δdi, 1− Δdi, …,di −1− Δdi } i=1 N ∏",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      383.3710021972656,
      474.4060974121094,
      554.6421966552734,
      642.6096038818359
    ],
    "caption": "For rotations and shear, we distinguish between the 2D and 3D case. Let ri be the angle of rotation about axis i, where the direction of rotation follows the right-hand rule. We abbreviate  and",
    "file_name": [
      "figures/fileoutpart148.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_399.png"
  },
  {
    "page": 30,
    "bbox": [
      383.3710021972656,
      474.4060974121094,
      554.6421966552734,
      642.6096038818359
    ],
    "caption": "For rotations and shear, we distinguish between the 2D and 3D case. Let ri be the angle of rotation about axis i, where the direction of rotation follows the right-hand rule. We abbreviate  and",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      396.2476043701172,
      405.2489929199219,
      441.3019256591797,
      418.7449951171875
    ],
    "caption": "B.1. Two-dimensional case",
    "file_name": [
      "figures/fileoutpart149.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_401.png"
  },
  {
    "page": 30,
    "bbox": [
      401.3726043701172,
      405.2489929199219,
      404.6766052246094,
      413.4739990234375
    ],
    "caption": "B.1. Two-dimensional case",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      435.7476043701172,
      405.2489929199219,
      439.0516052246094,
      413.4739990234375
    ],
    "caption": "In 2D, we apply the rotation angle  and shear e using matrices",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      396.2476043701172,
      405.2489929199219,
      441.3019256591797,
      418.7449951171875
    ],
    "caption": "B.1. Two-dimensional case",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      463.5975036621094,
      405.1490020751953,
      508.53199768066406,
      418.64500427246094
    ],
    "caption": "r = r",
    "file_name": [
      "figures/fileoutpart150.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_405.png"
  },
  {
    "page": 30,
    "bbox": [
      468.1286926269531,
      405.1490020751953,
      471.4326934814453,
      413.3739929199219
    ],
    "caption": "r = r",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      497.5036926269531,
      405.1490020751953,
      500.8076934814453,
      413.3739929199219
    ],
    "caption": "In 2D, we apply the rotation angle  and shear e using matrices",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      463.5975036621094,
      405.1490020751953,
      508.53199768066406,
      418.64500427246094
    ],
    "caption": "r = r",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      454.3488006591797,
      353.27699279785156,
      474.9282989501953,
      366.7449951171875
    ],
    "caption": "R= c3 −s3 0 s3 c3 0 0 0 1 ⎛ ⎝⎜⎜⎜ ⎞ ⎠⎟⎟⎟ and E = 1 e 0 0 1 0 0 0 1 ⎛ ⎝⎜⎜ ⎞ ⎠⎟⎟. (A7)",
    "file_name": [
      "figures/fileoutpart151.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_409.png"
  },
  {
    "page": 30,
    "bbox": [
      471.0363006591797,
      353.27699279785156,
      474.9282989501953,
      361.4389953613281
    ],
    "caption": "R= c3 −s3 0 s3 c3 0 0 0 1 ⎛ ⎝⎜⎜⎜ ⎞ ⎠⎟⎟⎟ and E = 1 e 0 0 1 0 0 0 1 ⎛ ⎝⎜⎜ ⎞ ⎠⎟⎟. (A7)",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      454.3488006591797,
      353.27699279785156,
      474.9282989501953,
      366.7449951171875
    ],
    "caption": "R= c3 −s3 0 s3 c3 0 0 0 1 ⎛ ⎝⎜⎜⎜ ⎞ ⎠⎟⎟⎟ and E = 1 e 0 0 1 0 0 0 1 ⎛ ⎝⎜⎜ ⎞ ⎠⎟⎟. (A7)",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      342.31080627441406,
      281.40809631347656,
      554.5989990234375,
      326.381103515625
    ],
    "caption": "B.2. Three-dimensional case",
    "file_name": [
      "figures/fileoutpart152.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_412.png"
  },
  {
    "page": 30,
    "bbox": [
      342.31080627441406,
      281.40809631347656,
      554.5989990234375,
      326.381103515625
    ],
    "caption": "B.2. Three-dimensional case",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      381.8159942626953,
      134.40660095214844,
      484.6311950683594,
      191.44459533691406
    ],
    "caption": "R2 = c2 0 s2 0 0 1 0 0 −s2 0 c2 0 0 0 0 1 ⎛ ⎝⎜⎜⎜⎜ ⎞ ⎠⎟⎟⎟⎟ ,",
    "file_name": [
      "figures/fileoutpart153.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_414.png"
  },
  {
    "page": 30,
    "bbox": [
      381.8159942626953,
      134.40660095214844,
      484.6311950683594,
      191.44459533691406
    ],
    "caption": "R2 = c2 0 s2 0 0 1 0 0 −s2 0 c2 0 0 0 0 1 ⎛ ⎝⎜⎜⎜⎜ ⎞ ⎠⎟⎟⎟⎟ ,",
    "file_name": ""
  },
  {
    "page": 30,
    "bbox": [
      379.8081970214844,
      56.40660095214844,
      487.0469970703125,
      113.44459533691406
    ],
    "caption": "R1 = 1 0 0 0 0 c1 −s1 0 0 s1 c1 0 0 0 0 1 ⎛ ⎝⎜⎜⎜⎜ ⎞ ⎠⎟⎟⎟⎟ ,",
    "file_name": [
      "figures/fileoutpart154.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_416.png"
  },
  {
    "page": 30,
    "bbox": [
      379.8081970214844,
      56.40660095214844,
      487.0469970703125,
      113.44459533691406
    ],
    "caption": "R1 = 1 0 0 0 0 c1 −s1 0 0 s1 c1 0 0 0 0 1 ⎛ ⎝⎜⎜⎜⎜ ⎞ ⎠⎟⎟⎟⎟ ,",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      127.81100463867188,
      678.9440002441406,
      235.0281982421875,
      736.22900390625
    ],
    "caption": "e{ }",
    "file_name": [
      "figures/fileoutpart155.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_418.png"
  },
  {
    "page": 31,
    "bbox": [
      127.81100463867188,
      678.9440002441406,
      235.0281982421875,
      736.22900390625
    ],
    "caption": "e{ }",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      165.63160705566406,
      647.2401885986328,
      185.068603515625,
      665.3128051757812
    ],
    "caption": "E= 1 e1 e2 0 0 1 e3 0 0 0 1 0 0 0 0 1 ⎛ ⎝⎜⎜⎜⎜ ⎞ ⎠⎟⎟⎟⎟ .",
    "file_name": [
      "figures/fileoutpart156.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_420.png"
  },
  {
    "page": 31,
    "bbox": [
      175.44410705566406,
      648.21240234375,
      176.99810791015625,
      656.4093933105469
    ],
    "caption": "E= 1 e1 e2 0 0 1 e3 0 0 0 1 0 0 0 0 1 ⎛ ⎝⎜⎜⎜⎜ ⎞ ⎠⎟⎟⎟⎟ .",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      165.63160705566406,
      647.2401885986328,
      185.068603515625,
      665.3128051757812
    ],
    "caption": "E= 1 e1 e2 0 0 1 e3 0 0 0 1 0 0 0 0 1 ⎛ ⎝⎜⎜⎜⎜ ⎞ ⎠⎟⎟⎟⎟ .",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      133.0028076171875,
      575.4440002441406,
      229.13800048828125,
      632.6054992675781
    ],
    "caption": "B.3. Transforming coordinates",
    "file_name": [
      "figures/fileoutpart157.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_423.png"
  },
  {
    "page": 31,
    "bbox": [
      133.0028076171875,
      575.4440002441406,
      229.13800048828125,
      632.6054992675781
    ],
    "caption": "B.3. Transforming coordinates",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      126.59800720214844,
      485.28700256347656,
      235.9866943359375,
      499.3489990234375
    ],
    "caption": "as x′ = Ax + v, or, using a single matrix product,",
    "file_name": [
      "figures/fileoutpart158.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_425.png"
  },
  {
    "page": 31,
    "bbox": [
      126.59800720214844,
      485.28700256347656,
      235.9866943359375,
      499.3489990234375
    ],
    "caption": "as x′ = Ax + v, or, using a single matrix product,",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      144.66969299316406,
      395.9414978027344,
      218.4394989013672,
      435.82249450683594
    ],
    "caption": "C. GENERATION HYPERPARAMETERS",
    "file_name": [
      "figures/fileoutpart159.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_427.png"
  },
  {
    "page": 31,
    "bbox": [
      144.66969299316406,
      395.9414978027344,
      218.4394989013672,
      435.82249450683594
    ],
    "caption": "C. GENERATION HYPERPARAMETERS",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      409.7445068359375,
      634.4758605957031,
      456.3320007324219,
      650.1965484619141
    ],
    "caption": "nslations and rotations are | r| = 5.6 ±( 6.0), respectivelyv= 8.5 ± 9.9(  (±) mm standard deviation,",
    "file_name": [
      "figures/fileoutpart160.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_429.png"
  },
  {
    "page": 31,
    "bbox": [
      409.7445068359375,
      634.4758605957031,
      456.3320007324219,
      650.1965484619141
    ],
    "caption": "nslations and rotations are | r| = 5.6 ±( 6.0), respectivelyv= 8.5 ± 9.9(  (±) mm standard deviation,",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      311.9996032714844,
      595.3161010742188,
      530.7508850097656,
      624.1215515136719
    ],
    "caption": "e= 3.3 ± 3.0( )%,",
    "file_name": [
      "figures/fileoutpart161.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_431.png"
  },
  {
    "page": 31,
    "bbox": [
      318.1875,
      595.3161010742188,
      321.4915008544922,
      603.5411071777344
    ],
    "caption": "| z− 1| ≤ 22.6%,",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      373.8437042236328,
      602.0260925292969,
      376.64366149902344,
      610.5030975341797
    ],
    "caption": "|v| ≤ 61.1 mm,",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      450.80169677734375,
      608.3161010742188,
      454.10569763183594,
      616.5411071777344
    ],
    "caption": "e= 3.3 ± 3.0( )%,",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      311.9996032714844,
      595.3161010742188,
      530.7508850097656,
      624.1215515136719
    ],
    "caption": "e= 3.3 ± 3.0( )%,",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      421.82598876953125,
      569.3161010742188,
      496.87139892578125,
      585.1215515136719
    ],
    "caption": "|r| ≤ 43.1,",
    "file_name": [
      "figures/fileoutpart162.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_436.png"
  },
  {
    "page": 31,
    "bbox": [
      428.9510040283203,
      569.3161010742188,
      432.2550048828125,
      577.5411071777344
    ],
    "caption": "|v| ≤ 61.1 mm,",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      421.82598876953125,
      569.3161010742188,
      496.87139892578125,
      585.1215515136719
    ],
    "caption": "|r| ≤ 43.1,",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      373.4378967285156,
      530.1161041259766,
      438.9966735839844,
      543.6121063232422
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": [
      "figures/fileoutpart163.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_439.png"
  },
  {
    "page": 31,
    "bbox": [
      381.2857971191406,
      530.1161041259766,
      384.5897979736328,
      538.3410949707031
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      373.4378967285156,
      530.1161041259766,
      438.9966735839844,
      543.6121063232422
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      475.4416046142578,
      530.3161010742188,
      522.0995330810547,
      544.7530975341797
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": [
      "figures/fileoutpart164.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_442.png"
  },
  {
    "page": 31,
    "bbox": [
      481.47279357910156,
      530.3161010742188,
      484.77679443359375,
      538.5411071777344
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      512.1602935791016,
      536.2760925292969,
      516.7102966308594,
      544.7530975341797
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      475.4416046142578,
      530.3161010742188,
      522.0995330810547,
      544.7530975341797
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      312.4062042236328,
      517.2660980224609,
      380.6374969482422,
      530.7621002197266
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": [
      "figures/fileoutpart165.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_446.png"
  },
  {
    "page": 31,
    "bbox": [
      321.8437042236328,
      517.2660980224609,
      325.147705078125,
      525.4911041259766
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      312.4062042236328,
      517.2660980224609,
      380.6374969482422,
      530.7621002197266
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      444.88470458984375,
      517.8161010742188,
      496.6318054199219,
      531.3121032714844
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": [
      "figures/fileoutpart166.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_449.png"
  },
  {
    "page": 31,
    "bbox": [
      452.91600036621094,
      517.8161010742188,
      456.2200012207031,
      526.0411071777344
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      444.88470458984375,
      517.8161010742188,
      496.6318054199219,
      531.3121032714844
    ],
    "caption": "Appendix Table A2. Uniform hyperparameter sampling ranges [a,b] for synthesizing training images from source segmentation maps.",
    "file_name": ""
  },
  {
    "page": 31,
    "bbox": [
      125.95001220703125,
      92.32002258300781,
      485.9499969482422,
      224.80001831054688
    ],
    "caption": "Appendix Fig. A4. Absolute affine transformation range across n = 1000 registration pairs randomly selected from either OASIS or ABCD. Each panel pools parameters relative to all axes  of 3D space. Black bars indicate median values. Circles represent parameters farther than 1.5 inter-quartile ranges from the median.",
    "file_name": [
      "figures/fileoutpart169.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_452.png"
  },
  {
    "page": 31,
    "bbox": [
      324.12669372558594,
      60.169647216796875,
      368.01365661621094,
      75.18020629882812
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart170.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_453.png"
  },
  {
    "page": 31,
    "bbox": [
      324.12669372558594,
      60.169647216796875,
      368.01365661621094,
      75.18020629882812
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 32,
    "bbox": [
      126.0,
      655.4500579833984,
      485.99998474121094,
      735.2500610351562
    ],
    "caption": "Appendix Fig. A5. Deformable registration accuracy at test across 100 T1-weighted cross-subject UKBB pairs after training with label-based versus image-based loss terms. Apart from the optimized similarity loss term, the trained models are identical. The dots indicate optimum accuracy: higher is better, except for MIND-MSE. Shaded areas indicate the standard error of the mean.",
    "file_name": [
      "figures/fileoutpart171.png"
    ],
    "output_file": "assets/images/paper/Anatomy-aware-and-acquisition-agnostic-joint-registration-with-SynthMorph/fig_455.png"
  }
]