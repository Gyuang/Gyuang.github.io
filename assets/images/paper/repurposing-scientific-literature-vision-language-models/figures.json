[
  {
    "page": 2,
    "bbox": [
      42.0,
      415.9537353515625,
      570.0,
      739.9537353515625
    ],
    "caption": "Fig. 1. Overview of key contributions. (A) We developed a pipeline for the acquisition, extraction, and filtering of figures, captions, and in-text mentions from a family of biomedical journals. We converted this data from unstructured biomedical texts and images into publication-, education-, and task-specific AI training datasets. (B) We trained CNS-Obsidian, a 34B parameter autoregressive vision-language model, to be tailored to domain-specific needs for neurosurgery by implementing a novel training step designed to specifically entrain capabilities in differential diagnosis while maintaining the ability to converse and answer questions. (C) We conducted a blinded, randomized, controlled trial comparing CNS Obsidian to GPT-4o as diagnostic copilots on a busy inpatient surgical service.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_01.png"
  },
  {
    "page": 4,
    "bbox": [
      34.09857177734375,
      407.0516052246094,
      577.9014282226562,
      744.1358947753906
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_02.png"
  },
  {
    "page": 5,
    "bbox": [
      111.0,
      198.45361328125,
      501.0,
      739.95361328125
    ],
    "caption": "Fig. 3. Visualization of concepts represented in scientific datasets. (A) A joint embedding of the PubMed-based and Neurosurgery-based instruction fine-tuning datasets. (B) The 15 largest hierarchical clusters of the joint embedding. (C) Comparison of the percent composition of the 15 largest clusters in the datasets. NeuroPubs is particularly dense in the highly-specialized neurosurgical topics.",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_03.png"
  },
  {
    "page": 7,
    "bbox": [
      93.74203491210938,
      408.00738525390625,
      518.8484191894531,
      743.2536773681641
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_04.png"
  },
  {
    "page": 8,
    "bbox": [
      37.5,
      319.2041320800781,
      577.5,
      739.9541015625
    ],
    "caption": "Fig. 5. Randomized controlled trial results. (A) A visual depiction of the workflow with a representative patient encounter. During the three-month trial, users interact with a web-based chat interface to interact with the VLM for differential diagnosis assistance (see Fig. S10 for interface details). The conversation is initialized by a neurosurgical resident who provides an image and a text input for evaluation (see Fig. S11 for interaction details). A single representative planar image (e.g., from X-ray, CT, MRI, or digitally captured photos of the pathology) is selected at the discretion of the neurosurgery trainee, as well as a brief one-liner statement summarizing pertinent elements of the patient history (e.g., age, sex, past medical history, medications, presenting symptoms). Upon prompting, GPT or CNS-Obsidian provides a text response with a tiered differential diagnosis. Outputs from GPT, counterfactual CNS-Obsidian output, and the actual diagnosis are shown. Compared to our CNS-Obsidian, GPT generated a broader but less specific set of diagnoses in a more conversational tone. (B) Diagnostic helpfulness of differential (user-rated) (C) Diagnostic accuracy of differential (AI-rated) (D) Diagnostic accuracy, adjusted for response length (E) Conversation length (F) Clinical helpfulness of follow-up chats (user-rated).",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_05.png"
  },
  {
    "page": 21,
    "bbox": [
      72.0,
      293.4062805175781,
      540.0,
      726.15625
    ],
    "caption": "Fig. S1. Automatically generated graphical abstract example. An abstract generated from Kumar A, Dmytriw AA, Salem MM, Kuhn AL, Phan K, Bharatha A, Spears J, Thomas A, Puri A, Marotta TR. Reconstructive vs Deconstructive Endovascular Approach to Intradural Vertebral Artery Aneurysms: A Multicenter Cohort Study (36) via parsing extracted figures and texts with Clade Sonnet-3.5.",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_06.png"
  },
  {
    "page": 22,
    "bbox": [
      72.0,
      559.953125,
      540.0,
      739.953125
    ],
    "caption": "Fig. S2. Dataset filtering. Validation confusion matrices for the image content classification system. The classifier, based on ResNet-50 feature extraction and a linear classifier trained on 400 manually annotated images, was used to filter figures for the differential diagnosis dataset (Class 2 only) and the multiple-choice dataset (Classes 1 and 2). Confusion matrices demonstrate validation performance on the held out 100 manually labeled images. (A) Performance of the classifier when retaining Class 2 images (medical imaging: CT, MRI, X-ray, angiography) and filtering out Classes 1 (clinical visuals: surgical fields, microscopy, anatomical drawings) and 0 (technical content: flowcharts, survival curves, tables). Used to create the differential diagnosis dataset. (B) Classifier performance when retaining Classes 1 and 2 combined while filtering out Class 0. Used to create the multiple choice dataset.",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_07.png"
  },
  {
    "page": 23,
    "bbox": [
      139.5,
      221.703125,
      472.5000305175781,
      739.953125
    ],
    "caption": "Fig. S3. Knowledge translation pipeline demonstrated through conversion of neurosurgical data into training datasets. The example illustrates the automated conversion of specialized knowledge (figure, caption, in-text mention) into three distinct data formats: 1) natural instructional dialogue between user and assistant, 2) clinical vignette with multiple-choice options and detailed explanation, and 3) concise one-liner with prioritized differential diagnoses. The pipeline employs large language models (GPT-4o here) with few-shot in-context learning (four examples for each task randomly selected from pools of 10) to generate consistently formatted outputs while preserving diagnostic accuracy and educational value. Image: case courtesy of Rodrigo Dias Duarte, Radiopaedia.org, rID: 50409. Caption and in-text mention written based on the case information provided on Radiopaedia.org.",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_08.png"
  },
  {
    "page": 24,
    "bbox": [
      72.0,
      494.703125,
      540.0,
      739.953125
    ],
    "caption": "Fig. S4. Multiple choice human evaluation. Thirty-question forms (10 GPT-generated, 10 Claude-generated, 10 board examination question bank - Self-Assessment of Neurological Surgery, SANS) were distributed to residents and attendings to evaluate accuracy and compare human performance with VLMs (GPT-4o and CNS-Obsidian). (A)-(C) Accuracy: VLM and human performance on GPT-generated, Claude-generated MCQs, and self-assessment question samples. The GPT-generated MCQs and Claude-generated MCQs consist of questions excluded from the CNS-Obsidian training data. (D) Human Identification of Question Origin: Participants guessed whether questions were from the self-assessment bank or AI-generated. SANS questions were more often perceived as human-made than GPT-generated (residents, p=0.0002; attendings, p=0.1091) and Claude-generated (residents, p=0.0002; attendings, p=0.0272) questions for both groups. Notably, 54% of AI-generated questions misled at least one evaluator, and 23% misled both. (E) Question Quality Ratings: Evaluators rated questions as suitable for neurosurgery board exams. SANS questions outperformed GPT-generated (residents, p < 10⁻⁵; attendings, p=0.0001) and Claude-generated (residents and attendings, p < 10⁻⁵) questions. Pooled together SANS outperformed AI-generated questions if measured by consensus of the reviewers (requiring both evaluators to mark the question as good, p < 10-7).",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_09.png"
  },
  {
    "page": 25,
    "bbox": [
      164.625,
      260.703125,
      447.375,
      739.953125
    ],
    "caption": "Fig. S5. The effect of figure quality on the reviewer perception of AI-generated neurosurgical board examination-like questions. (A) AI-generated question based on a figure form Gurdjian, Elisha S. M.D.. Certain Autobiographical Impressions in the Life of a Neurosurgeon: Chapter 3. Neurosurgery 19():p 58-68, January 1972 (37). (B). An AI-generated question based on a figure from Cosman, Eric R. Ph.D.; Nashold, Blaine S. M.D.; Ovelman-Levitt, Janice Ph.D.. Theoretical Aspects of Radiofrequency Lesions in the Dorsal Root Entry Zone. Neurosurgery 15(6):p 945-950, December 1984 (38). One of the reviewers of this question wrote “cat brain!?” across the evaluation form. (C). An example of an excellent AI-generated board-like question ranked highly and misleading both reviewers to think it is genuine self-assessment question. Logue, Valentine M.R.C.P., F.R.C.S.. Posterior Fossa Aneurysms: Chapter XV. Neurosurgery 11():p 183-219, January 1965 (39). We conjecture that the question quality as ranked by reviewers was often more dependent on the image being not suitable for the boards rather than the power of the AI model.",
    "file_name": [
      "figures/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_10.png"
  },
  {
    "page": 26,
    "bbox": [
      178.125,
      289.9531555175781,
      433.875,
      739.953125
    ],
    "caption": "Fig. S6. Training a specialty vision-language model. (A) LLaVA-Next is a vision-language model that combines two modalities by slicing the high-resolution image into small patches and embedding them together with a resized full image. It then projects the visual features into the text space, and uses a pre-trained autoregressive model to generate output conditioned on both the image and the prompt. (B) A specialist model is trained in a stages, beginning with general language and multimodal pretraining (Stages 0A and 0B), followed by general medical alignment and finetuning (Stages 1 and 2), and culminating in specialty knowledge integration (Stage 3) using NeuroPubs. This curriculum-based approach was used to create CNS-Obsidian, a neurosurgical expert system capable of interpreting medical imaging and providing specialized diagnostic assessments.",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_11.png"
  },
  {
    "page": 27,
    "bbox": [
      72.0,
      368.0726013183594,
      540.0,
      739.953125
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart11.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_12.png"
  },
  {
    "page": 28,
    "bbox": [
      81.0,
      402.453125,
      530.9999694824219,
      739.953125
    ],
    "caption": "Fig. S8. Ablation studies of three-stage training using Claude-generated evaluations. Configuration [X, Y, Z] denotes number of epochs in Stages 1, 2, and 3. (A) Impact of Stage 1 (alignment) and Stage 2 (general fine-tuning) epochs on model accuracy using Claude generated MCQs dataset (n=1,239), with Stage 3 fixed at 0. Darker red indicates higher accuracy. Baseline [0, 0, 0] achieves 46.53%. Alignment-only training [1, 0, 0] decreases performance to 43.54%. (B) Performance comparison across configurations showing each stage's contribution. Error bars represent standard error. The full three-stage model [1, 3, 3] shows substantial improvement, with Stage 3 providing the largest gain (+13.33%, p < 10-12). (C) Temporal evolution of model performance across training stages. Solid lines represent measurements, dashed lines show interpolated trajectories. (D) Optimization of Stage 3 duration using configuration [1, 3, X]. Performance peaks at X = 10 epochs (70.92%) before plateauing. (E) Effect of Stage 2 duration [1, X, 10]. Performance remains stable, with [1, 10, 10] achieving slightly better results (71.08%, p=0.9647). (F) Joint optimization of Stage 1 and Stage 2/3 durations (configurations [X, Y, Y]). Longer training shows improvements, leading to selection of [5, 10, 10] for final evaluation using both datasets.",
    "file_name": [
      "figures/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_13.png"
  },
  {
    "page": 29,
    "bbox": [
      112.93106079101562,
      281.6199035644531,
      498.53338623046875,
      749.3597106933594
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart13.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_14.png"
  },
  {
    "page": 30,
    "bbox": [
      72.0,
      310.953125,
      540.0,
      739.953125
    ],
    "caption": "Fig. S10. Randomized controlled trial user interface. (A) The landing login interface. All participants had individual accounts with stored chat histories. Access was manually curated by the team. (B) Post-login, the participants are directly forwarded to the chatbot interface which is designed to mimic the typical chatbot interfaces, but enhanced with randomized back-end, a patient MRN field, and an ability to submit an image, including via intuitive “drag-and-drop”. (C) An example interaction with the model. The users can provide binary feedback for individual messages using “upvote” and “downvote” buttons. The model interacted with for this chat is CNS-Obsidian, but the participants are blinded to this information.",
    "file_name": [
      "figures/fileoutpart14.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_15.png"
  },
  {
    "page": 31,
    "bbox": [
      154.125,
      131.703125,
      457.875,
      739.953125
    ],
    "caption": "Fig. S11. An example patient submitted to the study interface. Identifying details anonymized. The model interacted with is CNS-Obsidian-base (version actually used in the trial). For more examples of a base and final model see Movie S1 and Movie S2, respectively.",
    "file_name": [
      "figures/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/Repurposing-Scientific-Literature-Vision-Language-Models/fig_16.png"
  }
]