[
  {
    "page": -1,
    "bbox": [
      307.49925231933594,
      452.6142578125,
      525.3180084228516,
      619.4102172851562
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/Prefix-Tuning/fig_01.png"
  },
  {
    "page": 2,
    "bbox": [
      372.4251403808594,
      593.4564819335938,
      463.1211853027344,
      607.3060150146484
    ],
    "caption": "where the last layer of hi is used to compute the distribution for the next token: pφ(zi+1 | h≤i) = softmax(Wφ hi(n)) and Wφ is a pretrained matrix that map hi(n) to logits over the vocabulary.",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/Prefix-Tuning/fig_02.png"
  },
  {
    "page": 2,
    "bbox": [
      372.4251403808594,
      593.4564819335938,
      463.1211853027344,
      607.3060150146484
    ],
    "caption": "where the last layer of hi is used to compute the distribution for the next token: pφ(zi+1 | h≤i) = softmax(Wφ hi(n)) and Wφ is a pretrained matrix that map hi(n) to logits over the vocabulary.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      313.33055114746094,
      199.96571350097656,
      528.2687530517578,
      242.0530242919922
    ],
    "caption": "4 Prefix-Tuning",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/Prefix-Tuning/fig_04.png"
  },
  {
    "page": 2,
    "bbox": [
      313.33055114746094,
      199.96571350097656,
      528.2687530517578,
      242.0530242919922
    ],
    "caption": "4 Prefix-Tuning",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      71.93769836425781,
      626.0166931152344,
      525.5904846191406,
      779.1447448730469
    ],
    "caption": "Figure 2: An annotated example of prefix-tuning using an autoregressive LM (top) and an encoder-decoder model (bottom). The prefix activations ∀i ∈ Pidx, hi are drawn from a trainable matrix Pθ. The remaining activations are computed by the Transformer.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/Prefix-Tuning/fig_06.png"
  },
  {
    "page": 3,
    "bbox": [
      338.24761962890625,
      365.7677459716797,
      496.10850524902344,
      407.85467529296875
    ],
    "caption": "The training objective is the same as equation ((<>)2), but the set of trainable parameters changes: the language model parameters φ are fixed and the prefix parameters θ are the only trainable parameters.",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/Prefix-Tuning/fig_07.png"
  },
  {
    "page": 3,
    "bbox": [
      338.24761962890625,
      365.7677459716797,
      496.10850524902344,
      407.85467529296875
    ],
    "caption": "The training objective is the same as equation ((<>)2), but the set of trainable parameters changes: the language model parameters φ are fixed and the prefix parameters θ are the only trainable parameters.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      307.2760009765625,
      140.32089233398438,
      528.9847564697266,
      156.74705505371094
    ],
    "caption": "3We find in preliminary experiments that directly optimizing the prefix is very sensitive to the learning rate and initialization.",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/Prefix-Tuning/fig_09.png"
  },
  {
    "page": 3,
    "bbox": [
      314.2790069580078,
      141.8614959716797,
      318.2321319580078,
      149.8314971923828
    ],
    "caption": "3We find in preliminary experiments that directly optimizing the prefix is very sensitive to the learning rate and initialization.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      375.2760009765625,
      141.8614959716797,
      379.22914123535156,
      149.8314971923828
    ],
    "caption": "3We find in preliminary experiments that directly optimizing the prefix is very sensitive to the learning rate and initialization.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      391.2066955566406,
      140.32089233398438,
      395.1598358154297,
      148.2908935546875
    ],
    "caption": "3We find in preliminary experiments that directly optimizing the prefix is very sensitive to the learning rate and initialization.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      517.9806976318359,
      140.32089233398438,
      523.9263153076172,
      149.47044372558594
    ],
    "caption": "3We find in preliminary experiments that directly optimizing the prefix is very sensitive to the learning rate and initialization.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      307.2760009765625,
      140.32089233398438,
      528.9847564697266,
      156.74705505371094
    ],
    "caption": "3We find in preliminary experiments that directly optimizing the prefix is very sensitive to the learning rate and initialization.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      328.8301696777344,
      487.2440490722656,
      331.67735290527344,
      490.0912322998047
    ],
    "caption": "100",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/Prefix-Tuning/table_428.png"
  },
  {
    "page": 7,
    "bbox": [
      74.87577819824219,
      706.7327880859375,
      304.5869140625,
      779.1492767333984
    ],
    "caption": "Figure 4: Prefix length vs. performance on summer-ization (left) and table-to-text (right). Performance increases as the prefix length increases up to a threshold (200 for summarization and 10 for table-to-text) and then a slight performance drop occurs. Each plot reports two metrics (on two vertical axes).",
    "file_name": [
      "figures/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/Prefix-Tuning/fig_16.png"
  },
  {
    "page": 7,
    "bbox": [
      74.87577819824219,
      706.7327880859375,
      304.5869140625,
      779.1492767333984
    ],
    "caption": "Figure 4: Prefix length vs. performance on summer-ization (left) and table-to-text (right). Performance increases as the prefix length increases up to a threshold (200 for summarization and 10 for table-to-text) and then a slight performance drop occurs. Each plot reports two metrics (on two vertical axes).",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      306.7561492919922,
      459.56443786621094,
      534.1283874511719,
      584.9872131347656
    ],
    "caption": "Figure 5: Initializing the prefix with activations of real words significantly outperforms random initialization, in low-data settings.",
    "file_name": [
      "figures/fileoutpart18.png"
    ],
    "output_file": "assets/images/paper/Prefix-Tuning/fig_18.png"
  },
  {
    "page": 7,
    "bbox": [
      306.7561492919922,
      459.56443786621094,
      534.1283874511719,
      584.9872131347656
    ],
    "caption": "Figure 5: Initializing the prefix with activations of real words significantly outperforms random initialization, in low-data settings.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      84.32600402832031,
      540.9430389404297,
      508.93714904785156,
      736.0820465087891
    ],
    "caption": "Figure 6: Prefix-tuning (orange) outperforms fine-tuning (blue) in low-data regimes in addition to requiring many fewer parameters. The top three plots correspond to summarization, measured by ROUGE-1, ROUGE-2, and ROUGE-L. The bottom three plots correspond to table-to-text, measured by NIST, METEOR, and CIDEr. The x-axis is the training size and the y-axis is the evaluation metric (higher is better).",
    "file_name": [
      "figures/fileoutpart21.png"
    ],
    "output_file": "assets/images/paper/Prefix-Tuning/fig_20.png"
  },
  {
    "page": 13,
    "bbox": [
      84.32600402832031,
      540.9430389404297,
      508.93714904785156,
      736.0820465087891
    ],
    "caption": "Figure 6: Prefix-tuning (orange) outperforms fine-tuning (blue) in low-data regimes in addition to requiring many fewer parameters. The top three plots correspond to summarization, measured by ROUGE-1, ROUGE-2, and ROUGE-L. The bottom three plots correspond to table-to-text, measured by NIST, METEOR, and CIDEr. The x-axis is the training size and the y-axis is the evaluation metric (higher is better).",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      71.51231384277344,
      155.79995727539062,
      504.810302734375,
      389.19163513183594
    ],
    "caption": "Figure 7: Initializing the prefix with activations of real words significantly outperforms random initialization, in a low-data setting with 100 training data.",
    "file_name": [
      "figures/fileoutpart22.png"
    ],
    "output_file": "assets/images/paper/Prefix-Tuning/fig_22.png"
  },
  {
    "page": 13,
    "bbox": [
      71.51231384277344,
      155.79995727539062,
      504.810302734375,
      389.19163513183594
    ],
    "caption": "Figure 7: Initializing the prefix with activations of real words significantly outperforms random initialization, in a low-data setting with 100 training data.",
    "file_name": ""
  }
]