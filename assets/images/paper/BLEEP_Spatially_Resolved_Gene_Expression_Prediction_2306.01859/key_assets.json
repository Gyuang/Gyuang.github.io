{
  "architecture": {
    "page": 4,
    "bbox": [
      163.92007446289062,
      315.14178466796875,
      450.57154846191406,
      335.07847595214844
    ],
    "caption": "For BLEEP, we use the pretrained ResNet50[(<>)10] as the image encoder and a fully connected network (FCN) with an output dimension of 256 as the expression encoder, which doubles as a projection head. The image features from the image encoder are passed through a separate projection head to bring the two modalities to the same dimension before applying the contrastive loss similar to CLIP[(<>)15], where the model learns to pull the paired representations together while pushing other representations apart. We find that the ResNet50 image encoder with fewer trainable parameters obtained more favorable results compared to various pretrained vision-transformer (ViT) encoders (Supplementary Table 1). Larger models in conjunction with a relatively small training dataset may encourage information to be memorized in the weights of the network rather than being encoded in the projections, therefore rendering the learned joint embedding ineffective for downstream imputation for our use case.",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/BLEEP_Spatially_Resolved_Gene_Expression_Prediction_2306.01859/fig_04.png"
  },
  "results": {
    "page": 5,
    "bbox": [
      186.4929962158203,
      622.455810546875,
      428.0262756347656,
      679.158447265625
    ],
    "caption": "Table 2: Predicted gene expression values with top 5 correlations with original profile for each method from one representative replicate.",
    "file_name": [
      "tables/fileoutpart3.xlsx",
      "tables/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/BLEEP_Spatially_Resolved_Gene_Expression_Prediction_2306.01859/table_01.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/BLEEP_Spatially_Resolved_Gene_Expression_Prediction_2306.01859/fig_04.png)\n캡션: For BLEEP, we use the pretrained ResNet50[(<>)10] as the image encoder and a fully connected network (FCN) with an output dimension of 256 as the expression encoder, which doubles as a projection head. The image features from the image encoder are passed through a separate projection head to bring the two modalities to the same dimension before applying the contrastive loss similar to CLIP[(<>)15]…\n\n### Main Results Table\n![Results](/assets/images/paper/BLEEP_Spatially_Resolved_Gene_Expression_Prediction_2306.01859/table_01.png)\n캡션: Table 2: Predicted gene expression values with top 5 correlations with original profile for each method from one representative replicate."
}