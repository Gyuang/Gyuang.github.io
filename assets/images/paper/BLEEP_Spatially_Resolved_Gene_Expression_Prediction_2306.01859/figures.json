[
  {
    "page": 3,
    "bbox": [
      108.0,
      416.9969940185547,
      503.98655700683594,
      719.9980010986328
    ],
    "caption": "Figure 1: BLEEP achieves gene expression prediction from H&E image through (a) BLEEP learns a bimodal embedding from expression profiles and H&E image patches, (b) images patch queries are projected into the joint embedding space to index the k nearest reference expression profiles, and (c) the indexed reference expression profiles are linearly combined to produce the imputed gene expressions for queries.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/BLEEP_Spatially_Resolved_Gene_Expression_Prediction_2306.01859/fig_01.png"
  },
  {
    "page": 4,
    "bbox": [
      185.99185180664062,
      381.39691162109375,
      428.49302673339844,
      399.9112854003906
    ],
    "caption": "where τ is a temperature hyperparameter. Cross entropy(ce) loss is applied to align the image features and expression features to produce the final loss L:",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/BLEEP_Spatially_Resolved_Gene_Expression_Prediction_2306.01859/fig_02.png"
  },
  {
    "page": 4,
    "bbox": [
      185.99185180664062,
      381.39691162109375,
      428.49302673339844,
      399.9112854003906
    ],
    "caption": "where τ is a temperature hyperparameter. Cross entropy(ce) loss is applied to align the image features and expression features to produce the final loss L:",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      163.92007446289062,
      315.14178466796875,
      450.57154846191406,
      335.07847595214844
    ],
    "caption": "For BLEEP, we use the pretrained ResNet50[(<>)10] as the image encoder and a fully connected network (FCN) with an output dimension of 256 as the expression encoder, which doubles as a projection head. The image features from the image encoder are passed through a separate projection head to bring the two modalities to the same dimension before applying the contrastive loss similar to CLIP[(<>)15], where the model learns to pull the paired representations together while pushing other representations apart. We find that the ResNet50 image encoder with fewer trainable parameters obtained more favorable results compared to various pretrained vision-transformer (ViT) encoders (Supplementary Table 1). Larger models in conjunction with a relatively small training dataset may encourage information to be memorized in the weights of the network rather than being encoded in the projections, therefore rendering the learned joint embedding ineffective for downstream imputation for our use case.",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/BLEEP_Spatially_Resolved_Gene_Expression_Prediction_2306.01859/fig_04.png"
  },
  {
    "page": 4,
    "bbox": [
      163.92007446289062,
      315.14178466796875,
      450.57154846191406,
      335.07847595214844
    ],
    "caption": "For BLEEP, we use the pretrained ResNet50[(<>)10] as the image encoder and a fully connected network (FCN) with an output dimension of 256 as the expression encoder, which doubles as a projection head. The image features from the image encoder are passed through a separate projection head to bring the two modalities to the same dimension before applying the contrastive loss similar to CLIP[(<>)15], where the model learns to pull the paired representations together while pushing other representations apart. We find that the ResNet50 image encoder with fewer trainable parameters obtained more favorable results compared to various pretrained vision-transformer (ViT) encoders (Supplementary Table 1). Larger models in conjunction with a relatively small training dataset may encourage information to be memorized in the weights of the network rather than being encoded in the projections, therefore rendering the learned joint embedding ineffective for downstream imputation for our use case.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      108.0,
      489.0820007324219,
      503.9955291748047,
      720.0021057128906
    ],
    "caption": "Figure 2: Predicted expression profiles compare with reference expression profiles normalized by gene count means (Upper) or gene count variance (Lower).",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/BLEEP_Spatially_Resolved_Gene_Expression_Prediction_2306.01859/fig_06.png"
  },
  {
    "page": 6,
    "bbox": [
      108.0,
      270.2149963378906,
      503.9984893798828,
      435.1942138671875
    ],
    "caption": "Figure 3: Original and predicted spatially resolved expression levels for CYP3A4 overlaying the H&E image, visualized with variable (Top) and fixed (Bottom) color scale.",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/BLEEP_Spatially_Resolved_Gene_Expression_Prediction_2306.01859/fig_07.png"
  },
  {
    "page": 7,
    "bbox": [
      108.0,
      599.6399993896484,
      504.0048370361328,
      719.9996795654297
    ],
    "caption": "Figure 4: Gene-gene correlation heatmap calculated using the predicted expressions for each method.",
    "file_name": [
      "figures/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/BLEEP_Spatially_Resolved_Gene_Expression_Prediction_2306.01859/fig_08.png"
  },
  {
    "page": 8,
    "bbox": [
      116.65499877929688,
      430.5870056152344,
      492.84840393066406,
      719.9994201660156
    ],
    "caption": "Figure 5: Leiden clusterings for original and predicted expressions overlaying the H&E image. Image-only expression predictions are invariant to low quality regions (red) during actual experiment.",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/BLEEP_Spatially_Resolved_Gene_Expression_Prediction_2306.01859/fig_09.png"
  }
]