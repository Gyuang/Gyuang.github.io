[
  {
    "page": 1,
    "bbox": [
      111.78919982910156,
      537.0004272460938,
      495.6968688964844,
      718.1697235107422
    ],
    "caption": "Figure 1: Illustration of (a) an uncurated study for a patient. While previous work has relied on annotation and curation, HLIP enables language-image pre-training directly on such data. (b) HLIP achieves higher performance in zero-shot brain MRI disease classification while using less memory than the original ViT, which can only be trained with gradient checkpointing.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_01.png"
  },
  {
    "page": 1,
    "bbox": [
      111.78919982910156,
      537.0004272460938,
      495.6968688964844,
      718.1697235107422
    ],
    "caption": "Figure 1: Illustration of (a) an uncurated study for a patient. While previous work has relied on annotation and curation, HLIP enables language-image pre-training directly on such data. (b) HLIP achieves higher performance in zero-shot brain MRI disease classification while using less memory than the original ViT, which can only be trained with gradient checkpointing.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      165.4044952392578,
      162.52090454101562,
      256.51429748535156,
      181.96347045898438
    ],
    "caption": "3 Method",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_03.png"
  },
  {
    "page": 2,
    "bbox": [
      191.45899963378906,
      169.06890869140625,
      251.70565795898438,
      181.96347045898438
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      165.4044952392578,
      162.52090454101562,
      256.51429748535156,
      181.96347045898438
    ],
    "caption": "3 Method",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      428.74000549316406,
      145.0885467529297,
      479.8005065917969,
      160.4524688720703
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_06.png"
  },
  {
    "page": 2,
    "bbox": [
      433.80999755859375,
      152.44654846191406,
      440.3235626220703,
      159.42034912109375
    ],
    "caption": "N = M × d × h × w",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      435.1009979248047,
      145.0885467529297,
      440.9938659667969,
      153.09446716308594
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      447.35699462890625,
      152.44654846191406,
      453.88450622558594,
      159.42034912109375
    ],
    "caption": "N = M × d × h × w",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      448.5540008544922,
      145.0885467529297,
      454.9629211425781,
      153.09446716308594
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      461.23399353027344,
      152.44654846191406,
      470.4673156738281,
      160.4524688720703
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      428.74000549316406,
      145.0885467529297,
      479.8005065917969,
      160.4524688720703
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      278.3190002441406,
      127.89089965820312,
      333.6468963623047,
      147.3324737548828
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_13.png"
  },
  {
    "page": 2,
    "bbox": [
      284.7250061035156,
      134.216552734375,
      290.2273254394531,
      142.22247314453125
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      311.2220001220703,
      134.4379119873047,
      329.81414794921875,
      147.3324737548828
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      278.3190002441406,
      127.89089965820312,
      333.6468963623047,
      147.3324737548828
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      362.6549987792969,
      127.89089965820312,
      459.17266845703125,
      146.4014129638672
    ],
    "caption": "D",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_17.png"
  },
  {
    "page": 2,
    "bbox": [
      362.6549987792969,
      127.89089965820312,
      459.17266845703125,
      146.4014129638672
    ],
    "caption": "D",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      111.09367370605469,
      556.8477020263672,
      500.32798767089844,
      719.2247009277344
    ],
    "caption": "Figure 2: Illustration of (a) the radiology data hierarchy for a single patient, including the study, single scan, and adjacent slices. Our hierarchical attention mechanism mirrors this hierarchy and computes self-attention independently within each level. (b) Our HLIP framework incorporates a visual encoder that performs attention at different levels. In practice, lightweight slice or scan attention with a few study attention layers suffices to extract features from the full radiology study.",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_19.png"
  },
  {
    "page": 3,
    "bbox": [
      111.09367370605469,
      556.8477020263672,
      500.32798767089844,
      719.2247009277344
    ],
    "caption": "Figure 2: Illustration of (a) the radiology data hierarchy for a single patient, including the study, single scan, and adjacent slices. Our hierarchical attention mechanism mirrors this hierarchy and computes self-attention independently within each level. (b) Our HLIP framework incorporates a visual encoder that performs attention at different levels. In practice, lightweight slice or scan attention with a few study attention layers suffices to extract features from the full radiology study.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      234.7050018310547,
      352.2375030517578,
      242.08802795410156,
      366.57049560546875
    ],
    "caption": "We explore a simple hierarchical attention mechanism grounded in this structure. Specifically, as shown in Figure (<>)2, we compute self-attention independently within each hierarchical level:",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_21.png"
  },
  {
    "page": 3,
    "bbox": [
      234.9040069580078,
      359.59649658203125,
      241.41775512695312,
      366.57049560546875
    ],
    "caption": "We explore a simple hierarchical attention mechanism grounded in this structure. Specifically, as shown in Figure (<>)2, we compute self-attention independently within each hierarchical level:",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      236.19500732421875,
      352.2375030517578,
      242.08802795410156,
      360.24365234375
    ],
    "caption": "We explore a simple hierarchical attention mechanism grounded in this structure. Specifically, as shown in Figure (<>)2, we compute self-attention independently within each hierarchical level:",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      373.6236572265625,
      258.37852478027344,
      444.6470947265625,
      279.3449401855469
    ],
    "caption": "N2",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_24.png"
  },
  {
    "page": 3,
    "bbox": [
      385.88600158691406,
      270.1215057373047,
      397.58131408691406,
      279.3449401855469
    ],
    "caption": "M",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      387.18699645996094,
      262.7635040283203,
      394.78172302246094,
      269.7375030517578
    ],
    "caption": "This contrasts with existing methods that modify attention computation using convolution [(<>)15, (<>)16], pooling [(<>)17], or shifted windows [(<>)14], all of which require either regular activation shapes or attention masks. Our mechanism relies solely on a simple reshape operation, as there is no overlap between different scans or groups of adjacent slices1 (<>), allowing greater flexibility for radiology studies that comprise multiple scans. As shown in Figure (<>)2, each layer in the ViT can perform attention at any level. In practice, we evenly divide the ViT backbone into four subsets of layers (e.g., three layers per subset for the 12-layer ViT-B) and apply study attention only to the last layer of each subset, while the remaining layers perform the lighter scan or slice attention.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      373.6236572265625,
      258.37852478027344,
      444.6470947265625,
      279.3449401855469
    ],
    "caption": "N2",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      424.5246276855469,
      230.13372802734375,
      503.3280944824219,
      251.60293579101562
    ],
    "caption": "Ω(+ N × c).",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_28.png"
  },
  {
    "page": 3,
    "bbox": [
      440.6809997558594,
      242.3804931640625,
      452.3763122558594,
      251.60293579101562
    ],
    "caption": "This contrasts with existing methods that modify attention computation using convolution [(<>)15, (<>)16], pooling [(<>)17], or shifted windows [(<>)14], all of which require either regular activation shapes or attention masks. Our mechanism relies solely on a simple reshape operation, as there is no overlap between different scans or groups of adjacent slices1 (<>), allowing greater flexibility for radiology studies that comprise multiple scans. As shown in Figure (<>)2, each layer in the ViT can perform attention at any level. In practice, we evenly divide the ViT backbone into four subsets of layers (e.g., three layers per subset for the 12-layer ViT-B) and apply study attention only to the last layer of each subset, while the remaining layers perform the lighter scan or slice attention.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      436.7949981689453,
      230.13372802734375,
      457.26368713378906,
      243.02865600585938
    ],
    "caption": "Ω(+ N × c).",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      424.5246276855469,
      230.13372802734375,
      503.3280944824219,
      251.60293579101562
    ],
    "caption": "Ω(+ N × c).",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      246.19900512695312,
      592.0625457763672,
      253.5808563232422,
      606.3943481445312
    ],
    "caption": "Mmax×c",
    "file_name": [
      "figures/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_32.png"
  },
  {
    "page": 4,
    "bbox": [
      246.3979949951172,
      599.4205474853516,
      252.9115753173828,
      606.3943481445312
    ],
    "caption": "Mmax×c",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      247.68800354003906,
      592.0625457763672,
      253.5808563232422,
      600.0684661865234
    ],
    "caption": "Mmax×c",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      155.67599487304688,
      561.5899047851562,
      225.58839416503906,
      581.0314788818359
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_35.png"
  },
  {
    "page": 4,
    "bbox": [
      162.07200622558594,
      567.9165496826172,
      178.98345947265625,
      575.9224700927734
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      197.21200561523438,
      568.1369171142578,
      225.58839416503906,
      581.0314788818359
    ],
    "caption": "scan",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      155.67599487304688,
      561.5899047851562,
      225.58839416503906,
      581.0314788818359
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      445.4290008544922,
      561.5899047851562,
      507.78590393066406,
      581.0314788818359
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart11.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_39.png"
  },
  {
    "page": 4,
    "bbox": [
      451.8249969482422,
      567.9165496826172,
      470.0545196533203,
      575.9224700927734
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      488.2720031738281,
      568.1369171142578,
      503.95606994628906,
      581.0314788818359
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      445.4290008544922,
      561.5899047851562,
      507.78590393066406,
      581.0314788818359
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      195.76800537109375,
      550.680908203125,
      303.69769287109375,
      570.1224670410156
    ],
    "caption": "d",
    "file_name": [
      "figures/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_43.png"
  },
  {
    "page": 4,
    "bbox": [
      202.16400146484375,
      557.0065460205078,
      220.47720336914062,
      565.0124664306641
    ],
    "caption": "h×w×c",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      238.6009979248047,
      557.2279205322266,
      272.149658203125,
      570.1224670410156
    ],
    "caption": "d",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      286.0469970703125,
      557.0065460205078,
      303.69769287109375,
      565.0124664306641
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      195.76800537109375,
      550.680908203125,
      303.69769287109375,
      570.1224670410156
    ],
    "caption": "d",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      115.34101867675781,
      570.6520233154297,
      297.7687530517578,
      720.1310272216797
    ],
    "caption": "(a) RSNA head CT hemorrhage dataset benchmarks a 5-way intracranial hemorrhage diagnosis task.",
    "file_name": [
      "figures/fileoutpart17.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_48.png"
  },
  {
    "page": 7,
    "bbox": [
      115.34101867675781,
      570.6520233154297,
      297.7687530517578,
      720.1310272216797
    ],
    "caption": "(a) RSNA head CT hemorrhage dataset benchmarks a 5-way intracranial hemorrhage diagnosis task.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      316.72496032714844,
      570.6519622802734,
      499.156005859375,
      718.2073516845703
    ],
    "caption": "Figure 3: Results of linear probing on the (a) RSNA [(<>)26] and (b) CQ500 [(<>)27] datasets. We report AUC, with our method shown in blue, FM-HeadCT [(<>)25] in green, and Google CT [(<>)54] in red.",
    "file_name": [
      "figures/fileoutpart18.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_50.png"
  },
  {
    "page": 7,
    "bbox": [
      316.72496032714844,
      570.6519622802734,
      499.156005859375,
      718.2073516845703
    ],
    "caption": "Figure 3: Results of linear probing on the (a) RSNA [(<>)26] and (b) CQ500 [(<>)27] datasets. We report AUC, with our method shown in blue, FM-HeadCT [(<>)25] in green, and Google CT [(<>)54] in red.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      107.84478759765625,
      424.5045471191406,
      504.1565399169922,
      513.1028289794922
    ],
    "caption": "Figure 4: Results of disease classification on Pub-Brain-5, highlighting the equal importance of (a) data scale, (b) radiology study modeling, and (c) computational efficiency. We report mACC.",
    "file_name": [
      "figures/fileoutpart19.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_52.png"
  },
  {
    "page": 7,
    "bbox": [
      107.84478759765625,
      424.5045471191406,
      504.1565399169922,
      513.1028289794922
    ],
    "caption": "Figure 4: Results of disease classification on Pub-Brain-5, highlighting the equal importance of (a) data scale, (b) radiology study modeling, and (c) computational efficiency. We report mACC.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      107.87899780273438,
      -5999.388442993164,
      504.1219024658203,
      393.1707763671875
    ],
    "caption": "Figure 4: Results of disease classification on Pub-Brain-5, highlighting the equal importance of (a) data scale, (b) radiology study modeling, and (c) computational efficiency. We report mACC.",
    "file_name": [
      "figures/fileoutpart20.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_54.png"
  },
  {
    "page": 7,
    "bbox": [
      107.87899780273438,
      -5999.388442993164,
      504.1219024658203,
      393.1707763671875
    ],
    "caption": "Figure 4: Results of disease classification on Pub-Brain-5, highlighting the equal importance of (a) data scale, (b) radiology study modeling, and (c) computational efficiency. We report mACC.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      150.22055053710938,
      208.47019958496094,
      464.2680206298828,
      321.05909729003906
    ],
    "caption": "We acknowledge that one reason HLIP achieves superior performance (e.g., on Pub-Brain-5) is its training on a large-scale, domain-specific dataset. As shown in Figure (<>)4a, zero-shot disease classification on Pub-Brain-5 drops by 24.5% mACC when using only 10% of the training data. However, data scale is not the only contributing factor. Given an uncurated dataset, a straightforward approach is to randomly select one scan per study per step while training a 3D ViT. Modeling radiology studies in this way results in a 12.4% drop in mACC, even when trained on the full dataset with the same computational resources, as shown in Figure (<>)4b. Computational efficiency is equally important, as it determines how many samples can be contrasted within a batch. As shown in Figure (<>)4c, a small batch size (e.g., 64) results in a 6.6% drop in mACC. Therefore, HLIP bridges large-scale data, effective study modeling, and computational efficiency, enabling it to outperform current state-of-the-art foundation models [(<>)23, (<>)24] by a substantial margin.",
    "file_name": [
      "figures/fileoutpart21.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_56.png"
  },
  {
    "page": 7,
    "bbox": [
      150.22055053710938,
      208.47019958496094,
      464.2680206298828,
      321.05909729003906
    ],
    "caption": "We acknowledge that one reason HLIP achieves superior performance (e.g., on Pub-Brain-5) is its training on a large-scale, domain-specific dataset. As shown in Figure (<>)4a, zero-shot disease classification on Pub-Brain-5 drops by 24.5% mACC when using only 10% of the training data. However, data scale is not the only contributing factor. Given an uncurated dataset, a straightforward approach is to randomly select one scan per study per step while training a 3D ViT. Modeling radiology studies in this way results in a 12.4% drop in mACC, even when trained on the full dataset with the same computational resources, as shown in Figure (<>)4b. Computational efficiency is equally important, as it determines how many samples can be contrasted within a batch. As shown in Figure (<>)4c, a small batch size (e.g., 64) results in a 6.6% drop in mACC. Therefore, HLIP bridges large-scale data, effective study modeling, and computational efficiency, enabling it to outperform current state-of-the-art foundation models [(<>)23, (<>)24] by a substantial margin.",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      106.89399719238281,
      593.5496673583984,
      510.44911193847656,
      721.3272857666016
    ],
    "caption": "Figure 6: Qualitative results of zero-shot diagnosis on the Rad-ChestCT [(<>)22] and BraTS [(<>)46, (<>)47] datasets. The first row shows the original clinical scans with pathologic regions outlined (dashed red). The second row shows activation HLIP maps [(<>)55]. HLIP identifies the pathologic regions across multiple groups of adjacent chest CT slides (left) and brain MRI scans (right).",
    "file_name": [
      "figures/fileoutpart22.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_58.png"
  },
  {
    "page": 8,
    "bbox": [
      106.89399719238281,
      593.5496673583984,
      510.44911193847656,
      721.3272857666016
    ],
    "caption": "Figure 6: Qualitative results of zero-shot diagnosis on the Rad-ChestCT [(<>)22] and BraTS [(<>)46, (<>)47] datasets. The first row shows the original clinical scans with pathologic regions outlined (dashed red). The second row shows activation HLIP maps [(<>)55]. HLIP identifies the pathologic regions across multiple groups of adjacent chest CT slides (left) and brain MRI scans (right).",
    "file_name": ""
  },
  {
    "page": 8,
    "bbox": [
      306.96861267089844,
      -1754.3781433105469,
      500.88914489746094,
      286.2184753417969
    ],
    "caption": "Visualizations. In Figure (<>)6, we visualize the activation maps [(<>)55] of HLIP in the zero-shot setting on Rad-ChestCT [(<>)22] and BraTS [(<>)46, (<>)47]. For chest CT (curated, single scan), HLIP attends to pathologic regions across different slices, e.g., slice Z and Z+8, by leveraging scan attention. For brain MRI (uncurated, multiple scans), HLIP attends to pathologic regions across different scan types, e.g., FLAIR and T1W+contrast, by leveraging study attention. This demonstrates the effectiveness HLIP in modeling the hierarchical structure of 3D medical imaging.",
    "file_name": [
      "figures/fileoutpart23.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_60.png"
  },
  {
    "page": 8,
    "bbox": [
      306.96861267089844,
      -1754.3781433105469,
      500.88914489746094,
      286.2184753417969
    ],
    "caption": "Visualizations. In Figure (<>)6, we visualize the activation maps [(<>)55] of HLIP in the zero-shot setting on Rad-ChestCT [(<>)22] and BraTS [(<>)46, (<>)47]. For chest CT (curated, single scan), HLIP attends to pathologic regions across different slices, e.g., slice Z and Z+8, by leveraging scan attention. For brain MRI (uncurated, multiple scans), HLIP attends to pathologic regions across different scan types, e.g., FLAIR and T1W+contrast, by leveraging study attention. This demonstrates the effectiveness HLIP in modeling the hierarchical structure of 3D medical imaging.",
    "file_name": ""
  },
  {
    "page": 12,
    "bbox": [
      107.85063171386719,
      371.4905548095703,
      504.0738983154297,
      471.65147399902344
    ],
    "caption": "Figure 8: Statistic results on BrainMRI220K.",
    "file_name": [
      "figures/fileoutpart24.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_62.png"
  },
  {
    "page": 12,
    "bbox": [
      107.85063171386719,
      371.4905548095703,
      504.0738983154297,
      471.65147399902344
    ],
    "caption": "Figure 8: Statistic results on BrainMRI220K.",
    "file_name": ""
  },
  {
    "page": 12,
    "bbox": [
      107.83599853515625,
      -32768.0,
      504.1179962158203,
      333.6640167236328
    ],
    "caption": "Figure 8: Statistic results on BrainMRI220K.",
    "file_name": [
      "figures/fileoutpart25.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_64.png"
  },
  {
    "page": 12,
    "bbox": [
      107.83599853515625,
      -32768.0,
      504.1179962158203,
      333.6640167236328
    ],
    "caption": "Figure 8: Statistic results on BrainMRI220K.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      107.85092163085938,
      487.2595520019531,
      504.07005310058594,
      577.4574737548828
    ],
    "caption": "Figure 10: Statistic results on HeadCT240K.",
    "file_name": [
      "figures/fileoutpart26.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_66.png"
  },
  {
    "page": 13,
    "bbox": [
      107.85092163085938,
      487.2595520019531,
      504.07005310058594,
      577.4574737548828
    ],
    "caption": "Figure 10: Statistic results on HeadCT240K.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      107.83599853515625,
      -32768.0,
      504.1179962158203,
      447.828125
    ],
    "caption": "Figure 10: Statistic results on HeadCT240K.",
    "file_name": [
      "figures/fileoutpart27.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_68.png"
  },
  {
    "page": 13,
    "bbox": [
      107.83599853515625,
      -32768.0,
      504.1179962158203,
      447.828125
    ],
    "caption": "Figure 10: Statistic results on HeadCT240K.",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      468.8840026855469,
      292.33790588378906,
      508.1846923828125,
      311.10743713378906
    ],
    "caption": "In this section, we summarize the model configuration described in Section (<>)3.2 and provide details of the pre-training setup referenced in both Section (<>)3.2 and Section (<>)4.1. For linear probing evaluation on head CT, we refer readers to FM-HeadCT [(<>)25].",
    "file_name": [
      "figures/fileoutpart28.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_70.png"
  },
  {
    "page": 14,
    "bbox": [
      489.59800720214844,
      304.08154296875,
      497.1925048828125,
      311.0553436279297
    ],
    "caption": "M",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      485.4340057373047,
      296.7235565185547,
      493.02850341796875,
      303.6973571777344
    ],
    "caption": "In this section, we summarize the model configuration described in Section (<>)3.2 and provide details of the pre-training setup referenced in both Section (<>)3.2 and Section (<>)4.1. For linear probing evaluation on head CT, we refer readers to FM-HeadCT [(<>)25].",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      493.031005859375,
      296.07025146484375,
      502.8541259765625,
      302.0727233886719
    ],
    "caption": "In this section, we summarize the model configuration described in Section (<>)3.2 and provide details of the pre-training setup referenced in both Section (<>)3.2 and Section (<>)4.1. For linear probing evaluation on head CT, we refer readers to FM-HeadCT [(<>)25].",
    "file_name": ""
  },
  {
    "page": 14,
    "bbox": [
      468.8840026855469,
      292.33790588378906,
      508.1846923828125,
      311.10743713378906
    ],
    "caption": "In this section, we summarize the model configuration described in Section (<>)3.2 and provide details of the pre-training setup referenced in both Section (<>)3.2 and Section (<>)4.1. For linear probing evaluation on head CT, we refer readers to FM-HeadCT [(<>)25].",
    "file_name": ""
  },
  {
    "page": 17,
    "bbox": [
      150.50709533691406,
      456.7218017578125,
      496.2663879394531,
      693.5381927490234
    ],
    "caption": "Figure 12: Prospective evaluation on 52 brain MRI diagnoses. intra axial tumor lacunar stroke cerebral vascular calcification skull tumor traumatic subarachnoid hemorrhageposterior fossa tumors cerebral embolization parathyroid noduleencephalomalaciahead neck neoplasmextra axial tumorcerebral midline shiftcerebral subdural empyemaoptic nerve lesioncavernomacerebral arteriovenous malformationintraventricular hemorrhageaneurysmal subarachnoid hemorrhagecerebral fluid collectionbrain contusionbrain leadpharyngeal abscessaneurysm coilhead neck polypscraniotomyorbital traumapneumocephalusskull base fracturecholesteatomaorbital emphysemacerebral edemabrain mass effectother cerebral hemorrhageventriculomegaly catheterany skull fracturesubcutaneous emphysema mucosal thickening otitis media arterial dissection colloid cyst cerebral aneurysm large vessel stroke sinusitis brain herniation displaced skull fracture epidermoid dermoid cystpineal cyst cerebral foreign bodycerebral atrophy airway obstructionsubdural hygromacavum septum pellucidumsmall vessel diseasehead neck bony lesionsdysgenesis corpus callosumcerebral abscessmastoid effusioncerebral venous sinus thrombosissubarachnoid hemorrhagechiari malformationresection cavityperitonsillar abscessacute subdural hematomatransependymal flowobstructive hydrocephalusaneurysm clipsubdural hematomacerebral intraparenchymal hemorrhagethyroid nodulesellar parasellar tumorhead neck enlarged lymph nodebasal ganglia calcificationsubacute chronic subdural hematomaburr holeepidural hematoma sinus mucoceleany craniofacial injury brain tumor cranioplasty arachnoid cyst orbital inflammation fibrous dysplasia 50 66 74 82 90 64 71 78 85 64 71 78 85 62 68 74 80 68 77 86 95 68 77 86 95 70 80 90 100 70 80 90 100 66 74 82 90 68 77 86 95 64 71 78 85 70 80 90 100 70 80 90 100 66 74 82 90 60 65 70 75 66 74 82 90 70 80 90 100 70 80 90 100 68 77 86 95 66 74 82 90 68 77 86 95 70 80 90 100 70 80 90 100 68 77 86 95 70 80 90 100 68 77 86 95 70 80 90 100 70 80 90 100 70 80 90 100 68 77 86 95 68 77 86 95 68 77 86 95 68 77 86 95 68 77 86 95 68 77 86 95 66 74 82 90 66 74 82 90 64 71 78 85 70 80 90 100 68 77 86 95 62 68 74 80 68 778695 68778695 64 71 78 85 70 80 90 100 68 77 86 95 60 65 70 75 58 62 66 70 66 74 82 90 68 77 86 95 68 77 86 95 68 77 86 95 64 71 78 85 68 77 86 95 62 68 74 80 68 77 86 95 68 77 86 95 66 74 82 90 64 71 78 85 68 77 86 95 68 77 86 95 68 77 86 95 70 80 90 100 70 80 90 100 70 80 90 100 70 80 90 100 70 80 90 100 68 77 86 95 68 77 86 95 66 74 82 90 66 74 82 90 68 77 86 95 66 74 82 90 70 80 90 100 70 80 90 100 66 74 82 90 64 71 78 85 66 74 82 90 66 74 82 90 70 80 90 100 66 74 82 90 68 77 86 95 62 68 74 80 ViT (mAUC = 88.13) HLIP (mAUC = 90.05)",
    "file_name": [
      "figures/fileoutpart41.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_75.png"
  },
  {
    "page": 17,
    "bbox": [
      150.50709533691406,
      456.7218017578125,
      496.2663879394531,
      693.5381927490234
    ],
    "caption": "Figure 12: Prospective evaluation on 52 brain MRI diagnoses. intra axial tumor lacunar stroke cerebral vascular calcification skull tumor traumatic subarachnoid hemorrhageposterior fossa tumors cerebral embolization parathyroid noduleencephalomalaciahead neck neoplasmextra axial tumorcerebral midline shiftcerebral subdural empyemaoptic nerve lesioncavernomacerebral arteriovenous malformationintraventricular hemorrhageaneurysmal subarachnoid hemorrhagecerebral fluid collectionbrain contusionbrain leadpharyngeal abscessaneurysm coilhead neck polypscraniotomyorbital traumapneumocephalusskull base fracturecholesteatomaorbital emphysemacerebral edemabrain mass effectother cerebral hemorrhageventriculomegaly catheterany skull fracturesubcutaneous emphysema mucosal thickening otitis media arterial dissection colloid cyst cerebral aneurysm large vessel stroke sinusitis brain herniation displaced skull fracture epidermoid dermoid cystpineal cyst cerebral foreign bodycerebral atrophy airway obstructionsubdural hygromacavum septum pellucidumsmall vessel diseasehead neck bony lesionsdysgenesis corpus callosumcerebral abscessmastoid effusioncerebral venous sinus thrombosissubarachnoid hemorrhagechiari malformationresection cavityperitonsillar abscessacute subdural hematomatransependymal flowobstructive hydrocephalusaneurysm clipsubdural hematomacerebral intraparenchymal hemorrhagethyroid nodulesellar parasellar tumorhead neck enlarged lymph nodebasal ganglia calcificationsubacute chronic subdural hematomaburr holeepidural hematoma sinus mucoceleany craniofacial injury brain tumor cranioplasty arachnoid cyst orbital inflammation fibrous dysplasia 50 66 74 82 90 64 71 78 85 64 71 78 85 62 68 74 80 68 77 86 95 68 77 86 95 70 80 90 100 70 80 90 100 66 74 82 90 68 77 86 95 64 71 78 85 70 80 90 100 70 80 90 100 66 74 82 90 60 65 70 75 66 74 82 90 70 80 90 100 70 80 90 100 68 77 86 95 66 74 82 90 68 77 86 95 70 80 90 100 70 80 90 100 68 77 86 95 70 80 90 100 68 77 86 95 70 80 90 100 70 80 90 100 70 80 90 100 68 77 86 95 68 77 86 95 68 77 86 95 68 77 86 95 68 77 86 95 68 77 86 95 66 74 82 90 66 74 82 90 64 71 78 85 70 80 90 100 68 77 86 95 62 68 74 80 68 778695 68778695 64 71 78 85 70 80 90 100 68 77 86 95 60 65 70 75 58 62 66 70 66 74 82 90 68 77 86 95 68 77 86 95 68 77 86 95 64 71 78 85 68 77 86 95 62 68 74 80 68 77 86 95 68 77 86 95 66 74 82 90 64 71 78 85 68 77 86 95 68 77 86 95 68 77 86 95 70 80 90 100 70 80 90 100 70 80 90 100 70 80 90 100 70 80 90 100 68 77 86 95 68 77 86 95 66 74 82 90 66 74 82 90 68 77 86 95 66 74 82 90 70 80 90 100 70 80 90 100 66 74 82 90 64 71 78 85 66 74 82 90 66 74 82 90 70 80 90 100 66 74 82 90 68 77 86 95 62 68 74 80 ViT (mAUC = 88.13) HLIP (mAUC = 90.05)",
    "file_name": ""
  },
  {
    "page": 17,
    "bbox": [
      152.55958557128906,
      138.42918395996094,
      458.93333435058594,
      450.3878173828125
    ],
    "caption": "Figure 13: Prospective evaluation on 83 head CT diagnoses.",
    "file_name": [
      "figures/fileoutpart42.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_77.png"
  },
  {
    "page": 17,
    "bbox": [
      152.55958557128906,
      138.42918395996094,
      458.93333435058594,
      450.3878173828125
    ],
    "caption": "Figure 13: Prospective evaluation on 83 head CT diagnoses.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      107.64100646972656,
      615.8345031738281,
      508.27972412109375,
      653.0344696044922
    ],
    "caption": "(2)",
    "file_name": [
      "figures/fileoutpart43.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_79.png"
  },
  {
    "page": 18,
    "bbox": [
      107.64100646972656,
      615.8345031738281,
      508.27972412109375,
      653.0344696044922
    ],
    "caption": "(2)",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      272.87091064453125,
      595.9529113769531,
      341.6266632080078,
      615.8934783935547
    ],
    "caption": "Ω(N 2 × c)",
    "file_name": [
      "figures/fileoutpart44.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_81.png"
  },
  {
    "page": 18,
    "bbox": [
      272.87091064453125,
      595.9529113769531,
      341.6266632080078,
      615.8934783935547
    ],
    "caption": "Ω(N 2 × c)",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      283.50115966796875,
      550.1099090576172,
      330.9905090332031,
      570.0494689941406
    ],
    "caption": "Scan Attention. The scan attention is M standard self-attention operations, each over tokens. Therefore replace N with in Equation (<>)2 and Equation (<>)3 resulting in and for I/O and compute complexity of each self-attention operations, therefore the total I/O and compute complexity of the scan attention are and respectively.",
    "file_name": [
      "figures/fileoutpart45.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_83.png"
  },
  {
    "page": 18,
    "bbox": [
      283.50115966796875,
      550.1099090576172,
      330.9905090332031,
      570.0494689941406
    ],
    "caption": "Scan Attention. The scan attention is M standard self-attention operations, each over tokens. Therefore replace N with in Equation (<>)2 and Equation (<>)3 resulting in and for I/O and compute complexity of each self-attention operations, therefore the total I/O and compute complexity of the scan attention are and respectively.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      464.2669982910156,
      487.8655548095703,
      473.8039245605469,
      503.22947692871094
    ],
    "caption": "M2",
    "file_name": [
      "figures/fileoutpart46.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_85.png"
  },
  {
    "page": 18,
    "bbox": [
      465.10899353027344,
      495.2235565185547,
      473.1637420654297,
      503.22947692871094
    ],
    "caption": "Ω( ×c)",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      464.46600341796875,
      487.8655548095703,
      473.8039245605469,
      495.87147521972656
    ],
    "caption": "M2",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      210.6179962158203,
      473.2945556640625,
      220.15492248535156,
      488.6584777832031
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": [
      "figures/fileoutpart47.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_88.png"
  },
  {
    "page": 18,
    "bbox": [
      211.4600067138672,
      480.6525573730469,
      219.51473999023438,
      488.6584777832031
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      210.81700134277344,
      473.2945556640625,
      220.15492248535156,
      481.30047607421875
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      383.9080047607422,
      473.2945556640625,
      445.24009704589844,
      489.13897705078125
    ],
    "caption": "Ω(× c),",
    "file_name": [
      "figures/fileoutpart48.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_91.png"
  },
  {
    "page": 18,
    "bbox": [
      396.8159942626953,
      480.6525573730469,
      407.2662658691406,
      489.13897705078125
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      396.1730041503906,
      473.2945556640625,
      409.1555938720703,
      481.50421142578125
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      420.83299255371094,
      475.763916015625,
      439.42515563964844,
      488.6584777832031
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      425.08299255371094,
      473.2945556640625,
      432.67750549316406,
      480.2683563232422
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      383.9080047607422,
      473.2945556640625,
      445.24009704589844,
      489.13897705078125
    ],
    "caption": "Ω(× c),",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      461.25,
      468.90989685058594,
      507.6645050048828,
      489.13897705078125
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": [
      "figures/fileoutpart49.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_97.png"
  },
  {
    "page": 18,
    "bbox": [
      474.15899658203125,
      480.6525573730469,
      484.60926818847656,
      489.13897705078125
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      473.51499938964844,
      473.2945556640625,
      486.4975891113281,
      481.50421142578125
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      461.25,
      468.90989685058594,
      507.6645050048828,
      489.13897705078125
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      252.83627319335938,
      444.70790100097656,
      321.36865234375,
      465.67420959472656
    ],
    "caption": "N2",
    "file_name": [
      "figures/fileoutpart50.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_101.png"
  },
  {
    "page": 18,
    "bbox": [
      265.1000061035156,
      456.45155334472656,
      276.7965850830078,
      465.67420959472656
    ],
    "caption": "M",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      266.4010009765625,
      449.0935516357422,
      273.99549865722656,
      456.0673522949219
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      252.83627319335938,
      444.70790100097656,
      321.36865234375,
      465.67420959472656
    ],
    "caption": "N2",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      338.24530029296875,
      444.70790100097656,
      387.9931640625,
      465.67420959472656
    ],
    "caption": "We conduct experiments comparing ViT [(<>)11], the 3D Swin Transformer implemented in MONAI [(<>)63], and our HLIP visual encoder. Since Swin is not compatible with multi-scan inputs, the experiment is conducted on a single-channel 3D input of shape (B,1,224,224,224), where B=1 denotes the batch size. Both ViT and HLIP use a ViT-Base architecture with patch_size=(16,16,16). The Swin Transformer is configured with dim=128, patch_size=(4,4,4), window_size=(7,7,7), depth=(2,2,12,2), and head=(4,8,16,32). Following the original Swin Transformer [(<>)14] while matching the model size.",
    "file_name": [
      "figures/fileoutpart51.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_105.png"
  },
  {
    "page": 18,
    "bbox": [
      350.5050048828125,
      456.45155334472656,
      362.2005920410156,
      465.67420959472656
    ],
    "caption": "M",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      351.80499267578125,
      449.0935516357422,
      359.3995056152344,
      456.0673522949219
    ],
    "caption": "Slice Attention. The slice attention is M × d standard self-attention operations, each over N M×d tokens. Therefore, similar to the scan attention the total I/O and compute complexity of the slice attention are and respectively.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      338.24530029296875,
      444.70790100097656,
      387.9931640625,
      465.67420959472656
    ],
    "caption": "We conduct experiments comparing ViT [(<>)11], the 3D Swin Transformer implemented in MONAI [(<>)63], and our HLIP visual encoder. Since Swin is not compatible with multi-scan inputs, the experiment is conducted on a single-channel 3D input of shape (B,1,224,224,224), where B=1 denotes the batch size. Both ViT and HLIP use a ViT-Base architecture with patch_size=(16,16,16). The Swin Transformer is configured with dim=128, patch_size=(4,4,4), window_size=(7,7,7), depth=(2,2,12,2), and head=(4,8,16,32). Following the original Swin Transformer [(<>)14] while matching the model size.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      160.01473999023438,
      400.29591369628906,
      236.32066345214844,
      421.7662048339844
    ],
    "caption": "E.2 Experiments",
    "file_name": [
      "figures/fileoutpart52.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_109.png"
  },
  {
    "page": 18,
    "bbox": [
      176.16600036621094,
      412.5425567626953,
      187.86158752441406,
      421.7662048339844
    ],
    "caption": "E.2 Experiments",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      172.2790069580078,
      400.29591369628906,
      192.74710083007812,
      413.1904754638672
    ],
    "caption": "E.2 Experiments",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      160.01473999023438,
      400.29591369628906,
      236.32066345214844,
      421.7662048339844
    ],
    "caption": "E.2 Experiments",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      253.19729614257812,
      400.29591369628906,
      310.7171630859375,
      421.7662048339844
    ],
    "caption": "We conduct experiments comparing ViT [(<>)11], the 3D Swin Transformer implemented in MONAI [(<>)63], and our HLIP visual encoder. Since Swin is not compatible with multi-scan inputs, the experiment is conducted on a single-channel 3D input of shape (B,1,224,224,224), where B=1 denotes the batch size. Both ViT and HLIP use a ViT-Base architecture with patch_size=(16,16,16). The Swin Transformer is configured with dim=128, patch_size=(4,4,4), window_size=(7,7,7), depth=(2,2,12,2), and head=(4,8,16,32). Following the original Swin Transformer [(<>)14] while matching the model size.",
    "file_name": [
      "figures/fileoutpart53.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_113.png"
  },
  {
    "page": 18,
    "bbox": [
      269.3419952392578,
      412.5425567626953,
      281.03858947753906,
      421.7662048339844
    ],
    "caption": "We conduct experiments comparing ViT [(<>)11], the 3D Swin Transformer implemented in MONAI [(<>)63], and our HLIP visual encoder. Since Swin is not compatible with multi-scan inputs, the experiment is conducted on a single-channel 3D input of shape (B,1,224,224,224), where B=1 denotes the batch size. Both ViT and HLIP use a ViT-Base architecture with patch_size=(16,16,16). The Swin Transformer is configured with dim=128, patch_size=(4,4,4), window_size=(7,7,7), depth=(2,2,12,2), and head=(4,8,16,32). Following the original Swin Transformer [(<>)14] while matching the model size.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      265.45599365234375,
      400.29591369628906,
      285.9241027832031,
      413.1904754638672
    ],
    "caption": "We conduct experiments comparing ViT [(<>)11], the 3D Swin Transformer implemented in MONAI [(<>)63], and our HLIP visual encoder. Since Swin is not compatible with multi-scan inputs, the experiment is conducted on a single-channel 3D input of shape (B,1,224,224,224), where B=1 denotes the batch size. Both ViT and HLIP use a ViT-Base architecture with patch_size=(16,16,16). The Swin Transformer is configured with dim=128, patch_size=(4,4,4), window_size=(7,7,7), depth=(2,2,12,2), and head=(4,8,16,32). Following the original Swin Transformer [(<>)14] while matching the model size.",
    "file_name": ""
  },
  {
    "page": 18,
    "bbox": [
      253.19729614257812,
      400.29591369628906,
      310.7171630859375,
      421.7662048339844
    ],
    "caption": "We conduct experiments comparing ViT [(<>)11], the 3D Swin Transformer implemented in MONAI [(<>)63], and our HLIP visual encoder. Since Swin is not compatible with multi-scan inputs, the experiment is conducted on a single-channel 3D input of shape (B,1,224,224,224), where B=1 denotes the batch size. Both ViT and HLIP use a ViT-Base architecture with patch_size=(16,16,16). The Swin Transformer is configured with dim=128, patch_size=(4,4,4), window_size=(7,7,7), depth=(2,2,12,2), and head=(4,8,16,32). Following the original Swin Transformer [(<>)14] while matching the model size.",
    "file_name": ""
  }
]