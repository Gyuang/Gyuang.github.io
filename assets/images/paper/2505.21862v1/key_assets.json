{
  "architecture": {
    "page": 1,
    "bbox": [
      111.78919982910156,
      537.0004272460938,
      495.6968688964844,
      718.1697235107422
    ],
    "caption": "Figure 1: Illustration of (a) an uncurated study for a patient. While previous work has relied on annotation and curation, HLIP enables language-image pre-training directly on such data. (b) HLIP achieves higher performance in zero-shot brain MRI disease classification while using less memory than the original ViT, which can only be trained with gradient checkpointing.",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/fig_01.png"
  },
  "results": {
    "page": 6,
    "bbox": [
      125.97274780273438,
      522.3224639892578,
      483.78953552246094,
      662.0122680664062
    ],
    "caption": "Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot inference [(<>)2] with both models. Since both models require 2D inputs, we generate predictions for each study by applying average pooling and max pooling to the outputs across all slices. We observe that max pooling consistently yields positive predictions and is only suitable for binary classification; accordingly, we adopt average pooling for both models. Additionally, on Pub-Brain-5-GT, we assess an upper-bound scenario in which predictions are made only on lesion-containing slices. Note that this setting is not practical, as it requires manually annotation for the lesion locations.",
    "file_name": [
      "tables/fileoutpart15.xlsx",
      "tables/fileoutpart16.png"
    ],
    "output_file": "assets/images/paper/2505.21862v1/table_164.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/2505.21862v1/fig_01.png)\n캡션: Figure 1: Illustration of (a) an uncurated study for a patient. While previous work has relied on annotation and curation, HLIP enables language-image pre-training directly on such data. (b) HLIP achieves higher performance in zero-shot brain MRI disease classification while using less memory than the original ViT, which can only be trained with gradient checkpointing.\n\n### Main Results Table\n![Results](/assets/images/paper/2505.21862v1/table_164.png)\n캡션: Baselines. Given that recent 3D models such as Merlin [(<>)4] and M3D [(<>)5] do not include brain MRI in their training sets, we evaluate two 2D foundation models on our benchmark: BiomedCLIP [(<>)23], pre-trained on 15 million figure-caption pairs from PubMed, and ConceptCLIP [(<>)24], pre-trained on 23 million such pairs. We use the prompt \"This brain MRI shows: {disease}.\" to perform zero-shot…"
}