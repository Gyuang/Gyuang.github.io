[
  {
    "page": 1,
    "bbox": [
      177.899169921875,
      408.44024658203125,
      437.44371032714844,
      457.4215393066406
    ],
    "caption": "(a)",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/fig_01.png"
  },
  {
    "page": 3,
    "bbox": [
      228.063232421875,
      551.3085174560547,
      389.78846740722656,
      591.1407775878906
    ],
    "caption": "Let us consider an input feature map x ∈ RCin×H×W with height H, weight W and channels Cin. The output y ∈ RCout×H×W of a self-attention layer is computed with the help of projected input using the following equation:",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/fig_02.png"
  },
  {
    "page": 3,
    "bbox": [
      228.063232421875,
      551.3085174560547,
      389.78846740722656,
      591.1407775878906
    ],
    "caption": "Let us consider an input feature map x ∈ RCin×H×W with height H, weight W and channels Cin. The output y ∈ RCout×H×W of a self-attention layer is computed with the help of projected input using the following equation:",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      151.9979705810547,
      172.130859375,
      463.35520935058594,
      389.5387268066406
    ],
    "caption": "Fig. 2. (a) The main architecture diagram of MedT which uses LoGo strategy for training. (b) The gated axial transformer layer which is used in MedT. (c) Gated Axial Attention layer which is the basic building block of both height and width gated multi-head attention blocks found in the gated axial transformer layer.",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/fig_04.png"
  },
  {
    "page": 3,
    "bbox": [
      151.9979705810547,
      172.130859375,
      463.35520935058594,
      389.5387268066406
    ],
    "caption": "Fig. 2. (a) The main architecture diagram of MedT which uses LoGo strategy for training. (b) The gated axial transformer layer which is used in MedT. (c) Gated Axial Attention layer which is the basic building block of both height and width gated multi-head attention blocks found in the gated axial transformer layer.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      182.36630249023438,
      397.5194091796875,
      435.48069763183594,
      437.3504180908203
    ],
    "caption": "where the formulation in Eq. (<>)2 follows the attention model proposed in [(<>)24] and rq , rk , rv ∈ RW ×W for the width-wise axial attention model. Note that Eq. (<>)2 describes the axial attention applied along the width axis of the tensor. A similar formulation is also used to apply axial attention along the height axis and together they form a single self-attention model that is computationally eﬃcient.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/fig_06.png"
  },
  {
    "page": 4,
    "bbox": [
      182.36630249023438,
      397.5194091796875,
      435.48069763183594,
      437.3504180908203
    ],
    "caption": "where the formulation in Eq. (<>)2 follows the attention model proposed in [(<>)24] and rq , rk , rv ∈ RW ×W for the width-wise axial attention model. Note that Eq. (<>)2 describes the axial attention applied along the width axis of the tensor. A similar formulation is also used to apply axial attention along the height axis and together they form a single self-attention model that is computationally eﬃcient.",
    "file_name": ""
  },
  {
    "page": 5,
    "bbox": [
      142.21702575683594,
      611.7957000732422,
      483.08192443847656,
      651.6264190673828
    ],
    "caption": "where the self-attention formula closely follows Eq. (<>)2 with added gating mechanism. Also, GQ,GK ,GV 1,GV 2 ∈ R are learnable parameters and together they create gating mechanism which control inﬂuence of the learned relative positional encodings have on encoding non-local context. Typically, if a relative positional encoding is learned accurately, the gating mechanism will assign it high weight compared to the ones which are not learned accurately. Fig (<>)2 (c) illustrates the feed-forward in a typical gated axial attention layer.",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/fig_08.png"
  },
  {
    "page": 5,
    "bbox": [
      142.21702575683594,
      611.7957000732422,
      483.08192443847656,
      651.6264190673828
    ],
    "caption": "where the self-attention formula closely follows Eq. (<>)2 with added gating mechanism. Also, GQ,GK ,GV 1,GV 2 ∈ R are learnable parameters and together they create gating mechanism which control inﬂuence of the learned relative positional encodings have on encoding non-local context. Typically, if a relative positional encoding is learned accurately, the gating mechanism will assign it high weight compared to the ones which are not learned accurately. Fig (<>)2 (c) illustrates the feed-forward in a typical gated axial attention layer.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      135.68499755859375,
      585.6746978759766,
      482.15875244140625,
      631.6824645996094
    ],
    "caption": "where w and h are the dimensions of the image, p(x,y) corresponds to the pixel in the image and pˆ(x,y) denotes the output prediction at a specific location (x,y). The training details are provided in the supplementary document.",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/fig_10.png"
  },
  {
    "page": 6,
    "bbox": [
      135.68499755859375,
      585.6746978759766,
      482.15875244140625,
      631.6824645996094
    ],
    "caption": "where w and h are the dimensions of the image, p(x,y) corresponds to the pixel in the image and pˆ(x,y) denotes the output prediction at a specific location (x,y). The training details are provided in the supplementary document.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      169.26467895507812,
      219.4582977294922,
      446.04119873046875,
      398.1034698486328
    ],
    "caption": "4 Conclusion",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/fig_12.png"
  },
  {
    "page": 14,
    "bbox": [
      134.76499938964844,
      351.5950012207031,
      480.59300231933594,
      501.45379638671875
    ],
    "caption": "Fig. 1. Qualitative Results. The red box highlights the regions where our proposed method outperforms the convolutional baselines.",
    "file_name": [
      "figures/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/fig_13.png"
  }
]