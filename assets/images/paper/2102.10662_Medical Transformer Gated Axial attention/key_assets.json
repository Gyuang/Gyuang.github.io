{
  "architecture": {
    "page": 14,
    "bbox": [
      134.76499938964844,
      351.5950012207031,
      480.59300231933594,
      501.45379638671875
    ],
    "caption": "Fig. 1. Qualitative Results. The red box highlights the regions where our proposed method outperforms the convolutional baselines.",
    "file_name": [
      "figures/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/fig_13.png"
  },
  "results": {
    "page": 6,
    "bbox": [
      181.70880126953125,
      252.44651794433594,
      435.5223693847656,
      362.59930419921875
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": [
      "tables/fileoutpart6.xlsx",
      "tables/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/table_01.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/fig_13.png)\n캡션: Fig. 1. Qualitative Results. The red box highlights the regions where our proposed method outperforms the convolutional baselines.\n\n### Main Results Table\n![Results](/assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/table_01.png)\n캡션: For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is…"
}