[
  {
    "page": 6,
    "bbox": [
      181.70880126953125,
      252.44651794433594,
      435.5223693847656,
      362.59930419921875
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": [
      "tables/fileoutpart6.xlsx",
      "tables/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/table_01.png"
  },
  {
    "page": 6,
    "bbox": [
      198.9530029296875,
      354.0119323730469,
      218.11752319335938,
      362.59930419921875
    ],
    "caption": "Convolutional Baselines",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      198.9530029296875,
      354.0119323730469,
      218.11752319335938,
      362.59930419921875
    ],
    "caption": "Convolutional Baselines",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      252.83200073242188,
      354.0119323730469,
      282.91029357910156,
      362.59930419921875
    ],
    "caption": "FCN [(<>)1]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      252.83200073242188,
      354.0119323730469,
      282.91029357910156,
      362.59930419921875
    ],
    "caption": "FCN [(<>)1]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      306.0493927001953,
      354.0119323730469,
      339.2245330810547,
      362.59930419921875
    ],
    "caption": "F1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      306.0493927001953,
      354.0119323730469,
      339.2245330810547,
      362.59930419921875
    ],
    "caption": "F1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      358.868896484375,
      354.0119323730469,
      377.0235900878906,
      362.59930419921875
    ],
    "caption": "F1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      358.868896484375,
      354.0119323730469,
      377.0235900878906,
      362.59930419921875
    ],
    "caption": "F1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      396.0910949707031,
      354.0119323730469,
      430.42567443847656,
      362.59930419921875
    ],
    "caption": "F1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      396.0910949707031,
      354.0119323730469,
      430.42567443847656,
      362.59930419921875
    ],
    "caption": "F1",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      184.21910095214844,
      317.11090087890625,
      232.85601806640625,
      334.8392028808594
    ],
    "caption": "Fully Attention Baseline",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      184.21910095214844,
      317.11090087890625,
      232.85601806640625,
      334.8392028808594
    ],
    "caption": "Fully Attention Baseline",
    "file_name": ""
  },
  {
    "page": -1,
    "bbox": null,
    "caption": "",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      306.02439880371094,
      344.5372314453125,
      316.7586364746094,
      353.1246032714844
    ],
    "caption": "82.79",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      306.02439880371094,
      344.5372314453125,
      316.7586364746094,
      353.1246032714844
    ],
    "caption": "82.79",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      326.7532958984375,
      344.5372314453125,
      341.01075744628906,
      353.1246032714844
    ],
    "caption": "75.02",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      326.7532958984375,
      344.5372314453125,
      341.01075744628906,
      353.1246032714844
    ],
    "caption": "75.02",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      351.3356018066406,
      344.5372314453125,
      362.06982421875,
      353.1246032714844
    ],
    "caption": "66.61",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      351.3356018066406,
      344.5372314453125,
      362.06982421875,
      353.1246032714844
    ],
    "caption": "66.61",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      372.0635986328125,
      344.5372314453125,
      386.321044921875,
      353.1246032714844
    ],
    "caption": "50.84",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      372.0635986328125,
      344.5372314453125,
      386.321044921875,
      353.1246032714844
    ],
    "caption": "50.84",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      396.6459045410156,
      344.5372314453125,
      407.380126953125,
      353.1246032714844
    ],
    "caption": "28.84",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      396.6459045410156,
      344.5372314453125,
      407.380126953125,
      353.1246032714844
    ],
    "caption": "28.84",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      417.3739013671875,
      344.5372314453125,
      431.63134765625,
      353.1246032714844
    ],
    "caption": "28.71",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      417.3739013671875,
      344.5372314453125,
      431.63134765625,
      353.1246032714844
    ],
    "caption": "28.71",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      253.53610229492188,
      335.39453125,
      282.2080841064453,
      343.9819030761719
    ],
    "caption": "U-Net (<>)[17]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      253.53610229492188,
      335.39453125,
      282.2080841064453,
      343.9819030761719
    ],
    "caption": "U-Net (<>)[17]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      335.39453125,
      321.0810546875,
      343.9819030761719
    ],
    "caption": "85.37",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      335.39453125,
      321.0810546875,
      343.9819030761719
    ],
    "caption": "85.37",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      324.18870544433594,
      335.39453125,
      343.5701599121094,
      343.9819030761719
    ],
    "caption": "79.31",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      324.18870544433594,
      335.39453125,
      343.5701599121094,
      343.9819030761719
    ],
    "caption": "79.31",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      335.39453125,
      366.3922576904297,
      343.9819030761719
    ],
    "caption": "77.78",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      335.39453125,
      366.3922576904297,
      343.9819030761719
    ],
    "caption": "77.78",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      335.39453125,
      388.88136291503906,
      343.9819030761719
    ],
    "caption": "65.34",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      335.39453125,
      388.88136291503906,
      343.9819030761719
    ],
    "caption": "65.34",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      335.39453125,
      411.7025604248047,
      343.9819030761719
    ],
    "caption": "79.43",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      335.39453125,
      411.7025604248047,
      343.9819030761719
    ],
    "caption": "79.43",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      335.39453125,
      434.191650390625,
      343.9819030761719
    ],
    "caption": "65.99",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      335.39453125,
      434.191650390625,
      343.9819030761719
    ],
    "caption": "65.99",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      249.4239959716797,
      321.63922119140625,
      286.31683349609375,
      330.2266082763672
    ],
    "caption": "31]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      249.4239959716797,
      321.63922119140625,
      286.31683349609375,
      330.2266082763672
    ],
    "caption": "31]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      321.63922119140625,
      321.0810546875,
      330.2266082763672
    ],
    "caption": "86.59",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      321.63922119140625,
      321.0810546875,
      330.2266082763672
    ],
    "caption": "86.59",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      324.18870544433594,
      321.63922119140625,
      343.5701599121094,
      330.2266082763672
    ],
    "caption": "79.95",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      324.18870544433594,
      321.63922119140625,
      343.5701599121094,
      330.2266082763672
    ],
    "caption": "79.95",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      321.63922119140625,
      366.3922576904297,
      330.2266082763672
    ],
    "caption": "78.03",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      321.63922119140625,
      366.3922576904297,
      330.2266082763672
    ],
    "caption": "78.03",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      321.63922119140625,
      388.88136291503906,
      330.2266082763672
    ],
    "caption": "65.55",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      321.63922119140625,
      388.88136291503906,
      330.2266082763672
    ],
    "caption": "65.55",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      321.63922119140625,
      411.7025604248047,
      330.2266082763672
    ],
    "caption": "79.49",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      321.63922119140625,
      411.7025604248047,
      330.2266082763672
    ],
    "caption": "79.49",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      321.63922119140625,
      434.191650390625,
      330.2266082763672
    ],
    "caption": "66.04",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      321.63922119140625,
      434.191650390625,
      330.2266082763672
    ],
    "caption": "66.04",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      243.4449005126953,
      307.9665222167969,
      292.2987365722656,
      316.5539093017578
    ],
    "caption": "27]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      280.6070251464844,
      307.9665222167969,
      292.2987365722656,
      316.5539093017578
    ],
    "caption": "27]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      243.4449005126953,
      307.9665222167969,
      292.2987365722656,
      316.5539093017578
    ],
    "caption": "27]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      307.9665222167969,
      321.0810546875,
      316.5539093017578
    ],
    "caption": "87.50",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      307.9665222167969,
      321.0810546875,
      316.5539093017578
    ],
    "caption": "87.50",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      324.18870544433594,
      307.9665222167969,
      343.5701599121094,
      316.5539093017578
    ],
    "caption": "79.61",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      324.18870544433594,
      307.9665222167969,
      343.5701599121094,
      316.5539093017578
    ],
    "caption": "79.61",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      307.9665222167969,
      366.3922576904297,
      316.5539093017578
    ],
    "caption": "78.83",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      307.9665222167969,
      366.3922576904297,
      316.5539093017578
    ],
    "caption": "78.83",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      307.9665222167969,
      388.88136291503906,
      316.5539093017578
    ],
    "caption": "65.95",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      307.9665222167969,
      388.88136291503906,
      316.5539093017578
    ],
    "caption": "65.95",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      307.9665222167969,
      411.7025604248047,
      316.5539093017578
    ],
    "caption": "79.49",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      307.9665222167969,
      411.7025604248047,
      316.5539093017578
    ],
    "caption": "79.49",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      307.9665222167969,
      434.191650390625,
      316.5539093017578
    ],
    "caption": "66.07",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      307.9665222167969,
      434.191650390625,
      316.5539093017578
    ],
    "caption": "66.07",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      243.36900329589844,
      298.8239288330078,
      292.37245178222656,
      307.41131591796875
    ],
    "caption": "Axial Attention U-Net (<>)[24]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      280.6807403564453,
      298.8239288330078,
      292.37245178222656,
      307.41131591796875
    ],
    "caption": "Axial Attention U-Net (<>)[24]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      243.36900329589844,
      298.8239288330078,
      292.37245178222656,
      307.41131591796875
    ],
    "caption": "Axial Attention U-Net (<>)[24]",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      298.8239288330078,
      321.0810546875,
      307.41131591796875
    ],
    "caption": "87.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      298.8239288330078,
      321.0810546875,
      307.41131591796875
    ],
    "caption": "87.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      324.18870544433594,
      298.8239288330078,
      343.5701599121094,
      307.41131591796875
    ],
    "caption": "80.14",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      324.18870544433594,
      298.8239288330078,
      343.5701599121094,
      307.41131591796875
    ],
    "caption": "80.14",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      298.8239288330078,
      366.3922576904297,
      307.41131591796875
    ],
    "caption": "76.26",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      298.8239288330078,
      366.3922576904297,
      307.41131591796875
    ],
    "caption": "76.26",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      298.8239288330078,
      388.88136291503906,
      307.41131591796875
    ],
    "caption": "63.03",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      298.8239288330078,
      388.88136291503906,
      307.41131591796875
    ],
    "caption": "63.03",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      298.8239288330078,
      411.7025604248047,
      307.41131591796875
    ],
    "caption": "76.83",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      298.8239288330078,
      411.7025604248047,
      307.41131591796875
    ],
    "caption": "76.83",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      298.8239288330078,
      434.191650390625,
      307.41131591796875
    ],
    "caption": "62.49",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      298.8239288330078,
      434.191650390625,
      307.41131591796875
    ],
    "caption": "62.49",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      181.70880126953125,
      280.2082977294922,
      235.37246704101562,
      297.9366149902344
    ],
    "caption": "Proposed",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      181.70880126953125,
      280.2082977294922,
      235.37246704101562,
      297.9366149902344
    ],
    "caption": "Proposed",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      240.56170654296875,
      280.2082977294922,
      295.18284606933594,
      297.9366149902344
    ],
    "caption": "Gated Axial Attn.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      240.56170654296875,
      280.2082977294922,
      295.18284606933594,
      297.9366149902344
    ],
    "caption": "Gated Axial Attn.",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      284.7366180419922,
      321.0810546875,
      293.3240051269531
    ],
    "caption": "88.39",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      284.7366180419922,
      321.0810546875,
      293.3240051269531
    ],
    "caption": "88.39",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      324.18870544433594,
      284.7366180419922,
      343.5701599121094,
      293.3240051269531
    ],
    "caption": "80.7",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      324.18870544433594,
      284.7366180419922,
      343.5701599121094,
      293.3240051269531
    ],
    "caption": "80.7",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      284.7366180419922,
      366.3922576904297,
      293.3240051269531
    ],
    "caption": "79.91",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      284.7366180419922,
      366.3922576904297,
      293.3240051269531
    ],
    "caption": "79.91",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      284.7366180419922,
      388.88136291503906,
      293.3240051269531
    ],
    "caption": "67.85",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      284.7366180419922,
      388.88136291503906,
      293.3240051269531
    ],
    "caption": "67.85",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      284.7366180419922,
      411.7025604248047,
      293.3240051269531
    ],
    "caption": "76.44",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      284.7366180419922,
      411.7025604248047,
      293.3240051269531
    ],
    "caption": "76.44",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      284.7366180419922,
      434.191650390625,
      293.3240051269531
    ],
    "caption": "62.01",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      284.7366180419922,
      434.191650390625,
      293.3240051269531
    ],
    "caption": "62.01",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      192.03610229492188,
      261.58921813964844,
      225.0391845703125,
      270.1766052246094
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      192.03610229492188,
      261.58921813964844,
      225.0391845703125,
      270.1766052246094
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      236.1575927734375,
      270.73182678222656,
      299.58306884765625,
      279.3192138671875
    ],
    "caption": "LoGo",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      236.1575927734375,
      270.73182678222656,
      299.58306884765625,
      279.3192138671875
    ],
    "caption": "LoGo",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      270.73182678222656,
      321.0810546875,
      279.3192138671875
    ],
    "caption": "88.54",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      270.73182678222656,
      321.0810546875,
      279.3192138671875
    ],
    "caption": "88.54",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      326.11090087890625,
      270.73182678222656,
      341.64747619628906,
      279.3192138671875
    ],
    "caption": "80.84",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      326.11090087890625,
      270.73182678222656,
      341.64747619628906,
      279.3192138671875
    ],
    "caption": "80.84",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      270.73182678222656,
      366.3922576904297,
      279.3192138671875
    ],
    "caption": "79.68",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      270.73182678222656,
      366.3922576904297,
      279.3192138671875
    ],
    "caption": "79.68",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      270.73182678222656,
      388.88136291503906,
      279.3192138671875
    ],
    "caption": "67.69",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      270.73182678222656,
      388.88136291503906,
      279.3192138671875
    ],
    "caption": "67.69",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      270.73182678222656,
      411.7025604248047,
      279.3192138671875
    ],
    "caption": "79.56",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      270.73182678222656,
      411.7025604248047,
      279.3192138671875
    ],
    "caption": "79.56",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      270.73182678222656,
      434.191650390625,
      279.3192138671875
    ],
    "caption": "66.17",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      270.73182678222656,
      434.191650390625,
      279.3192138671875
    ],
    "caption": "66.17",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      257.6739959716797,
      261.58921813964844,
      278.0727844238281,
      270.1766052246094
    ],
    "caption": "MedT",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      257.6739959716797,
      261.58921813964844,
      278.0727844238281,
      270.1766052246094
    ],
    "caption": "MedT",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      261.58921813964844,
      321.0810546875,
      270.1766052246094
    ],
    "caption": "88.84",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      301.69960021972656,
      261.58921813964844,
      321.0810546875,
      270.1766052246094
    ],
    "caption": "88.84",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      324.18870544433594,
      261.58921813964844,
      343.5701599121094,
      270.1766052246094
    ],
    "caption": "81.34",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      324.18870544433594,
      261.58921813964844,
      343.5701599121094,
      270.1766052246094
    ],
    "caption": "81.34",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      261.58921813964844,
      366.3922576904297,
      270.1766052246094
    ],
    "caption": "81.02",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      347.01080322265625,
      261.58921813964844,
      366.3922576904297,
      270.1766052246094
    ],
    "caption": "81.02",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      261.58921813964844,
      388.88136291503906,
      270.1766052246094
    ],
    "caption": "69.61",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      369.49989318847656,
      261.58921813964844,
      388.88136291503906,
      270.1766052246094
    ],
    "caption": "69.61",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      261.58921813964844,
      411.7025604248047,
      270.1766052246094
    ],
    "caption": "79.55",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      392.32110595703125,
      261.58921813964844,
      411.7025604248047,
      270.1766052246094
    ],
    "caption": "79.55",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      261.58921813964844,
      434.191650390625,
      270.1766052246094
    ],
    "caption": "66.17",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      414.81019592285156,
      261.58921813964844,
      434.191650390625,
      270.1766052246094
    ],
    "caption": "66.17",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      256.79139709472656,
      252.44651794433594,
      278.9555358886719,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      256.79139709472656,
      252.44651794433594,
      278.9555358886719,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      300.3748016357422,
      252.44651794433594,
      322.41175842285156,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      300.3748016357422,
      252.44651794433594,
      322.41175842285156,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      322.86390686035156,
      252.44651794433594,
      344.90086364746094,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      322.86390686035156,
      252.44651794433594,
      344.90086364746094,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      345.6851043701172,
      252.44651794433594,
      367.72206115722656,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      345.6851043701172,
      252.44651794433594,
      367.72206115722656,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      368.17430114746094,
      252.44651794433594,
      390.2112579345703,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      368.17430114746094,
      252.44651794433594,
      390.2112579345703,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      390.9963073730469,
      252.44651794433594,
      413.03326416015625,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      390.9963073730469,
      252.44651794433594,
      413.03326416015625,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      413.4853973388672,
      252.44651794433594,
      435.5223693847656,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 6,
    "bbox": [
      413.4853973388672,
      252.44651794433594,
      435.5223693847656,
      261.0339050292969
    ],
    "caption": "For quantitative analysis, we use F1 and IoU scores for comparison. The quantitative results are tabulated in Table (<>)1. It can be noted that for datasets with relatively more images like Brain US, fully attention (transformer) based baseline performs better than convolutional baselines. For GlaS and MoNuSeg datasets, convolutional baselines perform better than fully attention baselines as it is diﬃcult to train fully attention models with less data [(<>)6]. The proposed method is able to overcome such issue with the help of gated axial attention and LoGo both individually perform better than the other methods. Our ﬁnal architecture MedT performs better than Gated axial attention, LoGo and all the previous methods. The improvements over fully attention baselines are 0.92",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      163.97689819335938,
      414.82684326171875,
      453.02760314941406,
      438.7806854248047
    ],
    "caption": "Table 2. Ablation Study",
    "file_name": [
      "tables/fileoutpart8.xlsx",
      "tables/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/table_145.png"
  },
  {
    "page": 7,
    "bbox": [
      164.4936065673828,
      427.14784240722656,
      190.96238708496094,
      434.7225799560547
    ],
    "caption": "F1 Score",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      164.4936065673828,
      427.14784240722656,
      190.96238708496094,
      434.7225799560547
    ],
    "caption": "F1 Score",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      192.16749572753906,
      427.14784240722656,
      224.62498474121094,
      434.7225799560547
    ],
    "caption": "85.37",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      192.16749572753906,
      427.14784240722656,
      224.62498474121094,
      434.7225799560547
    ],
    "caption": "85.37",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      225.32049560546875,
      427.18455505371094,
      268.44578552246094,
      434.75927734375
    ],
    "caption": "87.5",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      225.32049560546875,
      427.18455505371094,
      268.44578552246094,
      434.75927734375
    ],
    "caption": "87.5",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      269.12840270996094,
      427.18455505371094,
      317.3868713378906,
      434.75927734375
    ],
    "caption": "87.92",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      269.12840270996094,
      427.18455505371094,
      317.3868713378906,
      434.75927734375
    ],
    "caption": "87.92",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      318.06219482421875,
      427.18455505371094,
      373.8756561279297,
      434.75927734375
    ],
    "caption": "88.39",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      318.06219482421875,
      427.18455505371094,
      373.8756561279297,
      434.75927734375
    ],
    "caption": "88.39",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      374.560302734375,
      423.1639709472656,
      395.7906036376953,
      438.7609405517578
    ],
    "caption": "87.67",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      374.560302734375,
      423.1639709472656,
      395.7906036376953,
      438.7609405517578
    ],
    "caption": "87.67",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      396.4866027832031,
      423.1837158203125,
      414.2158203125,
      438.7806854248047
    ],
    "caption": "77.55",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      396.4866027832031,
      431.20594787597656,
      414.2158203125,
      438.7806854248047
    ],
    "caption": "only",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      398.2437286376953,
      423.1837158203125,
      412.4784240722656,
      430.7386932373047
    ],
    "caption": "77.55",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      414.889892578125,
      427.14784240722656,
      432.83628845214844,
      434.7028350830078
    ],
    "caption": "88.54",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      414.889892578125,
      427.14784240722656,
      432.83628845214844,
      434.7028350830078
    ],
    "caption": "88.54",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      433.5281066894531,
      427.14784240722656,
      453.02760314941406,
      434.7028350830078
    ],
    "caption": "88.84",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      433.5281066894531,
      427.14784240722656,
      453.02760314941406,
      434.7028350830078
    ],
    "caption": "88.84",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      163.97689819335938,
      414.82684326171875,
      191.4854736328125,
      422.4015808105469
    ],
    "caption": "Network",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      163.97689819335938,
      414.82684326171875,
      191.4854736328125,
      422.4015808105469
    ],
    "caption": "Network",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      199.87120056152344,
      414.82684326171875,
      216.92257690429688,
      422.3818359375
    ],
    "caption": "U-Net (<>)[17]",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      199.87120056152344,
      414.82684326171875,
      216.92257690429688,
      422.3818359375
    ],
    "caption": "U-Net (<>)[17]",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      240.04229736328125,
      414.82684326171875,
      253.71104431152344,
      422.3818359375
    ],
    "caption": "Res-UNet (<>)[27]",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      240.04229736328125,
      414.82684326171875,
      253.71104431152344,
      422.3818359375
    ],
    "caption": "Res-UNet (<>)[27]",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      284.72210693359375,
      414.82684326171875,
      301.7734680175781,
      422.3818359375
    ],
    "caption": "Axial UNet (<>)[24]",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      284.72210693359375,
      414.82684326171875,
      301.7734680175781,
      422.3818359375
    ],
    "caption": "Axial UNet (<>)[24]",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      337.43800354003906,
      414.82684326171875,
      354.48936462402344,
      422.3818359375
    ],
    "caption": "Gated Axial UNet",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      337.43800354003906,
      414.82684326171875,
      354.48936462402344,
      422.3818359375
    ],
    "caption": "Gated Axial UNet",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      376.6506042480469,
      414.82684326171875,
      393.70196533203125,
      422.3818359375
    ],
    "caption": "Global only",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      376.6506042480469,
      414.82684326171875,
      393.70196533203125,
      422.3818359375
    ],
    "caption": "Global only",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      396.8153991699219,
      414.82684326171875,
      413.8667755126953,
      422.3818359375
    ],
    "caption": "only",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      396.8153991699219,
      414.82684326171875,
      413.8667755126953,
      422.3818359375
    ],
    "caption": "only",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      415.3361053466797,
      414.82684326171875,
      432.38746643066406,
      422.3818359375
    ],
    "caption": "LoGo",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      415.3361053466797,
      414.82684326171875,
      432.38746643066406,
      422.3818359375
    ],
    "caption": "LoGo",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      434.75010681152344,
      414.82684326171875,
      451.8014678955078,
      422.3818359375
    ],
    "caption": "MedT",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      434.75010681152344,
      414.82684326171875,
      451.8014678955078,
      422.3818359375
    ],
    "caption": "MedT",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      157.5449981689453,
      367.8343963623047,
      460.0584411621094,
      411.40283203125
    ],
    "caption": "4.2 Number of Parameters",
    "file_name": [
      "tables/fileoutpart11.xlsx",
      "tables/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/table_183.png"
  },
  {
    "page": 13,
    "bbox": [
      158.24899291992188,
      390.10040283203125,
      194.30288696289062,
      400.3938293457031
    ],
    "caption": "F1 Score",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      158.24899291992188,
      390.10040283203125,
      194.30288696289062,
      400.3938293457031
    ],
    "caption": "F1 Score",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      195.95399475097656,
      390.10040283203125,
      240.17628479003906,
      400.3938293457031
    ],
    "caption": "85.37",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      195.95399475097656,
      390.10040283203125,
      240.17628479003906,
      400.3938293457031
    ],
    "caption": "85.37",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      241.12295532226562,
      384.6734619140625,
      282.26976013183594,
      404.5968017578125
    ],
    "caption": "87.5",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      251.78399658203125,
      395.6304016113281,
      269.3761291503906,
      404.5968017578125
    ],
    "caption": "UNet []",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      268.2552795410156,
      384.6734619140625,
      277.4727325439453,
      393.6398620605469
    ],
    "caption": "87.5",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      241.12295532226562,
      384.6734619140625,
      282.26976013183594,
      394.9668884277344
    ],
    "caption": "87.5",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      283.21961975097656,
      384.6734619140625,
      324.36643981933594,
      405.923828125
    ],
    "caption": "87.92",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      283.21961975097656,
      384.6734619140625,
      324.36643981933594,
      405.923828125
    ],
    "caption": "87.92",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      325.32000732421875,
      379.1955108642578,
      352.2012634277344,
      411.40283203125
    ],
    "caption": "88.39",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      325.32000732421875,
      379.1955108642578,
      352.2012634277344,
      411.40283203125
    ],
    "caption": "88.39",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      353.1490020751953,
      384.6734619140625,
      382.07460021972656,
      405.923828125
    ],
    "caption": "87.67",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      353.1490020751953,
      384.6734619140625,
      382.07460021972656,
      405.923828125
    ],
    "caption": "87.67",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      383.0229949951172,
      384.6734619140625,
      407.1515808105469,
      405.923828125
    ],
    "caption": "77.55",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      383.0229949951172,
      395.6304016113281,
      407.1515808105469,
      405.923828125
    ],
    "caption": "only",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      385.39013671875,
      384.6734619140625,
      404.7844543457031,
      394.9668884277344
    ],
    "caption": "77.55",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      408.0970001220703,
      390.10040283203125,
      432.5483703613281,
      400.3938293457031
    ],
    "caption": "88.54",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      408.0970001220703,
      390.10040283203125,
      432.5483703613281,
      400.3938293457031
    ],
    "caption": "88.54",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      433.4909973144531,
      390.10040283203125,
      460.0584411621094,
      400.3938293457031
    ],
    "caption": "88.84",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      433.4909973144531,
      390.10040283203125,
      460.0584411621094,
      400.3938293457031
    ],
    "caption": "88.84",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      157.5449981689453,
      367.8343963623047,
      195.0155792236328,
      378.12782287597656
    ],
    "caption": "4.2 Number of Parameters",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      157.5449981689453,
      367.8343963623047,
      195.0155792236328,
      378.12782287597656
    ],
    "caption": "4.2 Number of Parameters",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      206.4499969482422,
      367.8343963623047,
      229.68194580078125,
      378.12782287597656
    ],
    "caption": "4.2 Number of Parameters",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      206.4499969482422,
      367.8343963623047,
      229.68194580078125,
      378.12782287597656
    ],
    "caption": "4.2 Number of Parameters",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      252.38699340820312,
      367.8343963623047,
      271.0102081298828,
      378.12782287597656
    ],
    "caption": "4.2 Number of Parameters",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      252.38699340820312,
      367.8343963623047,
      271.0102081298828,
      378.12782287597656
    ],
    "caption": "4.2 Number of Parameters",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      292.1820068359375,
      367.8343963623047,
      315.4139404296875,
      378.12782287597656
    ],
    "caption": "Axial UNet [(<>)24]",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      292.1820068359375,
      367.8343963623047,
      315.4139404296875,
      378.12782287597656
    ],
    "caption": "Axial UNet [(<>)24]",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      327.14500427246094,
      367.8343963623047,
      350.37693786621094,
      378.12782287597656
    ],
    "caption": "Gated Axial UNet",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      327.14500427246094,
      367.8343963623047,
      350.37693786621094,
      378.12782287597656
    ],
    "caption": "Gated Axial UNet",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      355.9969940185547,
      367.8343963623047,
      379.22894287109375,
      378.12782287597656
    ],
    "caption": "Global only",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      355.9969940185547,
      367.8343963623047,
      379.22894287109375,
      378.12782287597656
    ],
    "caption": "Global only",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      383.4709930419922,
      367.8343963623047,
      406.70294189453125,
      378.12782287597656
    ],
    "caption": "only",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      383.4709930419922,
      367.8343963623047,
      406.70294189453125,
      378.12782287597656
    ],
    "caption": "only",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      408.7050018310547,
      367.8343963623047,
      431.9369354248047,
      378.12782287597656
    ],
    "caption": "LoGo",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      408.7050018310547,
      367.8343963623047,
      431.9369354248047,
      378.12782287597656
    ],
    "caption": "LoGo",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      435.156005859375,
      367.8343963623047,
      458.387939453125,
      378.12782287597656
    ],
    "caption": "MedT",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      435.156005859375,
      367.8343963623047,
      458.387939453125,
      378.12782287597656
    ],
    "caption": "MedT",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      136.15899658203125,
      200.63040161132812,
      491.22544860839844,
      255.5558319091797
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": [
      "tables/fileoutpart13.xlsx",
      "tables/fileoutpart14.png"
    ],
    "output_file": "assets/images/paper/2102.10662_Medical Transformer Gated Axial attention/table_223.png"
  },
  {
    "page": 13,
    "bbox": [
      142.02099609375,
      234.25439453125,
      178.07489013671875,
      244.54782104492188
    ],
    "caption": "Parameters",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      142.02099609375,
      234.25439453125,
      178.07489013671875,
      244.54782104492188
    ],
    "caption": "Parameters",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      184.88299560546875,
      234.25439453125,
      219.25120544433594,
      244.54782104492188
    ],
    "caption": "12.5 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      184.88299560546875,
      234.25439453125,
      219.25120544433594,
      244.54782104492188
    ],
    "caption": "12.5 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      220.19500732421875,
      234.25439453125,
      264.4172821044922,
      244.54782104492188
    ],
    "caption": "3.13 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      220.19500732421875,
      234.25439453125,
      264.4172821044922,
      244.54782104492188
    ],
    "caption": "3.13 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      265.36500549316406,
      228.8264617919922,
      309.5872802734375,
      250.0768280029297
    ],
    "caption": "1.3 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      265.36500549316406,
      228.8264617919922,
      309.5872802734375,
      250.0768280029297
    ],
    "caption": "1.3 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      310.53395080566406,
      228.8264617919922,
      351.6807556152344,
      248.7498016357422
    ],
    "caption": "5.32 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      321.19500732421875,
      239.7834014892578,
      338.78712463378906,
      248.7498016357422
    ],
    "caption": "UNet []",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      337.66627502441406,
      228.8264617919922,
      346.88372802734375,
      237.79286193847656
    ],
    "caption": "5.32 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      310.53395080566406,
      228.8264617919922,
      351.6807556152344,
      239.11988830566406
    ],
    "caption": "5.32 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      352.6346893310547,
      223.34852600097656,
      393.78150939941406,
      255.5558319091797
    ],
    "caption": "1.34 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      352.6346893310547,
      223.34852600097656,
      393.78150939941406,
      255.5558319091797
    ],
    "caption": "1.34 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      394.72862243652344,
      228.8264617919922,
      435.8754425048828,
      250.0768280029297
    ],
    "caption": "1.3 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      394.72862243652344,
      228.8264617919922,
      435.8754425048828,
      250.0768280029297
    ],
    "caption": "1.3 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      436.82899475097656,
      223.34852600097656,
      463.71026611328125,
      255.5558319091797
    ],
    "caption": "1.3 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      438.3981170654297,
      234.3054656982422,
      462.1411437988281,
      244.59889221191406
    ],
    "caption": "UNet",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      436.82899475097656,
      245.2624053955078,
      463.71026611328125,
      255.5558319091797
    ],
    "caption": "Axial",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      438.3981170654297,
      223.34852600097656,
      462.1411437988281,
      233.64195251464844
    ],
    "caption": "1.3 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      464.6580047607422,
      234.25439453125,
      491.22544860839844,
      244.54782104492188
    ],
    "caption": "1.4 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      464.6580047607422,
      234.25439453125,
      491.22544860839844,
      244.54782104492188
    ],
    "caption": "1.4 M",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      136.15899658203125,
      211.98739624023438,
      183.93197631835938,
      222.28082275390625
    ],
    "caption": "F1 Score",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      136.15899658203125,
      211.98739624023438,
      183.93197631835938,
      222.28082275390625
    ],
    "caption": "F1 Score",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      186.9949951171875,
      211.98739624023438,
      217.14002990722656,
      222.28082275390625
    ],
    "caption": "82.79",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      186.9949951171875,
      211.98739624023438,
      217.14002990722656,
      222.28082275390625
    ],
    "caption": "82.79",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      227.23599243164062,
      211.98739624023438,
      257.3720703125,
      222.28082275390625
    ],
    "caption": "87.71",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      227.23599243164062,
      211.98739624023438,
      257.3720703125,
      222.28082275390625
    ],
    "caption": "87.71",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      274.70899963378906,
      211.98739624023438,
      300.24530029296875,
      222.28082275390625
    ],
    "caption": "85.37",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      274.70899963378906,
      211.98739624023438,
      300.24530029296875,
      222.28082275390625
    ],
    "caption": "85.37",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      316.03900146484375,
      211.98739624023438,
      346.1840362548828,
      222.28082275390625
    ],
    "caption": "87.73",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      316.03900146484375,
      211.98739624023438,
      346.1840362548828,
      222.28082275390625
    ],
    "caption": "87.73",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      358.1369934082031,
      211.98739624023438,
      388.28204345703125,
      222.28082275390625
    ],
    "caption": "87.5",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      358.1369934082031,
      211.98739624023438,
      388.28204345703125,
      222.28082275390625
    ],
    "caption": "87.5",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      402.53900146484375,
      211.98739624023438,
      428.07530212402344,
      222.28082275390625
    ],
    "caption": "87.92",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      402.53900146484375,
      211.98739624023438,
      428.07530212402344,
      222.28082275390625
    ],
    "caption": "87.92",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      437.5030059814453,
      211.98739624023438,
      463.039306640625,
      222.28082275390625
    ],
    "caption": "88.39",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      437.5030059814453,
      211.98739624023438,
      463.039306640625,
      222.28082275390625
    ],
    "caption": "88.39",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      465.1719970703125,
      211.98739624023438,
      490.70831298828125,
      222.28082275390625
    ],
    "caption": "88.84",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      465.1719970703125,
      211.98739624023438,
      490.70831298828125,
      222.28082275390625
    ],
    "caption": "88.84",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      141.31700134277344,
      200.63040161132812,
      178.78758239746094,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      141.31700134277344,
      200.63040161132812,
      178.78758239746094,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      190.4499969482422,
      200.63040161132812,
      213.68194580078125,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      190.4499969482422,
      200.63040161132812,
      213.68194580078125,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      230.6909942626953,
      200.63040161132812,
      253.92294311523438,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      230.6909942626953,
      200.63040161132812,
      253.92294311523438,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      275.8609924316406,
      200.63040161132812,
      299.0929412841797,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      275.8609924316406,
      200.63040161132812,
      299.0929412841797,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      319.49400329589844,
      200.63040161132812,
      342.72593688964844,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      319.49400329589844,
      200.63040161132812,
      342.72593688964844,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      363.89599609375,
      200.63040161132812,
      382.5192108154297,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      363.89599609375,
      200.63040161132812,
      382.5192108154297,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      403.6909942626953,
      200.63040161132812,
      426.9229431152344,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      403.6909942626953,
      200.63040161132812,
      426.9229431152344,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      438.6540069580078,
      200.63040161132812,
      461.8859405517578,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      438.6540069580078,
      200.63040161132812,
      461.8859405517578,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      464.73500061035156,
      200.63040161132812,
      491.15000915527344,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  },
  {
    "page": 13,
    "bbox": [
      464.73500061035156,
      200.63040161132812,
      491.15000915527344,
      210.923828125
    ],
    "caption": "Although MedT is a multi-branch network, we reduce the number of parameters by using only 2 layers of encoder and decoder in the global branch and making the local branch operate on only patches of image. Also, the proposed gated axial attention block adds only 4 more learnable parameters to the layer.",
    "file_name": ""
  }
]