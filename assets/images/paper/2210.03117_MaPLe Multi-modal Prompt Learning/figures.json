[
  {
    "page": 1,
    "bbox": [
      74.86300659179688,
      520.8028106689453,
      269.7063293457031,
      719.852294921875
    ],
    "caption": "Figure 1. Comparison of MaPLe with standard prompt learning methods. (a) Existing methods adopt uni-modal prompting techniques to fine-tune CLIP representations as prompts are learned only in a single branch of CLIP (language or vision). (b) MaPLe introduces branch-aware hierarchical prompts that adapt both language and vision branches simultaneously for improved generalization. (c) MaPLe surpasses state-of-the-art methods on 11 diverse image recognition datasets for novel class generalization task.",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_01.png"
  },
  {
    "page": 1,
    "bbox": [
      284.82371520996094,
      538.3296356201172,
      517.5981140136719,
      718.8676452636719
    ],
    "caption": "Figure 1. Comparison of MaPLe with standard prompt learning methods. (a) Existing methods adopt uni-modal prompting techniques to fine-tune CLIP representations as prompts are learned only in a single branch of CLIP (language or vision). (b) MaPLe introduces branch-aware hierarchical prompts that adapt both language and vision branches simultaneously for improved generalization. (c) MaPLe surpasses state-of-the-art methods on 11 diverse image recognition datasets for novel class generalization task.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_02.png"
  },
  {
    "page": 1,
    "bbox": [
      276.2266845703125,
      519.7689056396484,
      518.5981750488281,
      534.3909454345703
    ],
    "caption": "Figure 1. Comparison of MaPLe with standard prompt learning methods. (a) Existing methods adopt uni-modal prompting techniques to fine-tune CLIP representations as prompts are learned only in a single branch of CLIP (language or vision). (b) MaPLe introduces branch-aware hierarchical prompts that adapt both language and vision branches simultaneously for improved generalization. (c) MaPLe surpasses state-of-the-art methods on 11 diverse image recognition datasets for novel class generalization task.",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_03.png"
  },
  {
    "page": 2,
    "bbox": [
      103.17924499511719,
      534.6233367919922,
      492.0497131347656,
      720.8506927490234
    ],
    "caption": "Figure 2. Overview of our proposed MaPLe (Multi-modal Prompt Learning) framework for prompt learning in V-L models. MaPLe tunes both vision and language branches where only the context prompts are learned, while the rest of the model is frozen. MaPLe conditions the vision prompts on language prompts via a V-L coupling function F to induce mutual synergy between the two modalities. Our framework uses deep contextual prompting where separate context prompts are learned across multiple transformer blocks.",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_04.png"
  },
  {
    "page": 3,
    "bbox": [
      55.050140380859375,
      555.5392303466797,
      540.2433776855469,
      720.0128021240234
    ],
    "caption": "Figure 3. t-SNE plots of image embeddings in uni-modal prompting method Co-CoOp, and MaPLe on 3 diverse image recognition datasets. MaPLe shows better separability in both base and novel classes.",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_05.png"
  },
  {
    "page": 3,
    "bbox": [
      76.30473327636719,
      471.50352478027344,
      262.65887451171875,
      490.2759552001953
    ],
    "caption": "To obtain the final image representation x, the class token cK of last transformer layer (VK ) is projected to a common V-L latent embedding space via ImageProj,",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_06.png"
  },
  {
    "page": 3,
    "bbox": [
      76.30473327636719,
      471.50352478027344,
      262.65887451171875,
      490.2759552001953
    ],
    "caption": "To obtain the final image representation x, the class token cK of last transformer layer (VK ) is projected to a common V-L latent embedding space via ImageProj,",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      94.42405700683594,
      414.52149963378906,
      182.97235107421875,
      426.9663543701172
    ],
    "caption": "Encoding Text: CLIP text encoder generates feature representations for text description by tokenizing the words and projecting them to word embeddings W0 = At each stage, Wi is input to the (i + 1)th transformer layer of text encoding branch (Li+1),",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_08.png"
  },
  {
    "page": 3,
    "bbox": [
      94.42405700683594,
      414.52149963378906,
      182.97235107421875,
      426.9663543701172
    ],
    "caption": "Encoding Text: CLIP text encoder generates feature representations for text description by tokenizing the words and projecting them to word embeddings W0 = At each stage, Wi is input to the (i + 1)th transformer layer of text encoding branch (Li+1),",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      50.11158752441406,
      352.6295166015625,
      172.24850463867188,
      372.07264709472656
    ],
    "caption": "[Wi] = Li(Wi−1) i = 1, 2, · · · , K.",
    "file_name": [
      "figures/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_10.png"
  },
  {
    "page": 3,
    "bbox": [
      60.279998779296875,
      364.0664978027344,
      65.99867248535156,
      372.07264709472656
    ],
    "caption": "The final text representation z is obtained by projecting the text embeddings corresponding to the last token of the last transformer block LK to a common V-L latent embedding space via TextProj,",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      60.014984130859375,
      357.9781951904297,
      63.99021911621094,
      364.9521942138672
    ],
    "caption": "The final text representation z is obtained by projecting the text embeddings corresponding to the last token of the last transformer block LK to a common V-L latent embedding space via TextProj,",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      76.5780029296875,
      364.0664978027344,
      82.29667663574219,
      372.07264709472656
    ],
    "caption": "The final text representation z is obtained by projecting the text embeddings corresponding to the last token of the last transformer block LK to a common V-L latent embedding space via TextProj,",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      76.30601501464844,
      357.9781951904297,
      80.28125,
      364.9521942138672
    ],
    "caption": "The final text representation z is obtained by projecting the text embeddings corresponding to the last token of the last transformer block LK to a common V-L latent embedding space via TextProj,",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      112.24699401855469,
      364.0664978027344,
      120.30197143554688,
      372.07264709472656
    ],
    "caption": "[Wi] = Li(Wi−1) i = 1, 2, · · · , K.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      111.98199462890625,
      357.9781951904297,
      115.95721435546875,
      364.9521942138672
    ],
    "caption": "[Wi] = Li(Wi−1) i = 1, 2, · · · , K.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      146.4459991455078,
      359.17771911621094,
      167.51412963867188,
      371.263671875
    ],
    "caption": "[Wi] = Li(Wi−1) i = 1, 2, · · · , K.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      50.11158752441406,
      352.6295166015625,
      172.24850463867188,
      372.07264709472656
    ],
    "caption": "[Wi] = Li(Wi−1) i = 1, 2, · · · , K.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      90.17384338378906,
      309.02052307128906,
      248.79640197753906,
      327.5317687988281
    ],
    "caption": "The final text representation z is obtained by projecting the text embeddings corresponding to the last token of the last transformer block LK to a common V-L latent embedding space via TextProj,",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_19.png"
  },
  {
    "page": 3,
    "bbox": [
      90.17384338378906,
      309.02052307128906,
      248.79640197753906,
      327.5317687988281
    ],
    "caption": "The final text representation z is obtained by projecting the text embeddings corresponding to the last token of the last transformer block LK to a common V-L latent embedding space via TextProj,",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      96.74713134765625,
      233.7565155029297,
      242.21546936035156,
      253.6966552734375
    ],
    "caption": "Zero-shot Classification: For zero-shot classification, text prompts are hand-crafted with class labels y ∈ {1, 2, . . . C} (e.g., ‘a photo of a <category>’) having C classes. Prediction yˆ corresponding to the image I having the highest cosine similarity score (sim(·)) is calculated with a temperature parameter τ,",
    "file_name": [
      "figures/fileoutpart11.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_21.png"
  },
  {
    "page": 3,
    "bbox": [
      96.74713134765625,
      233.7565155029297,
      242.21546936035156,
      253.6966552734375
    ],
    "caption": "Zero-shot Classification: For zero-shot classification, text prompts are hand-crafted with class labels y ∈ {1, 2, . . . C} (e.g., ‘a photo of a <category>’) having C classes. Prediction yˆ corresponding to the image I having the highest cosine similarity score (sim(·)) is calculated with a temperature parameter τ,",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      100.66600036621094,
      108.03952026367188,
      238.30323791503906,
      154.67779541015625
    ],
    "caption": "To efficiently fine-tune CLIP for downstream image recognition tasks, we explore the potential of multi-modal prompt",
    "file_name": [
      "figures/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_23.png"
  },
  {
    "page": 3,
    "bbox": [
      100.66600036621094,
      108.03952026367188,
      238.30323791503906,
      154.67779541015625
    ],
    "caption": "To efficiently fine-tune CLIP for downstream image recognition tasks, we explore the potential of multi-modal prompt",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      324.5769958496094,
      107.426513671875,
      547.6004486083984,
      126.19680786132812
    ],
    "caption": "Here [·, ·] refers to the concatenation operation. After Jth transformer layer, the subsequent layers process previous",
    "file_name": [
      "figures/fileoutpart13.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_25.png"
  },
  {
    "page": 3,
    "bbox": [
      324.5769958496094,
      107.426513671875,
      547.6004486083984,
      126.19680786132812
    ],
    "caption": "Here [·, ·] refers to the concatenation operation. After Jth transformer layer, the subsequent layers process previous",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      57.3946533203125,
      668.3022613525391,
      288.8600769042969,
      698.1644439697266
    ],
    "caption": "When J = 1, the learnable tokens P are only applied at the input of first transformer layer, and this deep language prompting technique degenerates to CoOp [(<>)49].",
    "file_name": [
      "figures/fileoutpart14.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_27.png"
  },
  {
    "page": 4,
    "bbox": [
      57.3946533203125,
      668.3022613525391,
      288.8600769042969,
      698.1644439697266
    ],
    "caption": "When J = 1, the learnable tokens P are only applied at the input of first transformer layer, and this deep language prompting technique degenerates to CoOp [(<>)49].",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      98.81448364257812,
      560.9449005126953,
      168.81829833984375,
      580.5033874511719
    ],
    "caption": "[ci, Ei, ] = Vi([ci−1, Ei−1, ˜Pi−1]) i = 1, 2, · · · , J, [cj, Ej, ˜Pj] = Vj([cj−1, Ej−1, ˜Pj−1]) j = J + 1, · · · , K, x = ImageProj(cK).",
    "file_name": [
      "figures/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_29.png"
  },
  {
    "page": 4,
    "bbox": [
      136.531005859375,
      571.8836822509766,
      145.5208282470703,
      579.3553466796875
    ],
    "caption": "[ci, Ei, ] = Vi([ci−1, Ei−1, ˜Pi−1]) i = 1, 2, · · · , J, [cj, Ej, ˜Pj] = Vj([cj−1, Ej−1, ˜Pj−1]) j = J + 1, · · · , K, x = ImageProj(cK).",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      150.43299865722656,
      572.3815460205078,
      153.933837890625,
      579.3553466796875
    ],
    "caption": "[ci, Ei, ] = Vi([ci−1, Ei−1, ˜Pi−1]) i = 1, 2, · · · , J, [cj, Ej, ˜Pj] = Vj([cj−1, Ej−1, ˜Pj−1]) j = J + 1, · · · , K, x = ImageProj(cK).",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      150.43299865722656,
      566.1748657226562,
      163.341552734375,
      573.1486663818359
    ],
    "caption": "[ci, Ei, ] = Vi([ci−1, Ei−1, ˜Pi−1]) i = 1, 2, · · · , J, [cj, Ej, ˜Pj] = Vj([cj−1, Ej−1, ˜Pj−1]) j = J + 1, · · · , K, x = ImageProj(cK).",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      98.81448364257812,
      560.9449005126953,
      168.81829833984375,
      580.5033874511719
    ],
    "caption": "[ci, Ei, ] = Vi([ci−1, Ei−1, ˜Pi−1]) i = 1, 2, · · · , J, [cj, Ej, ˜Pj] = Vj([cj−1, Ej−1, ˜Pj−1]) j = J + 1, · · · , K, x = ImageProj(cK).",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      50.11199951171875,
      478.6395568847656,
      290.0193786621094,
      523.5574951171875
    ],
    "caption": "Our deep prompting provides the flexibility to learn prompts across different feature hierarchies within the ViT architecture. We find that sharing prompts across stages is better compared to independent prompts as features are more correlated due to successive transformer block processing. Thus, the later stages do not provide independently-learned complimentary prompts as compared to the early stages.",
    "file_name": [
      "figures/fileoutpart16.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_34.png"
  },
  {
    "page": 4,
    "bbox": [
      50.11199951171875,
      478.6395568847656,
      290.0193786621094,
      523.5574951171875
    ],
    "caption": "Our deep prompting provides the flexibility to learn prompts across different feature hierarchies within the ViT architecture. We find that sharing prompts across stages is better compared to independent prompts as features are more correlated due to successive transformer block processing. Thus, the later stages do not provide independently-learned complimentary prompts as compared to the early stages.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      308.86199951171875,
      640.841552734375,
      551.9427185058594,
      684.7134094238281
    ],
    "caption": "Unlike independent V-L prompting, explicit conditioning of P˜ on P helps learn prompts in a shared embedding space between the two branches, thus improving mutual synergy.",
    "file_name": [
      "figures/fileoutpart17.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_36.png"
  },
  {
    "page": 4,
    "bbox": [
      308.86199951171875,
      640.841552734375,
      551.9427185058594,
      684.7134094238281
    ],
    "caption": "Unlike independent V-L prompting, explicit conditioning of P˜ on P helps learn prompts in a shared embedding space between the two branches, thus improving mutual synergy.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      50.11199951171875,
      116.86599731445312,
      286.35174560546875,
      188.61288452148438
    ],
    "caption": "Figure 4. Ablation on prompt depth (left) and prompt length (right) in MaPLe. We report average results on the held-out validation sets of all datasets.",
    "file_name": [
      "figures/fileoutpart50.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_38.png"
  },
  {
    "page": 7,
    "bbox": [
      311.3529968261719,
      625.1280059814453,
      441.28887939453125,
      705.7736968994141
    ],
    "caption": "complexity of MaPLe in comparison with other approaches. Although MaPLe utilizes multi-modal prompts, its overall FLOPS (Floating Point Operations) exceeds only by 0.1% over CoOp and Co-CoOp. The independent V-L prompting also provides comparable FLOP count. In terms of inference speed, Co-CoOp is significantly slower and the FPS (Frames Per Second) remains constant as the batch size increases. In contrast, MaPLe has no such overhead and provides much better inference and training speeds. Further, MaPLe provides better convergence as it requires only half training epochs as compared to Co-CoOp (5 vs 10 epochs). MaPLe adds about 2.85% training parameters on top of CLIP. To study if the performance gain is mainly attributed to more parameters, we experiment with MaPLe†, which uses a unified V-L coupling function for all layer prompts. MaPLe† with about 9x lesser parameters than MaPLe also improves over existing methods. We also ablate by comparing MaPLe with heavier CoCoOp in Appendix (<>)D.",
    "file_name": [
      "figures/fileoutpart51.png"
    ],
    "output_file": "assets/images/paper/2210.03117_MaPLe Multi-modal Prompt Learning/fig_39.png"
  }
]