{
  "architecture": {
    "page": 2,
    "bbox": [
      332.9700012207031,
      400.15589904785156,
      518.4032592773438,
      487.5260925292969
    ],
    "caption": "and global but also can give different weights to different views. To be specific, first, to better obtain the complete information of the 3D image, we cut each image along three views: sagittal view (along x-axis), coronal view (along y-axis), and axial view (along z-axis). We use n images from each view as the model input. Then similar to ViT, ADAPT also uses the image patch and patch embedding method to embed the 2D images into 3 sequences including 3 × n slices with guide patch embedding layer xguide, then concatenates them together as the input to the transformer encoders (Eq. (<>)1). The guide patch embedding aims to reshape the whole sequence into a sequence of flattened 2D patches that has the same shape as the sequence after the normal patch, which means the guide patch embedding has the input channel with the number 3 × n. Thus, we can use 3D models to extract the global information and add it to each special slice sequence. Because our model mainly focuses on 2D slice dimension, guide patch embedding can help to keep the relative position information of 3D brain.",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_03.png"
  },
  "results": {
    "page": 5,
    "bbox": [
      55.43998718261719,
      484.5911560058594,
      545.6352996826172,
      722.1910095214844
    ],
    "caption": "Table 1. Comparison of accuracy various 3D CNN-based and transformer-based models on multi-institutional Alzheimer’s disease dataset.",
    "file_name": [
      "tables/fileoutpart17.xlsx",
      "tables/fileoutpart18.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/table_01.png"
  },
  "results_2": null,
  "markdown": "### Main Architecture\n![Architecture](/assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_03.png)\n캡션: and global but also can give different weights to different views. To be specific, first, to better obtain the complete information of the 3D image, we cut each image along three views: sagittal view (along x-axis), coronal view (along y-axis), and axial view (along z-axis). We use n images from each view as the model input. Then similar to ViT, ADAPT also uses the image patch and patch embedding…\n\n### Main Results Table\n![Results](/assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/table_01.png)\n캡션: Table 1. Comparison of accuracy various 3D CNN-based and transformer-based models on multi-institutional Alzheimer’s disease dataset."
}