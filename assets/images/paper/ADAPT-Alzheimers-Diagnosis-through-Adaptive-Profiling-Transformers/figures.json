[
  {
    "page": -1,
    "bbox": [
      309.93409729003906,
      426.1931457519531,
      540.1299896240234,
      618.0511169433594
    ],
    "caption": "",
    "file_name": [
      "figures/fileoutpart0.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_01.png"
  },
  {
    "page": -1,
    "bbox": [
      309.93409729003906,
      426.1931457519531,
      540.1299896240234,
      618.0511169433594
    ],
    "caption": "",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      332.9700012207031,
      400.15589904785156,
      518.4032592773438,
      487.5260925292969
    ],
    "caption": "and global but also can give different weights to different views. To be specific, first, to better obtain the complete information of the 3D image, we cut each image along three views: sagittal view (along x-axis), coronal view (along y-axis), and axial view (along z-axis). We use n images from each view as the model input. Then similar to ViT, ADAPT also uses the image patch and patch embedding method to embed the 2D images into 3 sequences including 3 × n slices with guide patch embedding layer xguide, then concatenates them together as the input to the transformer encoders (Eq. (<>)1). The guide patch embedding aims to reshape the whole sequence into a sequence of flattened 2D patches that has the same shape as the sequence after the normal patch, which means the guide patch embedding has the input channel with the number 3 × n. Thus, we can use 3D models to extract the global information and add it to each special slice sequence. Because our model mainly focuses on 2D slice dimension, guide patch embedding can help to keep the relative position information of 3D brain.",
    "file_name": [
      "figures/fileoutpart1.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_03.png"
  },
  {
    "page": 2,
    "bbox": [
      332.9700012207031,
      400.15589904785156,
      518.4032592773438,
      487.5260925292969
    ],
    "caption": "and global but also can give different weights to different views. To be specific, first, to better obtain the complete information of the 3D image, we cut each image along three views: sagittal view (along x-axis), coronal view (along y-axis), and axial view (along z-axis). We use n images from each view as the model input. Then similar to ViT, ADAPT also uses the image patch and patch embedding method to embed the 2D images into 3 sequences including 3 × n slices with guide patch embedding layer xguide, then concatenates them together as the input to the transformer encoders (Eq. (<>)1). The guide patch embedding aims to reshape the whole sequence into a sequence of flattened 2D patches that has the same shape as the sequence after the normal patch, which means the guide patch embedding has the input channel with the number 3 × n. Thus, we can use 3D models to extract the global information and add it to each special slice sequence. Because our model mainly focuses on 2D slice dimension, guide patch embedding can help to keep the relative position information of 3D brain.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      353.3376922607422,
      251.86489868164062,
      498.0187072753906,
      272.0934753417969
    ],
    "caption": "Ss l = SAE(Ss l-1) l = 1...LSAE",
    "file_name": [
      "figures/fileoutpart2.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_05.png"
  },
  {
    "page": 2,
    "bbox": [
      353.3376922607422,
      251.86489868164062,
      498.0187072753906,
      272.0934753417969
    ],
    "caption": "Ss l = SAE(Ss l-1) l = 1...LSAE",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      357.16746520996094,
      234.3238525390625,
      492.9429626464844,
      249.51278686523438
    ],
    "caption": "The Dimension-specific Self-Attention Encoders (DS-AE) also aim to learn the attention of the slice itself. However, compared with SAE, these encoders focus more on the relationship between the slices from the same dimension sequence. These encoders can better extract the local features from the same view dimension. This will fill the gap that transformers cannot capture the local features well however the local embeddings of different brain tissues (such as hip-pocampus and cortex) are really important in AD diagnosis. In the following equation, t means the three different views.",
    "file_name": [
      "figures/fileoutpart3.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_07.png"
  },
  {
    "page": 2,
    "bbox": [
      357.16746520996094,
      234.3238525390625,
      492.9429626464844,
      249.51278686523438
    ],
    "caption": "The Dimension-specific Self-Attention Encoders (DS-AE) also aim to learn the attention of the slice itself. However, compared with SAE, these encoders focus more on the relationship between the slices from the same dimension sequence. These encoders can better extract the local features from the same view dimension. This will fill the gap that transformers cannot capture the local features well however the local embeddings of different brain tissues (such as hip-pocampus and cortex) are really important in AD diagnosis. In the following equation, t means the three different views.",
    "file_name": ""
  },
  {
    "page": 2,
    "bbox": [
      332.1210021972656,
      72.09036254882812,
      519.2510986328125,
      101.1524658203125
    ],
    "caption": "The Dimension-specific Self-Attention Encoders (DS-AE) also aim to learn the attention of the slice itself. However, compared with SAE, these encoders focus more on the relationship between the slices from the same dimension sequence. These encoders can better extract the local features from the same view dimension. This will fill the gap that transformers cannot capture the local features well however the local embeddings of different brain tissues (such as hip-pocampus and cortex) are really important in AD diagnosis. In the following equation, t means the three different views.",
    "file_name": [
      "figures/fileoutpart4.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_09.png"
  },
  {
    "page": 2,
    "bbox": [
      332.1210021972656,
      72.09036254882812,
      519.2510986328125,
      101.1524658203125
    ],
    "caption": "The Dimension-specific Self-Attention Encoders (DS-AE) also aim to learn the attention of the slice itself. However, compared with SAE, these encoders focus more on the relationship between the slices from the same dimension sequence. These encoders can better extract the local features from the same view dimension. This will fill the gap that transformers cannot capture the local features well however the local embeddings of different brain tissues (such as hip-pocampus and cortex) are really important in AD diagnosis. In the following equation, t means the three different views.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      54.94200134277344,
      561.4723968505859,
      541.9380035400391,
      734.9019927978516
    ],
    "caption": "Figure 2. The detailed architecture for our ADAPT. ADAPT consists of four main components: Self-Attention Encoders (SAE) across three views, Dimension-specific Self-Attention Encoders (DS-AE), Intra-dimension Cross-Attention Encoders (IntraCAE), Inter-dimension Cross-Attention Encoders (InterCAE). The figure shows more details in sagittal view for illustration purposes. In practice, the model will adaptively attend to different views.",
    "file_name": [
      "figures/fileoutpart5.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_11.png"
  },
  {
    "page": 3,
    "bbox": [
      54.94200134277344,
      561.4723968505859,
      541.9380035400391,
      734.9019927978516
    ],
    "caption": "Figure 2. The detailed architecture for our ADAPT. ADAPT consists of four main components: Self-Attention Encoders (SAE) across three views, Dimension-specific Self-Attention Encoders (DS-AE), Intra-dimension Cross-Attention Encoders (IntraCAE), Inter-dimension Cross-Attention Encoders (InterCAE). The figure shows more details in sagittal view for illustration purposes. In practice, the model will adaptively attend to different views.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      68.36201477050781,
      351.6123046875,
      268.378662109375,
      395.61865234375
    ],
    "caption": "After combining the features between slices of the same dimension independently, the last Inter-dimension Cross-Attention Encoders (InterCAE) are proposed to learn the inter-dimension relationship among different sequences from different views. This is corresponding to the SAE layer and will gather the global features together. Inter-CAE will apply cross embedding mechanism again into the view-dependent embeddings.",
    "file_name": [
      "figures/fileoutpart6.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_13.png"
  },
  {
    "page": 3,
    "bbox": [
      68.36201477050781,
      351.6123046875,
      268.378662109375,
      395.61865234375
    ],
    "caption": "After combining the features between slices of the same dimension independently, the last Inter-dimension Cross-Attention Encoders (InterCAE) are proposed to learn the inter-dimension relationship among different sequences from different views. This is corresponding to the SAE layer and will gather the global features together. Inter-CAE will apply cross embedding mechanism again into the view-dependent embeddings.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      88.4176025390625,
      197.67330932617188,
      258.95635986328125,
      241.67965698242188
    ],
    "caption": "Finally, the [class] tokens of the output from three dimensions will be averaged and sent to Layer Norm and classification MLP head as Eq. ?? and Eq. ?? to get the final diagnosis result: AD or normal.",
    "file_name": [
      "figures/fileoutpart7.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_15.png"
  },
  {
    "page": 3,
    "bbox": [
      88.4176025390625,
      197.67330932617188,
      258.95635986328125,
      241.67965698242188
    ],
    "caption": "Finally, the [class] tokens of the output from three dimensions will be averaged and sent to Layer Norm and classification MLP head as Eq. ?? and Eq. ?? to get the final diagnosis result: AD or normal.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      339.6304473876953,
      285.2995300292969,
      511.7413635253906,
      321.95664978027344
    ],
    "caption": "In a more formal way, the traditional attention mechanism is shown as Eq. (<>)9. After fusing these two embeddings, the K matrix of the first embedding will consist of the K value corresponding to the [class] token from the first embedding, and the K matrix corresponding to fusion embedding, similarly for Q matrix. After the matrix calculation, Eq. (<>)11 fuses the information from two embeddings while keeping some unique information from the special [class] token.",
    "file_name": [
      "figures/fileoutpart8.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_17.png"
  },
  {
    "page": 3,
    "bbox": [
      339.6304473876953,
      285.2995300292969,
      511.7413635253906,
      321.95664978027344
    ],
    "caption": "In a more formal way, the traditional attention mechanism is shown as Eq. (<>)9. After fusing these two embeddings, the K matrix of the first embedding will consist of the K value corresponding to the [class] token from the first embedding, and the K matrix corresponding to fusion embedding, similarly for Q matrix. After the matrix calculation, Eq. (<>)11 fuses the information from two embeddings while keeping some unique information from the special [class] token.",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      373.1300048828125,
      143.53050231933594,
      476.0247802734375,
      170.82965087890625
    ],
    "caption": "K1 = [Kclass1 , K1 + K2], Q1 = [Qclass1 , Q1 + Q2] (10) Q1K T 1 =  Qclass1 Kclass1 (Q1 + Q2)Kclass1 Qclass1 (K1 + K2) (Q1 + Q2)(K1 + K2)  (11)",
    "file_name": [
      "figures/fileoutpart9.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_19.png"
  },
  {
    "page": 3,
    "bbox": [
      373.1300048828125,
      143.53050231933594,
      476.0247802734375,
      170.82965087890625
    ],
    "caption": "K1 = [Kclass1 , K1 + K2], Q1 = [Qclass1 , Q1 + Q2] (10) Q1K T 1 =  Qclass1 Kclass1 (Q1 + Q2)Kclass1 Qclass1 (K1 + K2) (Q1 + Q2)(K1 + K2)  (11)",
    "file_name": ""
  },
  {
    "page": 3,
    "bbox": [
      310.4579315185547,
      74.1396484375,
      544.598388671875,
      133.21676635742188
    ],
    "caption": "H = softmax( QKT √ dk )V",
    "file_name": [
      "figures/fileoutpart10.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_21.png"
  },
  {
    "page": 3,
    "bbox": [
      310.4579315185547,
      74.1396484375,
      544.598388671875,
      133.21676635742188
    ],
    "caption": "H = softmax( QKT √ dk )V",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      57.881011962890625,
      591.7979888916016,
      287.0593719482422,
      725.9353790283203
    ],
    "caption": "Figure 3. The visualization of Alzheimer’s Disease (AD) image, Normal Control (NC) image and Mild Cognitive Impairment (MCI) image. The left is the raw image and the right is the augmented image. The cerebral ventricle (red circle) has a significant difference in size for AD and NC.",
    "file_name": [
      "figures/fileoutpart11.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_23.png"
  },
  {
    "page": 4,
    "bbox": [
      57.881011962890625,
      591.7979888916016,
      287.0593719482422,
      725.9353790283203
    ],
    "caption": "Figure 3. The visualization of Alzheimer’s Disease (AD) image, Normal Control (NC) image and Mild Cognitive Impairment (MCI) image. The left is the raw image and the right is the augmented image. The cerebral ventricle (red circle) has a significant difference in size for AD and NC.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      62.29454040527344,
      314.5716247558594,
      285.07232666015625,
      337.0417785644531
    ],
    "caption": "[f ⊕ bN ](x, y) = max (s,t)∈bN {f(x − s, y − t) + bN (s, t)}",
    "file_name": [
      "figures/fileoutpart12.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_25.png"
  },
  {
    "page": 4,
    "bbox": [
      62.29454040527344,
      314.5716247558594,
      285.07232666015625,
      337.0417785644531
    ],
    "caption": "[f ⊕ bN ](x, y) = max (s,t)∈bN {f(x − s, y − t) + bN (s, t)}",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      62.28968811035156,
      286.0176239013672,
      285.07232666015625,
      308.4878234863281
    ],
    "caption": "We apply atropy expansion augmentation to AD images and MCI images and label the resultant images as AD; on the other hand, we apply atropy reduction augmentation to Normal Control(NC) images and MCI images and label the resultant images as NC, where MCI is the prodromal stage of AD. The visualization of morphology augmentation is shown in Fig. (<>)3.",
    "file_name": [
      "figures/fileoutpart13.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_27.png"
  },
  {
    "page": 4,
    "bbox": [
      62.28968811035156,
      286.0176239013672,
      285.07232666015625,
      308.4878234863281
    ],
    "caption": "We apply atropy expansion augmentation to AD images and MCI images and label the resultant images as AD; on the other hand, we apply atropy reduction augmentation to Normal Control(NC) images and MCI images and label the resultant images as NC, where MCI is the prodromal stage of AD. The visualization of morphology augmentation is shown in Fig. (<>)3.",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      350.55909729003906,
      641.6334991455078,
      498.5968933105469,
      668.9336547851562
    ],
    "caption": "Algorithm 1 ADAPT Training Strategy",
    "file_name": [
      "figures/fileoutpart14.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_29.png"
  },
  {
    "page": 4,
    "bbox": [
      350.55909729003906,
      641.6334991455078,
      498.5968933105469,
      668.9336547851562
    ],
    "caption": "Algorithm 1 ADAPT Training Strategy",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      356.7521057128906,
      234.45614624023438,
      494.6204528808594,
      301.7688751220703
    ],
    "caption": "4. Experiments",
    "file_name": [
      "figures/fileoutpart15.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_31.png"
  },
  {
    "page": 4,
    "bbox": [
      356.7521057128906,
      234.45614624023438,
      494.6204528808594,
      301.7688751220703
    ],
    "caption": "4. Experiments",
    "file_name": ""
  },
  {
    "page": 4,
    "bbox": [
      356.4479522705078,
      229.79342651367188,
      482.51629638671875,
      262.65167236328125
    ],
    "caption": "4. Experiments",
    "file_name": [
      "figures/fileoutpart16.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_33.png"
  },
  {
    "page": 4,
    "bbox": [
      356.4479522705078,
      229.79342651367188,
      482.51629638671875,
      262.65167236328125
    ],
    "caption": "4. Experiments",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      314.889892578125,
      591.0951995849609,
      533.6148529052734,
      724.8302307128906
    ],
    "caption": "Figure 4. Attention map for Normal Control result. Each line corresponds to one view dimension: saggital, coronal and axial.",
    "file_name": [
      "figures/fileoutpart25.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_35.png"
  },
  {
    "page": 7,
    "bbox": [
      314.889892578125,
      591.0951995849609,
      533.6148529052734,
      724.8302307128906
    ],
    "caption": "Figure 4. Attention map for Normal Control result. Each line corresponds to one view dimension: saggital, coronal and axial.",
    "file_name": ""
  },
  {
    "page": 7,
    "bbox": [
      289.9654235839844,
      368.9385528564453,
      543.6596374511719,
      522.0119934082031
    ],
    "caption": "5. Conclusions",
    "file_name": [
      "figures/fileoutpart26.png"
    ],
    "output_file": "assets/images/paper/ADAPT-Alzheimers-Diagnosis-through-Adaptive-Profiling-Transformers/fig_37.png"
  },
  {
    "page": 7,
    "bbox": [
      289.9654235839844,
      368.9385528564453,
      543.6596374511719,
      522.0119934082031
    ],
    "caption": "5. Conclusions",
    "file_name": ""
  }
]