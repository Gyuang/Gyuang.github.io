{
  "papers": [
    {
      "title": "CLIP: Learning Transferable Visual Representations from Natural Language",
      "excerpt": "CLIP 논문 요약",
      "additional_tags": "Contrastive Learning, Zero-shot",
      "introduction": "CLIP introduces a method for learning visual concepts from natural language descriptions.",
      "related_work_vlm": "Previous vision-language models were limited in their ability to generalize...",
      "specific_domain": "Computer Vision",
      "related_work_domain": "Traditional computer vision approaches relied on supervised learning...",
      "architecture_description": "CLIP consists of an image encoder and text encoder trained with contrastive loss.",
      "architecture_image": "clip_architecture.png",
      "key_components": "Image encoder (ResNet or ViT), Text encoder (Transformer), Contrastive learning objective",
      "training_strategy": "Contrastive learning on 400M image-text pairs from the internet",
      "datasets": "400M image-text pairs, evaluated on ImageNet, CIFAR-100, etc.",
      "results": "Zero-shot performance competitive with supervised ResNet-50 on ImageNet",
      "ablation_studies": "Studies on different architectures, training data scale, and loss functions",
      "conclusion": "CLIP demonstrates the power of learning from natural language supervision.",
      "key_takeaways": "1. Natural language provides rich supervision signal\n2. Zero-shot transfer capabilities\n3. Scalable to large datasets"
    }
  ]
}