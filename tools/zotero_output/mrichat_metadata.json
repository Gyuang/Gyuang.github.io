{
  "extraction_date": "2025-07-25T15:44:26.926162",
  "total_papers": 22,
  "papers": [
    {
      "item_id": 148,
      "title": "(PDF) Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes",
      "abstract": "PDF | Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These... | Find, read and cite all the research you need on ResearchGate",
      "date": "2025-06-10 2025-06-10",
      "doi": "",
      "url": "https://www.researchgate.net/publication/389917229_Integrating_Chain-of-Thought_and_Retrieval_Augmented_Generation_Enhances_Rare_Disease_Diagnosis_from_Clinical_Notes",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [],
      "author_string": ""
    },
    {
      "item_id": 4,
      "title": "Anatomy-aware and acquisition-agnostic joint registration with SynthMorph",
      "abstract": "Affine image registration is a cornerstone of medical image analysis. While classical algorithms can achieve excellent accuracy, they solve a time-consuming optimization for every image pair. Deep-learning (DL) methods learn a function that maps an image pair to an output transform. Evaluating the function is fast, but capturing large transforms can be challenging, and networks tend to struggle if a test-image characteristic shifts from the training domain, such as resolution. Most affine methods are agnostic to the anatomy the user wishes to align, meaning the registration will be inaccurate if algorithms consider all structures in the image. We address these shortcomings with SynthMorph, a fast, symmetric, diffeomorphic, and easy-to-use DL tool for joint affine-deformable registration of any brain image without preprocessing. First, we leverage a strategy that trains networks with widely varying images synthesized from label maps, yielding robust performance for image types unseen at training. Second, we optimize the spatial overlap of select anatomical labels. This enables networks to distinguish anatomy of interest from irrelevant structures, removing the need for preprocessing that excludes content that may impinge on anatomy-specific registration. Third, we combine the affine model with a deformable hypernetwork that lets users choose the optimal deformation-field regularity for their specific data, at registration time, in a fraction of the time required by classical methods. We analyze how competing architectures learn affine transforms and compare state-of-the-art registration tools across an extremely diverse set of neuroimaging data, aiming to truly capture the behavior of methods in the real world. SynthMorph demonstrates high accuracy and is available at https://w3id.org/synthmorph, as a single complete end-to-end solution for registration of brain MRI.",
      "date": "2024-06-25 2024-06-25",
      "doi": "10.1162/imag_a_00197",
      "url": "http://arxiv.org/abs/2301.11329",
      "journal": "Imaging Neuroscience",
      "volume": "2",
      "issue": "",
      "pages": "1-33",
      "authors": [
        "Malte Hoffmann",
        "Andrew Hoopes",
        "Douglas N. Greve",
        "Bruce Fischl",
        "Adrian V. Dalca"
      ],
      "author_string": "Malte Hoffmann, Andrew Hoopes, Douglas N. Greve, Bruce Fischl, Adrian V. Dalca"
    },
    {
      "item_id": 154,
      "title": "AutoRG-Brain: Grounded Report Generation for Brain MRI",
      "abstract": "Radiologists are tasked with interpreting a large number of images in a daily base, with the responsibility of generating corresponding reports. This demanding workload elevates the risk of human error, potentially leading to treatment delays, increased healthcare costs, revenue loss, and operational inefficiencies. To address these challenges, we initiate a series of work on grounded Automatic Report Generation (AutoRG), starting from the brain MRI interpretation system, which supports the delineation of brain structures, the localization of anomalies, and the generation of well-organized findings. We make contributions from the following aspects, first, on dataset construction, we release a comprehensive dataset encompassing segmentation masks of anomaly regions and manually authored reports, termed as RadGenome-Brain MRI. This data resource is intended to catalyze ongoing research and development in the field of AI-assisted report generation systems. Second, on system design, we propose AutoRG-Brain, the first brain MRI report generation system with pixel-level grounded visual clues. Third, for evaluation, we conduct quantitative assessments and human evaluations of brain structure segmentation, anomaly localization, and report generation tasks to provide evidence of its reliability and accuracy. This system has been integrated into real clinical scenarios, where radiologists were instructed to write reports based on our generated findings and anomaly segmentation masks. The results demonstrate that our system enhances the report-writing skills of junior doctors, aligning their performance more closely with senior doctors, thereby boosting overall productivity.",
      "date": "2024-07-30 2024-07-30",
      "doi": "10.48550/arXiv.2407.16684",
      "url": "http://arxiv.org/abs/2407.16684",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [
        "Jiayu Lei",
        "Xiaoman Zhang",
        "Chaoyi Wu",
        "Lisong Dai",
        "Ya Zhang",
        "Yanyong Zhang",
        "Yanfeng Wang",
        "Weidi Xie",
        "Yuehua Li"
      ],
      "author_string": "Jiayu Lei, Xiaoman Zhang, Chaoyi Wu, Lisong Dai, Ya Zhang, Yanyong Zhang, Yanfeng Wang, Weidi Xie, Yuehua Li"
    },
    {
      "item_id": 199,
      "title": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models",
      "abstract": "Prompt learning is one of the most effective paradigms for adapting pre-trained vision-language models (VLMs) to the biomedical image classification tasks in few shot scenarios. However, most of the current prompt learning methods only used the text prompts and ignored the particular structures (such as the complex anatomical structures and subtle pathological features) in the biomedical images. In this work, we propose Biomed-DPT, a knowledge-enhanced dual modality prompt tuning technique. In designing the text prompt, Biomed-DPT constructs a dual prompt including the template-driven clinical prompts and the large language model (LLM)-driven domain-adapted prompts, then extracts the clinical knowledge from the domain-adapted prompts through the knowledge distillation technique. In designing the vision prompt, Biomed-DPT introduces the zero vector as a soft prompt to leverage attention re-weighting so that the focus on non-diagnostic regions and the recognition of non-critical pathological features are avoided. Biomed-DPT achieves an average classification accuracy of 66.14\\% across 11 biomedical image datasets covering 9 modalities and 10 organs, with performance reaching 78.06\\% in base classes and 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method by 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at \\underline{https://github.com/Kanyooo/Biomed-DPT}.",
      "date": "2025-05-08 2025-05-08",
      "doi": "10.48550/arXiv.2505.05189",
      "url": "http://arxiv.org/abs/2505.05189",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [
        "Wei Peng",
        "Kang Liu",
        "Jianchen Hu",
        "Meng Zhang"
      ],
      "author_string": "Wei Peng, Kang Liu, Jianchen Hu, Meng Zhang"
    },
    {
      "item_id": 190,
      "title": "Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models",
      "abstract": "Understanding brain disorders is crucial for accurate clinical diagnosis and treatment. Recent advances in Multimodal Large Language Models (MLLMs) offer a promising approach to interpreting medical images with the support of text descriptions. However, previous research has primarily focused on 2D medical images, leaving richer spatial information of 3D images under-explored, and single-modality-based methods are limited by overlooking the critical clinical information contained in other modalities. To address this issue, this paper proposes Brain-Adapter, a novel approach that incorporates an extra bottleneck layer to learn new knowledge and instill it into the original pre-trained knowledge. The major idea is to incorporate a lightweight bottleneck layer to train fewer parameters while capturing essential information and utilize a Contrastive Language-Image Pre-training (CLIP) strategy to align multimodal data within a unified representation space. Extensive experiments demonstrated the effectiveness of our approach in integrating multimodal data to significantly improve the diagnosis accuracy without high computational costs, highlighting the potential to enhance real-world diagnostic workflows.",
      "date": "2025-04-14 2025-04-14",
      "doi": "10.1109/ISBI60581.2025.10980770",
      "url": "http://arxiv.org/abs/2501.16282",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "1-5",
      "authors": [
        "Jing Zhang",
        "Xiaowei Yu",
        "Yanjun Lyu",
        "Lu Zhang",
        "Tong Chen",
        "Chao Cao",
        "Yan Zhuang",
        "Minheng Chen",
        "Tianming Liu",
        "Dajiang Zhu"
      ],
      "author_string": "Jing Zhang, Xiaowei Yu, Yanjun Lyu, Lu Zhang, Tong Chen, Chao Cao, Yan Zhuang, Minheng Chen, Tianming Liu, Dajiang Zhu"
    },
    {
      "item_id": 6,
      "title": "ChatCAD+: Toward a Universal and Reliable Interactive CAD Using LLMs",
      "abstract": "The integration of Computer-Aided Diagnosis (CAD) with Large Language Models (LLMs) presents a promising frontier in clinical applications, notably in automating diagnostic processes akin to those performed by radiologists and providing consultations similar to a virtual family doctor. Despite the promising potential of this integration, current works face at least two limitations: (1) From the perspective of a radiologist, existing studies typically have a restricted scope of applicable imaging domains, failing to meet the diagnostic needs of different patients. Also, the insufficient diagnostic capability of LLMs further undermine the quality and reliability of the generated medical reports. (2) Current LLMs lack the requisite depth in medical expertise, rendering them less effective as virtual family doctors due to the potential unreliability of the advice provided during patient consultations. To address these limitations, we introduce ChatCAD+, to be universal and reliable. Specifically, it is featured by two main modules: (1) Reliable Report Generation and (2) Reliable Interaction. The Reliable Report Generation module is capable of interpreting medical images from diverse domains and generate high-quality medical reports via our proposed hierarchical in-context learning. Concurrently, the interaction module leverages up-to-date information from reputable medical websites to provide reliable medical advice. Together, these designed modules synergize to closely align with the expertise of human medical professionals, offering enhanced consistency and reliability for interpretation and advice. The source code is available at GitHub.",
      "date": "2024-01-00 2024-01",
      "doi": "10.1109/TMI.2024.3398350",
      "url": "https://ieeexplore.ieee.org/abstract/document/10522762",
      "journal": "IEEE Transactions on Medical Imaging",
      "volume": "43",
      "issue": "11",
      "pages": "3755-3766",
      "authors": [
        "Zihao Zhao",
        "Sheng Wang",
        "Jinchen Gu",
        "Yitao Zhu",
        "Lanzhuju Mei",
        "Zixu Zhuang",
        "Zhiming Cui",
        "Qian Wang",
        "Dinggang Shen"
      ],
      "author_string": "Zihao Zhao, Sheng Wang, Jinchen Gu, Yitao Zhu, Lanzhuju Mei, Zixu Zhuang, Zhiming Cui, Qian Wang, Dinggang Shen"
    },
    {
      "item_id": 8,
      "title": "ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models",
      "abstract": "Large language models (LLMs) have recently demonstrated their potential in clinical applications, providing valuable medical knowledge and advice. For example, a large dialog LLM like ChatGPT has successfully passed part of the US medical licensing exam. However, LLMs currently have difficulty processing images, making it challenging to interpret information from medical images, which are rich in information that supports clinical decisions. On the other hand, computer-aided diagnosis (CAD) networks for medical images have seen significant success in the medical field by using advanced deep-learning algorithms to support clinical decision-making. This paper presents a method for integrating LLMs into medical-image CAD networks. The proposed framework uses LLMs to enhance the output of multiple CAD networks, such as diagnosis networks, lesion segmentation networks, and report generation networks, by summarizing and reorganizing the information presented in natural language text format. The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems. In the future, LLM's medical knowledge can be also used to improve the performance of vision-based medical-image CAD models.",
      "date": "2023-02-14 2023/02/14",
      "doi": "",
      "url": "https://arxiv.org/abs/2302.07257v1",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [
        "Sheng Wang",
        "Zihao Zhao",
        "Xi Ouyang",
        "Qian Wang",
        "Dinggang Shen"
      ],
      "author_string": "Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, Dinggang Shen"
    },
    {
      "item_id": 181,
      "title": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models",
      "abstract": "Chain-of-thought (CoT) reasoning boosts large language models' (LLMs) performance on complex tasks but faces two key limitations: a lack of reliability when solely relying on LLM-generated reasoning chains and interference from natural language reasoning steps with the models' inference process, also known as the inference logic of LLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation,featuring knowledge graphs to modulate reasoning chain generation of LLMs, thereby enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented generation (RAG) into knowledge graphs to retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable information; (iii) Pseudo-Program Prompting Execution, which promotes greater logical rigor by guiding LLMs to execute reasoning tasks as pseudo-programs. Evaluations on nine public datasets spanning three reasoning tasks reveal significant accuracy gains--ranging from 4.0% to 44.3%--over state-of-the-art methods. Furthermore, tests on four domain-specific datasets demonstrate exceptional accuracy and efficient execution, underscoring its practical applicability and scalability.",
      "date": "2025-05-19 2025-05-19",
      "doi": "10.48550/arXiv.2504.13534",
      "url": "http://arxiv.org/abs/2504.13534",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [
        "Feiyang Li",
        "Peng Fang",
        "Zhan Shi",
        "Arijit Khan",
        "Fang Wang",
        "Dan Feng",
        "Weihao Wang",
        "Xin Zhang",
        "Yongjian Cui"
      ],
      "author_string": "Feiyang Li, Peng Fang, Zhan Shi, Arijit Khan, Fang Wang, Dan Feng, Weihao Wang, Xin Zhang, Yongjian Cui"
    },
    {
      "item_id": 146,
      "title": "Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine",
      "abstract": "One of the major barriers to using large language models (LLMs) in medicine is the perception they use uninterpretable methods to make clinical decisions that are inherently different from the cognitive processes of clinicians. In this manuscript we develop diagnostic reasoning prompts to study whether LLMs can imitate clinical reasoning while accurately forming a diagnosis. We find that GPT-4 can be prompted to mimic the common clinical reasoning processes of clinicians without sacrificing diagnostic accuracy. This is significant because an LLM that can imitate clinical reasoning to provide an interpretable rationale offers physicians a means to evaluate whether an LLMs response is likely correct and can be trusted for patient care. Prompting methods that use diagnostic reasoning have the potential to mitigate the “black box” limitations of LLMs, bringing them one step closer to safe and effective use in medicine.",
      "date": "2024-01-24 2024-01-24",
      "doi": "10.1038/s41746-024-01010-1",
      "url": "https://www.nature.com/articles/s41746-024-01010-1",
      "journal": "npj Digital Medicine",
      "volume": "7",
      "issue": "1",
      "pages": "20",
      "authors": [
        "Thomas Savage",
        "Ashwin Nayak",
        "Robert Gallo",
        "Ekanath Rangan",
        "Jonathan H. Chen"
      ],
      "author_string": "Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, Jonathan H. Chen"
    },
    {
      "item_id": 1,
      "title": "Enhancing radiomics features via a large language model for classifying benign and malignant breast tumors in mammography",
      "abstract": "Background and Objectives\nRadiomics is widely used to assist in clinical decision-making, disease diagnosis, and treatment planning for various target organs, including the breast. Recent advances in large language models (LLMs) have helped enhance radiomics analysis.\nMaterials and Methods\nHerein, we sought to improve radiomics analysis by incorporating LLM-learned clinical knowledge, to classify benign and malignant tumors in breast mammography. We extracted radiomics features from the mammograms based on the region of interest and retained the features related to the target task. Using prompt engineering, we devised an input sequence that reflected the selected features and the target task. The input sequence was fed to the chosen LLM (LLaMA variant), which was fine-tuned using low-rank adaptation to enhance radiomics features. This was then evaluated on two mammogram datasets (VinDr-Mammo and INbreast) against conventional baselines.\nResults\nThe enhanced radiomics-based method performed better than baselines using conventional radiomics features tested on two mammogram datasets, achieving accuracies of 0.671 for the VinDr-Mammo dataset and 0.839 for the INbreast dataset. Conventional radiomics models require retraining from scratch for an unseen dataset using a new set of features. In contrast, the model developed in this study effectively reused the common features between the training and unseen datasets by explicitly linking feature names with feature values, leading to extensible learning across datasets. Our method performed better than the baseline method in this retraining setting using an unseen dataset.\nConclusions\nOur method, one of the first to incorporate LLM into radiomics, has the potential to improve radiomics analysis.",
      "date": "2025-06-01 2025-06-01",
      "doi": "10.1016/j.cmpb.2025.108765",
      "url": "https://www.sciencedirect.com/science/article/pii/S0169260725001828",
      "journal": "Computer Methods and Programs in Biomedicine",
      "volume": "265",
      "issue": "",
      "pages": "108765",
      "authors": [
        "Sinyoung Ra",
        "Jonghun Kim",
        "Inye Na",
        "Eun Sook Ko",
        "Hyunjin Park"
      ],
      "author_string": "Sinyoung Ra, Jonghun Kim, Inye Na, Eun Sook Ko, Hyunjin Park"
    },
    {
      "item_id": 159,
      "title": "Enhancing vision-language models for medical imaging: bridging the 3D gap with innovative slice selection",
      "abstract": "",
      "date": "2024-12-16 2024-12-16",
      "doi": "",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/b53513b83232116ae25f57a174a7c993-Abstract-Datasets_and_Benchmarks_Track.html",
      "journal": "Advances in Neural Information Processing Systems",
      "volume": "37",
      "issue": "",
      "pages": "99947-99964",
      "authors": [
        "Yuli Wang",
        "Jian Peng",
        "Yuwei Dai",
        "Craig Jones",
        "Haris Sair",
        "Jinglai Shen",
        "Nicolas Loizou",
        "Jing Wu",
        "Wen-Chi Hsu",
        "Maliha Imami",
        "Zhicheng Jiao",
        "Paul Zhang",
        "Harrison Bai"
      ],
      "author_string": "Yuli Wang, Jian Peng, Yuwei Dai, Craig Jones, Haris Sair, Jinglai Shen, Nicolas Loizou, Jing Wu, Wen-Chi Hsu, Maliha Imami, Zhicheng Jiao, Paul Zhang, Harrison Bai"
    },
    {
      "item_id": 150,
      "title": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes",
      "abstract": "Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in real-world settings, foundation models are not optimized for domain-specific tasks like clinical diagnosis, yet inputs are unstructured clinical notes rather than standardized terms. How LLMs can be instructed to predict candidate genes or disease diagnosis from unstructured clinical notes remains a major challenge. Methods: We introduce RAG-driven CoT and CoT-driven RAG, two methods that combine Chain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze clinical notes. A five-question CoT protocol mimics expert reasoning, while RAG retrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in Man). We evaluated these approaches on rare disease datasets, including 5,980 Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house clinical notes from Childrens Hospital of Philadelphia. Results: We found that recent foundations models, including Llama 3.3-70B-Instruct and DeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2 and GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both outperform foundation models in candidate gene prioritization from clinical notes; in particular, both methods with DeepSeek backbone resulted in a top-10 gene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT works better for high-quality notes, where early retrieval can anchor the subsequent reasoning steps in domain-specific evidence, while CoT-driven RAG has advantage when processing lengthy and noisy notes.",
      "date": "2025-03-15 2025-03-15",
      "doi": "10.48550/arXiv.2503.12286",
      "url": "http://arxiv.org/abs/2503.12286",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [
        "Da Wu",
        "Zhanliang Wang",
        "Quan Nguyen",
        "Kai Wang"
      ],
      "author_string": "Da Wu, Zhanliang Wang, Quan Nguyen, Kai Wang"
    },
    {
      "item_id": 3,
      "title": "LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle Zero-shot Radiology Recognition?",
      "abstract": "Recently, multimodal large models (MLLMs) have demonstrated exceptional capabilities in visual understanding and reasoning across various vision-language tasks. However, MLLMs usually perform poorly in zero-shot medical disease recognition, as they do not fully exploit the captured features and available medical knowledge. To address this challenge, we propose LLaVA-RadZ, a simple yet effective framework for zero-shot medical disease recognition. Specifically, we design an end-to-end training strategy, termed Decoding-Side Feature Alignment Training (DFAT) to take advantage of the characteristics of the MLLM decoder architecture and incorporate modality-specific tokens tailored for different modalities, which effectively utilizes image and text representations and facilitates robust cross-modal alignment. Additionally, we introduce a Domain Knowledge Anchoring Module (DKAM) to exploit the intrinsic medical knowledge of large models, which mitigates the category semantic gap in image-text alignment. DKAM improves category-level alignment, allowing for accurate disease recognition. Extensive experiments on multiple benchmarks demonstrate that our LLaVA-RadZ significantly outperforms traditional MLLMs in zero-shot disease recognition and exhibits the state-of-the-art performance compared to the well-established and highly-optimized CLIP-based approaches.",
      "date": "2025-03-10 2025-03-10",
      "doi": "10.48550/arXiv.2503.07487",
      "url": "http://arxiv.org/abs/2503.07487",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [
        "Bangyan Li",
        "Wenxuan Huang",
        "Yunhang Shen",
        "Yeqiang Wang",
        "Shaohui Lin",
        "Jingzhong Lin",
        "Ling You",
        "Yinqi Zhang",
        "Ke Li",
        "Xing Sun",
        "Yuling Sun"
      ],
      "author_string": "Bangyan Li, Wenxuan Huang, Yunhang Shen, Yeqiang Wang, Shaohui Lin, Jingzhong Lin, Ling You, Yinqi Zhang, Ke Li, Xing Sun, Yuling Sun"
    },
    {
      "item_id": 203,
      "title": "Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales",
      "abstract": "Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a \"reasoning-aware\" diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales. Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various settings. We further propose a novel set of criteria for evaluating machine-generated rationales' potential for real-world clinical settings, facilitating and benefiting future research in this area.",
      "date": "2024-05-10 2024-05-10",
      "doi": "10.48550/arXiv.2312.07399",
      "url": "http://arxiv.org/abs/2312.07399",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [
        "Taeyoon Kwon",
        "Kai Tzu-iunn Ong",
        "Dongjin Kang",
        "Seungjun Moon",
        "Jeong Ryong Lee",
        "Dosik Hwang",
        "Yongsik Sim",
        "Beomseok Sohn",
        "Dongha Lee",
        "Jinyoung Yeo"
      ],
      "author_string": "Taeyoon Kwon, Kai Tzu-iunn Ong, Dongjin Kang, Seungjun Moon, Jeong Ryong Lee, Dosik Hwang, Yongsik Sim, Beomseok Sohn, Dongha Lee, Jinyoung Yeo"
    },
    {
      "item_id": 163,
      "title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models",
      "abstract": "Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to address these issues. However, the amount of high-quality data and distribution shifts between training data and deployment data limit the application of fine-tuning methods. Although RAG is lightweight and effective, existing RAG-based approaches are not sufficiently general to different medical domains and can potentially cause misalignment issues, both between modalities and between the model and the ground truth. In this paper, we propose a versatile multimodal RAG system, MMed-RAG, designed to enhance the factuality of Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an adaptive retrieved contexts selection method, and a provable RAG-based preference fine-tuning strategy. These innovations make the RAG process sufficiently general and reliable, significantly improving alignment when introducing retrieved contexts. Experimental results across five medical datasets (involving radiology, ophthalmology, pathology) on medical VQA and report generation demonstrate that MMed-RAG can achieve an average improvement of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available in https://github.com/richard-peng-xia/MMed-RAG.",
      "date": "2025-03-03 2025-03-03",
      "doi": "10.48550/arXiv.2410.13085",
      "url": "http://arxiv.org/abs/2410.13085",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [
        "Peng Xia",
        "Kangyu Zhu",
        "Haoran Li",
        "Tianze Wang",
        "Weijia Shi",
        "Sheng Wang",
        "Linjun Zhang",
        "James Zou",
        "Huaxiu Yao"
      ],
      "author_string": "Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, Huaxiu Yao"
    },
    {
      "item_id": 171,
      "title": "MedCoT: Medical Chain of Thought via Hierarchical Expert",
      "abstract": "Artificial intelligence has advanced in Medical Visual Question Answering (Med-VQA), but prevalent research tends to focus on the accuracy of the answers, often overlooking the reasoning paths and interpretability, which are crucial in clinical settings. Besides, current Med-VQA algorithms, typically reliant on singular models, lack the robustness needed for real-world medical diagnostics which usually require collaborative expert evaluation. To address these shortcomings, this paper presents MedCoT, a novel hierarchical expert verification reasoning chain method designed to enhance interpretability and accuracy in biomedical imaging inquiries. MedCoT is predicated on two principles: The necessity for explicit reasoning paths in Med-VQA and the requirement for multi-expert review to formulate accurate conclusions. The methodology involves an Initial Specialist proposing diagnostic rationales, followed by a Follow-up Specialist who validates these rationales, and finally, a consensus is reached through a vote among a sparse Mixture of Experts within the locally deployed Diagnostic Specialist, which then provides the definitive diagnosis. Experimental evaluations on four standard Med-VQA datasets demonstrate that MedCoT surpasses existing state-of-the-art approaches, providing significant improvements in performance and interpretability.",
      "date": "2024-01-00 2024-01",
      "doi": "10.18653/v1/2024.emnlp-main.962",
      "url": "https://aclanthology.org/2024.emnlp-main.962/",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "17371–17389",
      "authors": [
        "Jiaxiang Liu",
        "Yuan Wang",
        "Jiawei Du",
        "Joey Tianyi Zhou",
        "Zuozhu Liu",
        "Yaser Al-Onaizan",
        "Mohit Bansal",
        "Yun-Nung Chen"
      ],
      "author_string": "Jiaxiang Liu, Yuan Wang, Jiawei Du, Joey Tianyi Zhou, Zuozhu Liu, Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen"
    },
    {
      "item_id": 2,
      "title": "RadioRAG: Factual large language models for enhanced diagnostics in radiology using online retrieval augmented generation",
      "abstract": "Large language models (LLMs) often generate outdated or inaccurate information based on static training datasets. Retrieval augmented generation (RAG) mitigates this by integrating outside data sources. While previous RAG systems used pre-assembled, fixed databases with limited flexibility, we have developed Radiology RAG (RadioRAG), an end-to-end framework that retrieves data from authoritative radiologic online sources in real-time. We evaluate the diagnostic accuracy of various LLMs when answering radiology-specific questions with and without access to additional online information via RAG. Using 80 questions from the RSNA Case Collection across radiologic subspecialties and 24 additional expert-curated questions with reference standard answers, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were prompted with and without RadioRAG in a zero-shot inference scenario RadioRAG retrieved context-specific information from www.radiopaedia.org in real-time. Accuracy was investigated. Statistical analyses were performed using bootstrapping. The results were further compared with human performance. RadioRAG improved diagnostic accuracy across most LLMs, with relative accuracy increases ranging up to 54% for different LLMs. It matched or exceeded non-RAG models and the human radiologist in question answering across radiologic subspecialties, particularly in breast imaging and emergency radiology. However, the degree of improvement varied among models; GPT-3.5-turbo and Mixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2 showed no improvement, highlighting variability in RadioRAG's effectiveness. LLMs benefit when provided access to domain-specific data beyond their training data. For radiology, RadioRAG establishes a robust framework that substantially improves diagnostic accuracy and factuality in radiological question answering.",
      "date": "2024-12-25 2024-12-25",
      "doi": "10.48550/arXiv.2407.15621",
      "url": "http://arxiv.org/abs/2407.15621",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [
        "Soroosh Tayebi Arasteh",
        "Mahshad Lotfinia",
        "Keno Bressem",
        "Robert Siepmann",
        "Lisa Adams",
        "Dyke Ferber",
        "Christiane Kuhl",
        "Jakob Nikolas Kather",
        "Sven Nebelung",
        "Daniel Truhn"
      ],
      "author_string": "Soroosh Tayebi Arasteh, Mahshad Lotfinia, Keno Bressem, Robert Siepmann, Lisa Adams, Dyke Ferber, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn"
    },
    {
      "item_id": 193,
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
      "date": "2021-04-12 2021-04-12",
      "doi": "10.48550/arXiv.2005.11401",
      "url": "http://arxiv.org/abs/2005.11401",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [
        "Patrick Lewis",
        "Ethan Perez",
        "Aleksandra Piktus",
        "Fabio Petroni",
        "Vladimir Karpukhin",
        "Naman Goyal",
        "Heinrich Küttler",
        "Mike Lewis",
        "Wen-tau Yih",
        "Tim Rocktäschel",
        "Sebastian Riedel",
        "Douwe Kiela"
      ],
      "author_string": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela"
    },
    {
      "item_id": 144,
      "title": "Retrieval-augmented generation for generative artificial intelligence in health care",
      "abstract": "Generative artificial intelligence has brought disruptive innovations in health care but faces certain challenges. Retrieval-augmented generation (RAG) enables models to generate more reliable content by leveraging the retrieval of external knowledge. In this perspective, we analyze the possible contributions that RAG could bring to health care in equity, reliability, and personalization. Additionally, we discuss the current limitations and challenges of implementing RAG in medical scenarios.",
      "date": "2025-01-25 2025-01-25",
      "doi": "10.1038/s44401-024-00004-1",
      "url": "https://www.nature.com/articles/s44401-024-00004-1",
      "journal": "npj Health Systems",
      "volume": "2",
      "issue": "1",
      "pages": "2",
      "authors": [
        "Rui Yang",
        "Yilin Ning",
        "Emilia Keppo",
        "Mingxuan Liu",
        "Chuan Hong",
        "Danielle S. Bitterman",
        "Jasmine Chiat Ling Ong",
        "Daniel Shu Wei Ting",
        "Nan Liu"
      ],
      "author_string": "Rui Yang, Yilin Ning, Emilia Keppo, Mingxuan Liu, Chuan Hong, Danielle S. Bitterman, Jasmine Chiat Ling Ong, Daniel Shu Wei Ting, Nan Liu"
    },
    {
      "item_id": 157,
      "title": "Towards a holistic framework for multimodal LLM in 3D brain CT radiology report generation | Nature Communications",
      "abstract": "",
      "date": "",
      "doi": "",
      "url": "https://www.nature.com/articles/s41467-025-57426-0",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [],
      "author_string": ""
    },
    {
      "item_id": 175,
      "title": "V2T-CoT: From Vision to Text Chain-of-Thought for Medical Reasoning and Diagnosis",
      "abstract": "Recent advances in multimodal techniques have led to significant progress in Medical Visual Question Answering (Med-VQA). However, most existing models focus on global image features rather than localizing disease-specific regions crucial for diagnosis. Additionally, current research tends to emphasize answer accuracy at the expense of the reasoning pathway, yet both are crucial for clinical decision-making. To address these challenges, we propose From Vision to Text Chain-of-Thought (V2T-CoT), a novel approach that automates the localization of preference areas within biomedical images and incorporates this localization into region-level pixel attention as knowledge for Vision CoT. By fine-tuning the vision language model on constructed R-Med 39K dataset, V2T-CoT provides definitive medical reasoning paths. V2T-CoT integrates visual grounding with textual rationale generation to establish precise and explainable diagnostic results. Experimental results across four Med-VQA benchmarks demonstrate state-of-the-art performance, achieving substantial improvements in both performance and interpretability.",
      "date": "2025-06-27 2025-06-27",
      "doi": "10.48550/arXiv.2506.19610",
      "url": "http://arxiv.org/abs/2506.19610",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [
        "Yuan Wang",
        "Jiaxiang Liu",
        "Shujian Gao",
        "Bin Feng",
        "Zhihang Tang",
        "Xiaotang Gai",
        "Jian Wu",
        "Zuozhu Liu"
      ],
      "author_string": "Yuan Wang, Jiaxiang Liu, Shujian Gao, Bin Feng, Zhihang Tang, Xiaotang Gai, Jian Wu, Zuozhu Liu"
    },
    {
      "item_id": 5,
      "title": "VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis",
      "abstract": "We present VoxelPrompt, an agent-driven vision-language framework that tackles diverse radiological tasks through joint modeling of natural language, image volumes, and analytical metrics. VoxelPrompt is multi-modal and versatile, leveraging the flexibility of language interaction while providing quantitatively grounded image analysis. Given a variable number of 3D medical volumes, such as MRI and CT scans, VoxelPrompt employs a language agent that iteratively predicts executable instructions to solve a task specified by an input prompt. These instructions communicate with a vision network to encode image features and generate volumetric outputs (e.g., segmentations). VoxelPrompt interprets the results of intermediate instructions and plans further actions to compute discrete measures (e.g., tumor growth across a series of scans) and present relevant outputs to the user. We evaluate this framework in a sandbox of diverse neuroimaging tasks, and we show that the single VoxelPrompt model can delineate hundreds of anatomical and pathological features, measure many complex morphological properties, and perform open-language analysis of lesion characteristics. VoxelPrompt carries out these objectives with accuracy similar to that of fine-tuned, single-task models for segmentation and visual question-answering, while facilitating a much larger range of tasks. Therefore, by supporting accurate image processing with language interaction, VoxelPrompt provides comprehensive utility for numerous imaging tasks that traditionally require specialized models to address.",
      "date": "2024-10-10 2024/10/10",
      "doi": "",
      "url": "https://arxiv.org/abs/2410.08397v1",
      "journal": "",
      "volume": "",
      "issue": "",
      "pages": "",
      "authors": [
        "Andrew Hoopes",
        "Victor Ion Butoi",
        "John V. Guttag",
        "Adrian V. Dalca"
      ],
      "author_string": "Andrew Hoopes, Victor Ion Butoi, John V. Guttag, Adrian V. Dalca"
    }
  ]
}