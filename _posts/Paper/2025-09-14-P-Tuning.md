---
categories:
- Paper
- VLM
date: '2025-09-16'
excerpt: 'GPT Understands, Too: P-Tuning을 통한 자연어 이해 성능 향상에 대한 체계적 분석'
header:
  teaser: /assets/images/paper/p-tuning-teaser.png
last_modified_at: '2025-09-16'
published: true
tags:
- P-Tuning
- Prompt Engineering
- GPT
- NLP
title: 'GPT Understands, Too: P-Tuning으로 자연어 이해 향상하기'
toc: true
toc_sticky: true
---

# GPT Understands, Too: P-Tuning으로 자연어 이해 향상하기

## 논문 정보
- **저자**: **: Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang
- **발표**: **: ArXiv 2021
- **ArXiv**: N/A

## 1. 핵심 요약 (2-3문장)
이 논문의 핵심 기여와 주요 발견을 간결하게 요약합니다.

## 2. 배경 및 동기
기존의 discrete prompt는 다음과 같은 한계가 있었습니다:
- **불안정성**: 프롬프트 표현이 조금만 바뀌어도 성능이 크게 변함
- **제한된 표현력**: 자연어의 제약으로 인한 최적화 어려움
- **태스크 특화**: 각 태스크마다 수동으로 프롬프트 설계 필요

P-Tuning은 이를 해결하기 위해 **continuous prompt embedding**을 도입했습니다.

## 3. 제안 방법

### 3.1 아키텍처 개요

![Architecture Overview 0](/assets/images/paper/p-tuning/architecture_overview_0.png)
*Figure: Architecture Overview 0*



### 3.2 핵심 기술/알고리즘
- **저자**: Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang
- **발표**: ArXiv 2021
- **ArXiv ID**: [2103.10385](https://arxiv.org/abs/2103.10385)




```
기존: [MASK] 뉴스는 [정치/스포츠/경제] 분야입니다.
P-Tuning: [P0][P1][P2] 뉴스는 [P3][P4] 분야입니다.
```
- `[Pi]`는 학습 가능한 continuous embedding
- 자연어 제약 없이 최적화 가능


P-Tuning의 핵심 혁신은 **프롬프트를 입력 시퀀스 어디에든 삽입 가능**하다는 점입니다.


```python

[P0][P1] input text [MASK]


input [P0][P1] text [MASK]


[P0] input [P1] text [P2] [MASK]


input text [P0][P1] [MASK]
```


```python
template = "It was [P0][P1]. [INPUT] [P2][P3][P4] [MASK]."
```
- 각 `[Pi]`는 독립적으로 학습
- 템플릿 구조는 태스크별로 설계




- **최적**: 시작 + 중간 조합
- **이유**: 컨텍스트 설정과 판단 지점 모두 중요


- **최적**: 전제와 가설 사이에 삽입
- **이유**: 논리적 관계 모델링에 효과적


- **최적**: 질문 뒤, 답변 앞에 삽입
- **이유**: 질문 이해와 답변 생성 사이의 브릿지 역할




- **장점**: 전체 시퀀스에 영향
- **단점**: 입력 정보 부족 상태에서 결정


- **장점**: 충분한 컨텍스트 정보 활용
- **단점**: 지역적 영향에 제한


- **장점**: 각 위치의 장점 결합
- **단점**: 파라미터 수 증가


- **기존 GPT-3**: 32.8% (SuperGLUE)
- **P-Tuning**: 69.0% (SuperGLUE) - **36.2%p 향상**


- **LAMA 데이터셋**: 기존 대비 20%p 향상
- **위치별**: 관계 유형에 따라 최적 위치 상이


- **GPT-Medium**: 15%p 향상
- **GPT-Large**: 25%p 향상  
- **GPT-XL**: 35%p 향상
- **결론**: 큰 모델일수록 위치 효과 증가




```python

init_embeddings = model.embed_tokens(["the", "great", "good", "best"])


init_embeddings = torch.randn(prompt_length, hidden_size) * 0.1
```


1. **프롬프트 파라미터만 업데이트**
2. **다양한 위치에서 gradient 수집**
3. **위치별 학습률 조정**


```python

"[P0] This movie is [P1] [MASK] [P2]"


"[P0][P1][P2][P3] [MASK]" # 구조적 정보 부족
```






- **위치**: 입력 뒤, 레이블 앞
- **길이**: 2-4 토큰
- **예시**: `input [P0][P1] [MASK]`


- **위치**: 시작 부분 + 생성 지점
- **길이**: 4-8 토큰
- **예시**: `[P0][P1] input [P2][P3] generated text`


- **위치**: 논리적 연결 지점
- **길이**: 3-6 토큰
- **예시**: `premise [P0][P1] hypothesis [P2] [MASK]`




- **템플릿 의존성**: 여전히 수동 템플릿 설계 필요
- **위치 민감성**: 최적 위치 찾기 위한 실험 비용
- **해석 어려움**: Continuous embedding의 의미 해석 한계


- **자동 템플릿 생성**: 템플릿 구조 자동 최적화
- **위치 자동 탐색**: 강화학습을 통한 위치 선택
- **다중 모달**: 텍스트 외 다른 모달리티로 확장

### 3.3 구현 세부사항


## 4. 실험 및 결과

### 4.1 실험 설정
실험에 사용된 데이터셋, 평가 지표, 비교 대상을 설명합니다.

### 4.2 주요 결과


| 삽입 위치 | SuperGLUE 점수 | 개선도 |
|-----------|----------------|--------|
| 시작만 | 71.2 | +2.1 |
| 중간만 | 69.8 | +0.7 |
| 끝만 | 68.5 | -0.6 |
| **혼합** | **73.4** | **+4.3** |





![Architecture Overview 0](/assets/images/paper/p-tuning/architecture_overview_0.png)
*Figure: Model architecture and component design*
*Figure: Architecture Overview 0*

```python
positions = ['start', 'middle', 'end', 'mixed']
for pos in positions:
    template = create_template(pos)
    score = evaluate_template(template)
    print(f"Position {pos}: {score}")
```

### 4.3 분석
결과에 대한 정성적 분석과 해석을 제공합니다.

## 5. 의의 및 영향
P-Tuning은 **프롬프트 위치의 중요성**을 처음으로 체계적으로 연구한 논문입니다:

1. **위치 유연성**: 시작 부분에만 국한되지 않는 프롬프트 배치
2. **태스크별 최적화**: 태스크 특성에 따른 위치 선택 중요성 입증  
3. **혼합 전략**: 여러 위치의 조합이 단일 위치보다 효과적임을 보여줌

이 연구는 이후 Prefix-Tuning, P-Tuning v2 등의 발전된 방법론의 기초가 되었으며, **소프트 프롬프트 위치가 성능에 미치는 결정적 영향**에 대한 이해의 출발점이 되었습니다.

## 6. 개인적 평가

**강점**: 이 논문의 주요 강점과 인상 깊었던 부분
**약점**: 아쉬웠던 부분이나 의문점  
**적용 가능성**: 실제 연구나 응용에서의 활용 가능성
**추천도**: 다른 연구자들에게 추천할 만한 수준

