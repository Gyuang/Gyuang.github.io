---
categories:
- paper
- vlm
date: 2025-07-29
excerpt: '3D 의료영상의 대규모 언어-이미지 사전훈련을 위한 HLIP 프레임워크: 460K 환자 데이터로 훈련되어 기존 ViT 대비 45% 메모리 절약과 32.4% 성능 향상을 달성한 계층적 어텐션 메커니즘'
header: {}
last_modified_at: '2025-09-16'
published: true
tags:
- VLM
- Vision-Language
- 3D Medical Imaging
- Hierarchical Attention
- CLIP
- Medical AI
title: 'HLIP: Towards Scalable Language-Image Pre-training for 3D Medical Imaging'
toc: true
toc_sticky: true
---

# HLIP: Towards Scalable Language-Image Pre-training for 3D Medical Imaging

## 0. 체크리스트
- [x] `categories` 두 번째 값이 `medical-ai`, `vlm`, `rag`, `multimodal`, `transformer` 등 실제 분류인지 확인했나요? → vlm (Vision-Language Model) 분류 확인
- [x] `excerpt`에 구체적인 결과/기여가 들어가나요? → 460K 환자 데이터, 45% 메모리 절약, 32.4% 성능 향상 등 구체적 수치 포함
- [x] 모든 섹션에 실제 내용이 채워졌나요? → 모든 섹션에 HLIP 논문 기반 실제 내용 작성 완료
- [x] 수치/결과 표는 3~5개 이하로 요약했나요? → 흉부 CT, 뇌 MRI, 두부 CT 성능 표 3개로 요약
- [x] 참고 링크(코드/데이터)가 있으면 마지막에 정리했나요? → 원문, 코드, 관련 데이터셋 링크 모두 정리 완료

> **작성 완료**: 3D 의료영상에서의 확장 가능한 언어-이미지 사전훈련을 위한 HLIP 프레임워크에 대한 체계적 분석이 완료되었습니다. 계층적 어텐션 메커니즘, 대규모 미가공 데이터 활용, 실제 임상 적용 가능성 등 핵심 기여사항들을 상세히 다뤘습니다.

## 1. 핵심 요약 (3문장 이하)

HLIP(Hierarchical attention for Language-Image Pre-training)은 3D 의료 영상의 대규모 언어-이미지 사전훈련을 위한 확장 가능한 프레임워크로, 기존 2D 의료영상 대비 제한적이던 3D 모달리티(CT, MRI)의 성능 한계를 극복했습니다. 220K 환자의 뇌 MRI 데이터와 240K 환자의 두부 CT 데이터로 훈련된 HLIP은 Pub-Brain-5 벤치마크에서 32.4% 향상된 균형 정확도를 달성하고, 기존 ViT 대비 45% 메모리 사용량을 감소시켰습니다. 본 연구는 미가공 임상 데이터에서 직접 사전훈련을 수행할 수 있는 첫 번째 대규모 3D 의료영상 모델로, 실제 임상 워크플로우에 직접 적용 가능한 혁신적 솔루션을 제시합니다.

## 2. 배경 & 동기

3D 의료영상에서의 언어-이미지 사전훈련은 2D 모달리티 대비 현저히 제한적인 성과를 보여왔습니다. 흉부 X-ray CLIP 모델이 500K 이상의 이미지로 훈련되어 인간 수준의 성능을 달성한 반면, 3D 의료영상 모델들은 상대적으로 성능이 부족했습니다. 이는 세 가지 주요 요인에 기인합니다: (1) 데이터 큐레이션과 주석의 필요성, (2) 모델 아키텍처의 한계, (3) 3D 의료영상 연구의 복잡성입니다.

CT와 MRI는 단일 임상 연구에서 여러 3D 볼륨(스캔 또는 시퀀스)을 생성하며, 각각 다른 촬영 프로토콜이나 방향을 포착합니다. 표준 ViT로 이러한 미가공 연구를 나이브하게 인코딩하면 연구당 10⁴개의 토큰이 생성되어 상당한 계산 오버헤드가 발생합니다. 기존 접근법들은 방사선과 의사가 대표적인 스캔이나 2D 슬라이스를 수동으로 선택하는 데이터 큐레이션에 의존하거나, 전처리에 크게 의존하는 특수 모델 아키텍처를 제안했으나 확장성과 일반화 성능이 제한적이었습니다.

HLIP의 핵심 아이디어는 방사선학 데이터의 자연스러운 계층구조(슬라이스-스캔-연구)를 활용한 계층적 어텐션 메커니즘을 통해 미가공 연구에서 직접 특징을 효과적으로 추출하는 것입니다.

## 3. 방법론

### 3.1 전체 구조

HLIP의 전체 아키텍처는 다음과 같이 구성됩니다:

1. **입력 처리**: 미가공 방사선학 연구 S ∈ R^(M×1×D×H×W) (M: 스캔 수, D×H×W: 각 스캔의 차원)
2. **토큰화**: 각 스캔을 3D 볼륨으로 분할하여 시각적 토큰 Fs ∈ R^(N×c) 생성 (N = M × d × h × w)
3. **계층적 어텐션**: 방사선학 데이터 계층구조에 따른 어텐션 수행
4. **텍스트 인코더**: PubMedBERT를 사용한 방사선학 보고서 인코딩
5. **대조 학습**: 시각적 및 텍스트 표현 간의 대조 손실 계산

### 3.2 핵심 기법: 계층적 어텐션 메커니즘

HLIP의 핵심 혁신은 방사선학 데이터의 자연스러운 계층구조를 반영한 계층적 어텐션입니다:

#### 3.2.1 데이터 계층구조
- **연구(Study)**: M개의 영상 스캔을 포함하여 방사선학 진단에 필요한 모든 시각적 정보 제공
- **스캔(Scan)**: D개의 슬라이스를 포함하며 대상 병리의 완전한 범위 캡처
- **인접 슬라이스(Adjacent Slices)**: D/d개의 연속 슬라이스로 국소적 진단 특징 캡처

#### 3.2.2 어텐션 연산
각 계층에서 독립적으로 셀프 어텐션을 계산합니다:

- **연구 어텐션**: 모든 N개 토큰에 대한 단일 셀프 어텐션 (I/O 복잡도: Ω(N² + N×c))
- **스캔 어텐션**: M개의 독립적 셀프 어텐션, 각각 d×h×w 토큰 (I/O 복잡도: Ω(N²/M + N×c))
- **슬라이스 어텐션**: M×d개의 독립적 셀프 어텐션, 각각 h×w 토큰 (I/O 복잡도: Ω(N²/(M×d) + N×c))

#### 3.2.3 주요 개선점
1. **메모리 효율성**: 기존 ViT 대비 45% 메모리 사용량 감소
2. **계산 효율성**: 단순한 reshape 연산만으로 구현 가능하며 Flash Attention과 호환
3. **성능 유지**: 윈도우 어텐션과 달리 정확도를 희생하지 않고 효율성 확보

### 3.3 학습 및 구현 세부사항

#### 3.3.1 모델 구성
- **시각 인코더**: MAE 사전훈련된 ViT-B/16
- **토큰 크기**: (8,16,16) 기본값
- **위치 임베딩**: 2D MAE 임베딩을 3D로 확장 (슬라이스 및 스캔 차원 추가)
- **최대 스캔 수**: 40개 (훈련 시 10개 무작위 샘플링)

#### 3.3.2 훈련 설정
- **데이터셋**: BrainMRI220K (220K 환자, 3.13M 스캔), HeadCT240K (240K 환자, 1.44M 스캔)
- **전처리**: 방향/간격 표준화 없이 (48,224,224)로 리사이즈
- **정규화**: 뇌 MRI는 [0.5,99.5] 백분위수 클리핑, 두부 CT는 HU 값 절단
- **훈련**: 20 에포크, 배치 크기 256, 25% 패치 드롭아웃
- **하드웨어**: 8 L40 GPU에서 약 1일 소요

## 4. 실험 & 결과

### 4.1 실험 설정

#### 데이터셋 및 벤치마크
- **흉부 CT**: CT-RATE (내부 검증), Rad-ChestCT (외부 검증)
- **뇌 MRI**: 새로운 공개 벤치마크 Pub-Brain-5 구축 (18,343 연구, 5개 클래스)
- **두부 CT**: RSNA, CQ500 벤치마크

#### 평가 지표
- 다중 레이블 분류: 매크로 AUC, 균형 정확도, F1 점수
- 제로샷 전이: 별도 파인튜닝 없이 성능 평가

#### 비교 모델
- CT-CLIP, BIUD, Merlin (3D 모델)
- BiomedCLIP, ConceptCLIP (2D 파운데이션 모델)
- fVLM (세분화된 사전훈련 프레임워크)

### 4.2 주요 결과

#### 흉부 CT 성능 (CT-RATE/Rad-ChestCT)
| 모델 | 내부 검증 AUC | 외부 검증 AUC | 개선율 |
|------|---------------|---------------|--------|
| CT-CLIP | 73.3 | 63.3 | - |
| BIUD | 71.3 | 62.9 | - |
| Merlin | 72.8 | 64.4 | - |
| **HLIP** | **77.7** | **72.3** | **+4.9%/+7.9%** |
| fVLM | 77.8 | 68.0 | - |
| **HLIP (요약 보고서)** | **78.7** | **71.7** | **+0.9%/+3.7%** |

#### 뇌 MRI 성능 (Pub-Brain-5)
| 모델 | 질병 탐지 | 종양 분류 | 질병 분류 | 평균 개선율 |
|------|-----------|-----------|-----------|-------------|
| BiomedCLIP | 64.7 | 87.8 | 63.6 | - |
| ConceptCLIP | 66.8 | 91.9 | 57.7 | - |
| **HLIP** | **91.5** | **89.2** | **79.2** | **+32.4%** |

#### 두부 CT 성능
| 벤치마크 | FM-HeadCT | HLIP | 개선율 |
|----------|-----------|------|--------|
| RSNA | 기준값 | +1.4% AUC | 1.4% |
| CQ500 | 기준값 | +6.9% AUC | 6.9% |

### 4.3 핵심 분석 결과

#### 4.3.1 확장성 분석
HLIP의 성능은 세 가지 핵심 요소의 조합에서 나타납니다:
- **데이터 규모**: 10% 데이터만 사용시 24.5% 성능 저하
- **연구 모델링**: 무작위 스캔 선택시 12.4% 성능 저하
- **계산 효율성**: 작은 배치 크기(64)에서 6.6% 성능 저하

#### 4.3.2 어블레이션 연구
- **계층적 어텐션 위치**: 균등 분포가 최적 성능
- **CLS 토큰 전략**: 복제 및 평균화 방식이 효과적
- **배치 크기 효과**: 512 이상에서 최적 성능

#### 4.3.3 시각화 분석
HLIP은 제로샷 설정에서 병리학적 영역을 정확히 식별:
- 흉부 CT: 여러 슬라이스에 걸친 병변 영역 탐지
- 뇌 MRI: 서로 다른 스캔 유형 간 병변 영역 일관성 유지

## 5. 의의 & 한계

### 5.1 임상적 의의

#### 실제 임상 워크플로우 적용 가능성
HLIP은 미가공 방사선학 연구에서 직접 예측을 수행할 수 있어 실제 임상 환경에서의 유연성을 제공합니다. 1년간의 전향적 평가에서 52개 뇌 MRI 진단과 83개 두부 CT 진단에서 일관된 성능 향상을 보여 방사선과 의사의 진단 보조 도구로서의 잠재력을 입증했습니다.

#### 대규모 미가공 데이터 활용
기존 방법들이 데이터 큐레이션과 주석에 의존했던 반면, HLIP은 460K 환자의 대규모 미가공 임상 데이터에서 직접 훈련 가능하여 실제 병원 규모의 데이터 활용이 현실적으로 가능합니다.

### 5.2 연구적 기여

#### 확장 가능한 3D 의료영상 사전훈련 프레임워크
- 지금까지 수행된 가장 대규모 3D 의료영상 훈련 (220K 뇌 MRI + 240K 두부 CT 환자)
- 기존 ViT 아키텍처와 호환되어 Flash Attention, Patch Dropout 등 최신 기술 활용 가능
- 다양한 해부학적 영역(흉부, 뇌, 두부)과 모달리티(CT, MRI)에서 일관된 성능 향상

#### 계산 효율성 혁신
메모리 사용량 45% 감소와 동시에 성능 향상을 달성하여 제한된 컴퓨팅 자원에서도 대규모 3D 의료영상 모델 훈련을 가능하게 했습니다.

### 5.3 한계 및 향후 연구 방향

#### 현재 한계점
1. **컴퓨팅 자원 제약**: 현재 배치 크기(256-512)는 자연 이미지나 2D 의료영상 도메인 대비 여전히 작음
2. **클래스 불균형**: 키워드 검색 기반 수집으로 인한 사전훈련 데이터셋의 불균형
3. **제로샷 성능 불일치**: 연구 수와 제로샷 전이 성능 간 완전한 상관관계 부재 (예: 교종 vs 수막종)

#### 향후 연구 방향
1. **균형잡힌 사전훈련 데이터셋 구축**: 자연 이미지 도메인의 발견을 바탕으로 한 체계적 접근법 개발
2. **더 큰 모델 아키텍처**: ViT-Large 등 대형 모델에서의 성능 검증
3. **다중 모달리티 통합**: 여러 의료영상 모달리티를 통합한 범용 모델 개발
4. **임상 검증 확대**: 다양한 의료 기관에서의 실제 임상 성능 검증

## 6. 개인 평가

**강점**: 
- 3D 의료영상에서 확장 가능한 언어-이미지 사전훈련의 실질적 해결책 제시
- 방사선학 데이터의 자연스러운 계층구조를 활용한 직관적이고 효과적인 어텐션 메커니즘
- 미가공 임상 데이터에서 직접 훈련 가능한 첫 번째 대규모 모델로 실용성이 높음
- 기존 ViT와 호환되어 최신 기술 활용 가능하며 구현이 단순함

**약점**: 
- 여전히 상당한 컴퓨팅 자원이 필요하며 일반적인 연구 환경에서는 재현 어려움
- 사전훈련 데이터의 클래스 불균형 문제가 성능에 영향을 미칠 수 있음
- 일부 특정 질병(교종 등)에서의 성능 불일치에 대한 명확한 원인 분석 부족

**적용 가능성**: 
대형 의료기관이나 연구소에서 자체 3D 의료영상 파운데이션 모델 구축 시 매우 유용한 프레임워크입니다. 특히 미가공 PACS 데이터를 직접 활용할 수 있어 데이터 큐레이션 비용을 크게 절감할 수 있으며, 방사선과 의사의 진단 보조 시스템 개발에 직접 활용 가능합니다.

**추천도**: ★★★★★ (3D 의료영상 AI 연구자 및 대형 의료기관 개발팀에게 강력 추천)

## 7. 참고 자료

- **원문**: [HLIP: Towards Scalable Language-Image Pre-training for 3D Medical Imaging](https://arxiv.org/abs/2505.21862)
- **코드**: [GitHub - HLIP](https://github.com/Zch0414/hlip)
- **데이터**: 
  - [CT-RATE](https://huggingface.co/datasets/ibrahimhamamci/CT-RATE) (흉부 CT)
  - [Rad-ChestCT](https://github.com/rachellea/ct-net-models) (외부 검증)
  - [Pub-Brain-5 벤치마크](https://github.com/Zch0414/hlip) (새로 구축된 뇌 MRI 벤치마크)
- **관련 연구**:
  - [CT-CLIP](https://arxiv.org/abs/2403.17834) - 3D CT를 위한 기존 CLIP 접근법
  - [BiomedCLIP](https://arxiv.org/abs/2303.00915) - 생의학 멀티모달 파운데이션 모델
  - [OpenCLIP](https://github.com/mlfoundations/open_clip) - 오픈소스 CLIP 구현
