---
categories:
- Paper
- VLM
date: '2025-09-16'
excerpt: 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally
  Across Scales and Tasks에 대한 체계적 분석과 핵심 기여 요약'
header:
  teaser: /assets/images/paper/p-tuning-v2-teaser.png
last_modified_at: '2025-09-16'
published: true
tags:
- Prompt Tuning
- Parameter Efficient
- Deep Prompt
- NLP
title: 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across
  Scales and Tasks'
toc: true
toc_sticky: true
---

# P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks

## 논문 정보
- **저자**: Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang
- **발표**: arXiv 2021
- **ArXiv**: [2110.07602](https://arxiv.org/abs/2110.07602)

## 1. 핵심 요약 (2-3문장)
P-Tuning v2는 모든 레이어에 학습 가능한 프롬프트를 추가하여 다양한 스케일의 모델에서 full fine-tuning에 비견하는 성능을 달성하는 개선된 방법입니다.

## 2. 배경 및 동기


## 3. 제안 방법

### 3.1 아키텍처 개요
시스템의 전체 아키텍처와 주요 구성 요소들을 설명합니다.

### 3.2 핵심 기술/알고리즘
핵심 기술적 혁신과 알고리즘에 대해 설명합니다.

### 3.3 구현 세부사항
구현과 관련된 중요한 기술적 세부사항들을 다룹니다.

## 4. 실험 및 결과

### 4.1 실험 설정
![Results Table 4 1](/assets/images/paper/p-tuning-v2/results_table_4_1.png)
*Figure: Results Table 4 1*
![Results Table 4 0](/assets/images/paper/p-tuning-v2/results_table_4_0.png)

### 4.2 주요 결과
![Results Table 4 0](/assets/images/paper/p-tuning-v2/results_table_4_0.png)
*Figure: Experimental results and performance metrics*
*Figure: Results Table 4 0*
- **입력만**: 기본 P-tuning 수준
- **모든 레이어**: 2-5% 성능 향상

### 4.3 분석
실험 결과에 대한 정성적 분석과 해석을 제공합니다.

## 5. 의의 및 영향
P-Tuning v2는 **레이어별 prompt 위치**가 성능에 미치는 결정적 영향을 보여준 중요한 연구입니다. 특히 모든 레이어에 prompt를 적용함으로써 각 레이어가 서로 다른 언어적 특성을 학습할 수 있음을 입증했습니다.
이 연구는 이후 parameter-efficient fine-tuning 방법론의 발전에 큰 영향을 미쳤으며, **소프트 프롬프트의 위치가 성능에 미치는 영향**에 대한 이해를 크게 발전시켰습니다.

## 6. 개인적 평가

**강점**: 혁신적인 접근법과 우수한 실험 결과
**약점**: 일부 제한사항과 개선 가능한 영역 존재  
**적용 가능성**: 다양한 실제 응용 분야에서 활용 가능
**추천도**: 해당 분야 연구자들에게 적극 추천