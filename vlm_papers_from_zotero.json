{
  "extraction_info": {
    "source": "Zotero mrichat collection",
    "extraction_date": "2025-07-25T15:44:26.926900",
    "total_papers_in_collection": 22,
    "vlm_papers_found": 21
  },
  "papers": [
    {
      "title": "(PDF) Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes",
      "excerpt": "(PDF) Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "PDF | Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These... | Find, read and cite all the research you need on ResearchGate",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "(pdf)_integrating_chain-of-thought_and_retrieval_augmented_generation_enhances_rare_disease_diagnosis_from_clinical_notes_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "",
        "journal": "",
        "authors": "",
        "date": "2025-06-10 2025-06-10",
        "url": "https://www.researchgate.net/publication/389917229_Integrating_Chain-of-Thought_and_Retrieval_Augmented_Generation_Enhances_Rare_Disease_Diagnosis_from_Clinical_Notes"
      }
    },
    {
      "title": "Anatomy-aware and acquisition-agnostic joint registration with SynthMorph",
      "excerpt": "Anatomy-aware and acquisition-agnostic joint registration with SynthMorph 논문 요약",
      "additional_tags": "Vision-Language, Imaging Neuroscience",
      "introduction": "Affine image registration is a cornerstone of medical image analysis. While classical algorithms can achieve excellent accuracy, they solve a time-consuming optimization for every image pair. Deep-learning (DL) methods learn a function that maps an image pair to an output transform. Evaluating the function is fast, but capturing large transforms can be challenging, and networks tend to struggle if a test-image characteristic shifts from the training domain, such as resolution. Most affine method...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "anatomy-aware_and_acquisition-agnostic_joint_registration_with_synthmorph_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.1162/imag_a_00197",
        "journal": "Imaging Neuroscience",
        "authors": "Malte Hoffmann, Andrew Hoopes, Douglas N. Greve, Bruce Fischl, Adrian V. Dalca",
        "date": "2024-06-25 2024-06-25",
        "url": "http://arxiv.org/abs/2301.11329"
      }
    },
    {
      "title": "AutoRG-Brain: Grounded Report Generation for Brain MRI",
      "excerpt": "AutoRG-Brain: Grounded Report Generation for Brain MRI 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Radiologists are tasked with interpreting a large number of images in a daily base, with the responsibility of generating corresponding reports. This demanding workload elevates the risk of human error, potentially leading to treatment delays, increased healthcare costs, revenue loss, and operational inefficiencies. To address these challenges, we initiate a series of work on grounded Automatic Report Generation (AutoRG), starting from the brain MRI interpretation system, which supports the deli...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "autorg-brain_grounded_report_generation_for_brain_mri_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.48550/arXiv.2407.16684",
        "journal": "",
        "authors": "Jiayu Lei, Xiaoman Zhang, Chaoyi Wu, Lisong Dai, Ya Zhang, Yanyong Zhang, Yanfeng Wang, Weidi Xie, Yuehua Li",
        "date": "2024-07-30 2024-07-30",
        "url": "http://arxiv.org/abs/2407.16684"
      }
    },
    {
      "title": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models",
      "excerpt": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Prompt learning is one of the most effective paradigms for adapting pre-trained vision-language models (VLMs) to the biomedical image classification tasks in few shot scenarios. However, most of the current prompt learning methods only used the text prompts and ignored the particular structures (such as the complex anatomical structures and subtle pathological features) in the biomedical images. In this work, we propose Biomed-DPT, a knowledge-enhanced dual modality prompt tuning technique. In d...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "biomed-dpt_dual_modality_prompt_tuning_for_biomedical_vision-language_models_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.48550/arXiv.2505.05189",
        "journal": "",
        "authors": "Wei Peng, Kang Liu, Jianchen Hu, Meng Zhang",
        "date": "2025-05-08 2025-05-08",
        "url": "http://arxiv.org/abs/2505.05189"
      }
    },
    {
      "title": "Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models",
      "excerpt": "Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Understanding brain disorders is crucial for accurate clinical diagnosis and treatment. Recent advances in Multimodal Large Language Models (MLLMs) offer a promising approach to interpreting medical images with the support of text descriptions. However, previous research has primarily focused on 2D medical images, leaving richer spatial information of 3D images under-explored, and single-modality-based methods are limited by overlooking the critical clinical information contained in other modali...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "brain-adapter_enhancing_neurological_disorder_analysis_with_adapter-tuning_multimodal_large_language_models_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.1109/ISBI60581.2025.10980770",
        "journal": "",
        "authors": "Jing Zhang, Xiaowei Yu, Yanjun Lyu, Lu Zhang, Tong Chen, Chao Cao, Yan Zhuang, Minheng Chen, Tianming Liu, Dajiang Zhu",
        "date": "2025-04-14 2025-04-14",
        "url": "http://arxiv.org/abs/2501.16282"
      }
    },
    {
      "title": "ChatCAD+: Toward a Universal and Reliable Interactive CAD Using LLMs",
      "excerpt": "ChatCAD+: Toward a Universal and Reliable Interactive CAD Using LLMs 논문 요약",
      "additional_tags": "Vision-Language, IEEE Transactions on Medical Imaging",
      "introduction": "The integration of Computer-Aided Diagnosis (CAD) with Large Language Models (LLMs) presents a promising frontier in clinical applications, notably in automating diagnostic processes akin to those performed by radiologists and providing consultations similar to a virtual family doctor. Despite the promising potential of this integration, current works face at least two limitations: (1) From the perspective of a radiologist, existing studies typically have a restricted scope of applicable imaging...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "chatcad+_toward_a_universal_and_reliable_interactive_cad_using_llms_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.1109/TMI.2024.3398350",
        "journal": "IEEE Transactions on Medical Imaging",
        "authors": "Zihao Zhao, Sheng Wang, Jinchen Gu, Yitao Zhu, Lanzhuju Mei, Zixu Zhuang, Zhiming Cui, Qian Wang, Dinggang Shen",
        "date": "2024-01-00 2024-01",
        "url": "https://ieeexplore.ieee.org/abstract/document/10522762"
      }
    },
    {
      "title": "ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models",
      "excerpt": "ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Large language models (LLMs) have recently demonstrated their potential in clinical applications, providing valuable medical knowledge and advice. For example, a large dialog LLM like ChatGPT has successfully passed part of the US medical licensing exam. However, LLMs currently have difficulty processing images, making it challenging to interpret information from medical images, which are rich in information that supports clinical decisions. On the other hand, computer-aided diagnosis (CAD) netw...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "chatcad_interactive_computer-aided_diagnosis_on_medical_image_using_large_language_models_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "",
        "journal": "",
        "authors": "Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, Dinggang Shen",
        "date": "2023-02-14 2023/02/14",
        "url": "https://arxiv.org/abs/2302.07257v1"
      }
    },
    {
      "title": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models",
      "excerpt": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Chain-of-thought (CoT) reasoning boosts large language models' (LLMs) performance on complex tasks but faces two key limitations: a lack of reliability when solely relying on LLM-generated reasoning chains and interference from natural language reasoning steps with the models' inference process, also known as the inference logic of LLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation,featuring knowledge g...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "cot-rag_integrating_chain_of_thought_and_retrieval-augmented_generation_to_enhance_reasoning_in_large_language_models_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.48550/arXiv.2504.13534",
        "journal": "",
        "authors": "Feiyang Li, Peng Fang, Zhan Shi, Arijit Khan, Fang Wang, Dan Feng, Weihao Wang, Xin Zhang, Yongjian Cui",
        "date": "2025-05-19 2025-05-19",
        "url": "http://arxiv.org/abs/2504.13534"
      }
    },
    {
      "title": "Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine",
      "excerpt": "Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine 논문 요약",
      "additional_tags": "Vision-Language, npj Digital Medicine",
      "introduction": "One of the major barriers to using large language models (LLMs) in medicine is the perception they use uninterpretable methods to make clinical decisions that are inherently different from the cognitive processes of clinicians. In this manuscript we develop diagnostic reasoning prompts to study whether LLMs can imitate clinical reasoning while accurately forming a diagnosis. We find that GPT-4 can be prompted to mimic the common clinical reasoning processes of clinicians without sacrificing diag...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "diagnostic_reasoning_prompts_reveal_the_potential_for_large_language_model_interpretability_in_medicine_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.1038/s41746-024-01010-1",
        "journal": "npj Digital Medicine",
        "authors": "Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, Jonathan H. Chen",
        "date": "2024-01-24 2024-01-24",
        "url": "https://www.nature.com/articles/s41746-024-01010-1"
      }
    },
    {
      "title": "Enhancing radiomics features via a large language model for classifying benign and malignant breast tumors in mammography",
      "excerpt": "Enhancing radiomics features via a large language model for classifying benign and malignant breast tumors in mammography 논문 요약",
      "additional_tags": "Vision-Language, Computer Methods and Programs in Biomedicine",
      "introduction": "Background and Objectives\nRadiomics is widely used to assist in clinical decision-making, disease diagnosis, and treatment planning for various target organs, including the breast. Recent advances in large language models (LLMs) have helped enhance radiomics analysis.\nMaterials and Methods\nHerein, we sought to improve radiomics analysis by incorporating LLM-learned clinical knowledge, to classify benign and malignant tumors in breast mammography. We extracted radiomics features from the mammogra...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "enhancing_radiomics_features_via_a_large_language_model_for_classifying_benign_and_malignant_breast_tumors_in_mammography_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.1016/j.cmpb.2025.108765",
        "journal": "Computer Methods and Programs in Biomedicine",
        "authors": "Sinyoung Ra, Jonghun Kim, Inye Na, Eun Sook Ko, Hyunjin Park",
        "date": "2025-06-01 2025-06-01",
        "url": "https://www.sciencedirect.com/science/article/pii/S0169260725001828"
      }
    },
    {
      "title": "Enhancing vision-language models for medical imaging: bridging the 3D gap with innovative slice selection",
      "excerpt": "Enhancing vision-language models for medical imaging: bridging the 3D gap with innovative slice selection 논문 요약",
      "additional_tags": "Vision-Language, Advances in Neural Information Processing Systems",
      "introduction": "",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "enhancing_vision-language_models_for_medical_imaging_bridging_the_3d_gap_with_innovative_slice_selection_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "",
        "journal": "Advances in Neural Information Processing Systems",
        "authors": "Yuli Wang, Jian Peng, Yuwei Dai, Craig Jones, Haris Sair, Jinglai Shen, Nicolas Loizou, Jing Wu, Wen-Chi Hsu, Maliha Imami, Zhicheng Jiao, Paul Zhang, Harrison Bai",
        "date": "2024-12-16 2024-12-16",
        "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/b53513b83232116ae25f57a174a7c993-Abstract-Datasets_and_Benchmarks_Track.html"
      }
    },
    {
      "title": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes",
      "excerpt": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in real-world settings, foundation models are not optimized for domain-specific tasks like clinical diagnosis, yet inputs are unstructured clinical notes rather than standardized terms. How LLMs can be instructed t...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "integrating_chain-of-thought_and_retrieval_augmented_generation_enhances_rare_disease_diagnosis_from_clinical_notes_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.48550/arXiv.2503.12286",
        "journal": "",
        "authors": "Da Wu, Zhanliang Wang, Quan Nguyen, Kai Wang",
        "date": "2025-03-15 2025-03-15",
        "url": "http://arxiv.org/abs/2503.12286"
      }
    },
    {
      "title": "LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle Zero-shot Radiology Recognition?",
      "excerpt": "LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle Zero-shot Radiology Recognition? 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Recently, multimodal large models (MLLMs) have demonstrated exceptional capabilities in visual understanding and reasoning across various vision-language tasks. However, MLLMs usually perform poorly in zero-shot medical disease recognition, as they do not fully exploit the captured features and available medical knowledge. To address this challenge, we propose LLaVA-RadZ, a simple yet effective framework for zero-shot medical disease recognition. Specifically, we design an end-to-end training st...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "llava-radz_can_multimodal_large_language_models_effectively_tackle_zero-shot_radiology_recognition?_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.48550/arXiv.2503.07487",
        "journal": "",
        "authors": "Bangyan Li, Wenxuan Huang, Yunhang Shen, Yeqiang Wang, Shaohui Lin, Jingzhong Lin, Ling You, Yinqi Zhang, Ke Li, Xing Sun, Yuling Sun",
        "date": "2025-03-10 2025-03-10",
        "url": "http://arxiv.org/abs/2503.07487"
      }
    },
    {
      "title": "Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales",
      "excerpt": "Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a \"reasoning-aware\" diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficien...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "large_language_models_are_clinical_reasoners_reasoning-aware_diagnosis_framework_with_prompt-generated_rationales_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.48550/arXiv.2312.07399",
        "journal": "",
        "authors": "Taeyoon Kwon, Kai Tzu-iunn Ong, Dongjin Kang, Seungjun Moon, Jeong Ryong Lee, Dosik Hwang, Yongsik Sim, Beomseok Sohn, Dongha Lee, Jinyoung Yeo",
        "date": "2024-05-10 2024-05-10",
        "url": "http://arxiv.org/abs/2312.07399"
      }
    },
    {
      "title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models",
      "excerpt": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to address these issues. However, the amount o...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "mmed-rag_versatile_multimodal_rag_system_for_medical_vision_language_models_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.48550/arXiv.2410.13085",
        "journal": "",
        "authors": "Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, Huaxiu Yao",
        "date": "2025-03-03 2025-03-03",
        "url": "http://arxiv.org/abs/2410.13085"
      }
    },
    {
      "title": "MedCoT: Medical Chain of Thought via Hierarchical Expert",
      "excerpt": "MedCoT: Medical Chain of Thought via Hierarchical Expert 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Artificial intelligence has advanced in Medical Visual Question Answering (Med-VQA), but prevalent research tends to focus on the accuracy of the answers, often overlooking the reasoning paths and interpretability, which are crucial in clinical settings. Besides, current Med-VQA algorithms, typically reliant on singular models, lack the robustness needed for real-world medical diagnostics which usually require collaborative expert evaluation. To address these shortcomings, this paper presents Me...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "medcot_medical_chain_of_thought_via_hierarchical_expert_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.18653/v1/2024.emnlp-main.962",
        "journal": "",
        "authors": "Jiaxiang Liu, Yuan Wang, Jiawei Du, Joey Tianyi Zhou, Zuozhu Liu, Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen",
        "date": "2024-01-00 2024-01",
        "url": "https://aclanthology.org/2024.emnlp-main.962/"
      }
    },
    {
      "title": "RadioRAG: Factual large language models for enhanced diagnostics in radiology using online retrieval augmented generation",
      "excerpt": "RadioRAG: Factual large language models for enhanced diagnostics in radiology using online retrieval augmented generation 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Large language models (LLMs) often generate outdated or inaccurate information based on static training datasets. Retrieval augmented generation (RAG) mitigates this by integrating outside data sources. While previous RAG systems used pre-assembled, fixed databases with limited flexibility, we have developed Radiology RAG (RadioRAG), an end-to-end framework that retrieves data from authoritative radiologic online sources in real-time. We evaluate the diagnostic accuracy of various LLMs when answ...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "radiorag_factual_large_language_models_for_enhanced_diagnostics_in_radiology_using_online_retrieval_augmented_generation_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.48550/arXiv.2407.15621",
        "journal": "",
        "authors": "Soroosh Tayebi Arasteh, Mahshad Lotfinia, Keno Bressem, Robert Siepmann, Lisa Adams, Dyke Ferber, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn",
        "date": "2024-12-25 2024-12-25",
        "url": "http://arxiv.org/abs/2407.15621"
      }
    },
    {
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "excerpt": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "retrieval-augmented_generation_for_knowledge-intensive_nlp_tasks_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.48550/arXiv.2005.11401",
        "journal": "",
        "authors": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela",
        "date": "2021-04-12 2021-04-12",
        "url": "http://arxiv.org/abs/2005.11401"
      }
    },
    {
      "title": "Towards a holistic framework for multimodal LLM in 3D brain CT radiology report generation | Nature Communications",
      "excerpt": "Towards a holistic framework for multimodal LLM in 3D brain CT radiology report generation | Nature Communications 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "towards_a_holistic_framework_for_multimodal_llm_in_3d_brain_ct_radiology_report_generation_|_nature_communications_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "",
        "journal": "",
        "authors": "",
        "date": "",
        "url": "https://www.nature.com/articles/s41467-025-57426-0"
      }
    },
    {
      "title": "V2T-CoT: From Vision to Text Chain-of-Thought for Medical Reasoning and Diagnosis",
      "excerpt": "V2T-CoT: From Vision to Text Chain-of-Thought for Medical Reasoning and Diagnosis 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "Recent advances in multimodal techniques have led to significant progress in Medical Visual Question Answering (Med-VQA). However, most existing models focus on global image features rather than localizing disease-specific regions crucial for diagnosis. Additionally, current research tends to emphasize answer accuracy at the expense of the reasoning pathway, yet both are crucial for clinical decision-making. To address these challenges, we propose From Vision to Text Chain-of-Thought (V2T-CoT), ...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "v2t-cot_from_vision_to_text_chain-of-thought_for_medical_reasoning_and_diagnosis_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "10.48550/arXiv.2506.19610",
        "journal": "",
        "authors": "Yuan Wang, Jiaxiang Liu, Shujian Gao, Bin Feng, Zhihang Tang, Xiaotang Gai, Jian Wu, Zuozhu Liu",
        "date": "2025-06-27 2025-06-27",
        "url": "http://arxiv.org/abs/2506.19610"
      }
    },
    {
      "title": "VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis",
      "excerpt": "VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis 논문 요약",
      "additional_tags": "Vision-Language",
      "introduction": "We present VoxelPrompt, an agent-driven vision-language framework that tackles diverse radiological tasks through joint modeling of natural language, image volumes, and analytical metrics. VoxelPrompt is multi-modal and versatile, leveraging the flexibility of language interaction while providing quantitatively grounded image analysis. Given a variable number of 3D medical volumes, such as MRI and CT scans, VoxelPrompt employs a language agent that iteratively predicts executable instructions to...",
      "related_work_vlm": "기존 Vision-Language 모델들과의 비교 연구가 필요합니다.",
      "specific_domain": "Computer Vision",
      "related_work_domain": "컴퓨터 비전 분야의 관련 연구들을 분석합니다.",
      "architecture_description": "논문에서 제안하는 아키텍처에 대한 설명이 필요합니다.",
      "architecture_image": "voxelprompt_a_vision-language_agent_for_grounded_medical_image_analysis_architecture.png",
      "key_components": "주요 구성 요소들에 대한 설명이 필요합니다.",
      "training_strategy": "훈련 전략에 대한 설명이 필요합니다.",
      "datasets": "사용된 데이터셋에 대한 정보가 필요합니다.",
      "results": "실험 결과에 대한 설명이 필요합니다.",
      "ablation_studies": "Ablation study 결과에 대한 설명이 필요합니다.",
      "conclusion": "논문의 결론 및 기여도에 대한 설명이 필요합니다.",
      "key_takeaways": "주요 시사점들을 정리해주세요.",
      "metadata": {
        "doi": "",
        "journal": "",
        "authors": "Andrew Hoopes, Victor Ion Butoi, John V. Guttag, Adrian V. Dalca",
        "date": "2024-10-10 2024/10/10",
        "url": "https://arxiv.org/abs/2410.08397v1"
      }
    }
  ]
}